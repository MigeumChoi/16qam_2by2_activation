{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPK4CNrLaQD/mNgkD+X/Bww"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ACadmPh2Fgl","executionInfo":{"status":"ok","timestamp":1694932866069,"user_tz":-540,"elapsed":47055,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"db6251eb-e2ca-4d48-b910-837f9bc35be4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3681\n","Epoch 1: val_loss improved from inf to 0.35037, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 0.3681 - val_loss: 0.3504\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3632\n","Epoch 2: val_loss improved from 0.35037 to 0.34590, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3632 - val_loss: 0.3459\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3585\n","Epoch 3: val_loss improved from 0.34590 to 0.34150, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3585 - val_loss: 0.3415\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3538"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4: val_loss improved from 0.34150 to 0.33716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3538 - val_loss: 0.3372\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3492\n","Epoch 5: val_loss improved from 0.33716 to 0.33289, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3492 - val_loss: 0.3329\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3446\n","Epoch 6: val_loss improved from 0.33289 to 0.32867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.3446 - val_loss: 0.3287\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3401\n","Epoch 7: val_loss improved from 0.32867 to 0.32452, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.3401 - val_loss: 0.3245\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3357\n","Epoch 8: val_loss improved from 0.32452 to 0.32043, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3357 - val_loss: 0.3204\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3313\n","Epoch 9: val_loss improved from 0.32043 to 0.31640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3313 - val_loss: 0.3164\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3270\n","Epoch 10: val_loss improved from 0.31640 to 0.31243, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.3270 - val_loss: 0.3124\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3228\n","Epoch 11: val_loss improved from 0.31243 to 0.30851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.3228 - val_loss: 0.3085\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3186\n","Epoch 12: val_loss improved from 0.30851 to 0.30465, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.3186 - val_loss: 0.3046\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3144\n","Epoch 13: val_loss improved from 0.30465 to 0.30084, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.3144 - val_loss: 0.3008\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3104\n","Epoch 14: val_loss improved from 0.30084 to 0.29709, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.3104 - val_loss: 0.2971\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3064\n","Epoch 15: val_loss improved from 0.29709 to 0.29339, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.3064 - val_loss: 0.2934\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3024\n","Epoch 16: val_loss improved from 0.29339 to 0.28974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.3024 - val_loss: 0.2897\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2985\n","Epoch 17: val_loss improved from 0.28974 to 0.28615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.2985 - val_loss: 0.2861\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2946\n","Epoch 18: val_loss improved from 0.28615 to 0.28260, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.2946 - val_loss: 0.2826\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2908\n","Epoch 19: val_loss improved from 0.28260 to 0.27911, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.2908 - val_loss: 0.2791\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2871\n","Epoch 20: val_loss improved from 0.27911 to 0.27566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.2871 - val_loss: 0.2757\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2834\n","Epoch 21: val_loss improved from 0.27566 to 0.27226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.2834 - val_loss: 0.2723\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2797\n","Epoch 22: val_loss improved from 0.27226 to 0.26891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.2797 - val_loss: 0.2689\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2761\n","Epoch 23: val_loss improved from 0.26891 to 0.26560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.2761 - val_loss: 0.2656\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2725\n","Epoch 24: val_loss improved from 0.26560 to 0.26234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.2725 - val_loss: 0.2623\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2690\n","Epoch 25: val_loss improved from 0.26234 to 0.25912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.2690 - val_loss: 0.2591\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2656\n","Epoch 26: val_loss improved from 0.25912 to 0.25595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.2656 - val_loss: 0.2560\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2621\n","Epoch 27: val_loss improved from 0.25595 to 0.25282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.2621 - val_loss: 0.2528\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2588\n","Epoch 28: val_loss improved from 0.25282 to 0.24974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2588 - val_loss: 0.2497\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2554\n","Epoch 29: val_loss improved from 0.24974 to 0.24670, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.2554 - val_loss: 0.2467\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2522\n","Epoch 30: val_loss improved from 0.24670 to 0.24369, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.2522 - val_loss: 0.2437\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2489\n","Epoch 31: val_loss improved from 0.24369 to 0.24073, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.2489 - val_loss: 0.2407\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2457\n","Epoch 32: val_loss improved from 0.24073 to 0.23781, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.2457 - val_loss: 0.2378\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2426\n","Epoch 33: val_loss improved from 0.23781 to 0.23493, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.2426 - val_loss: 0.2349\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2394\n","Epoch 34: val_loss improved from 0.23493 to 0.23209, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.2394 - val_loss: 0.2321\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2364\n","Epoch 35: val_loss improved from 0.23209 to 0.22929, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.2364 - val_loss: 0.2293\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2333\n","Epoch 36: val_loss improved from 0.22929 to 0.22652, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.2333 - val_loss: 0.2265\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2303\n","Epoch 37: val_loss improved from 0.22652 to 0.22380, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.2303 - val_loss: 0.2238\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2274\n","Epoch 38: val_loss improved from 0.22380 to 0.22111, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.2274 - val_loss: 0.2211\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2244\n","Epoch 39: val_loss improved from 0.22111 to 0.21845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2244 - val_loss: 0.2185\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2216\n","Epoch 40: val_loss improved from 0.21845 to 0.21583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2216 - val_loss: 0.2158\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2187\n","Epoch 41: val_loss improved from 0.21583 to 0.21325, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2187 - val_loss: 0.2132\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2159\n","Epoch 42: val_loss improved from 0.21325 to 0.21070, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.2159 - val_loss: 0.2107\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2131\n","Epoch 43: val_loss improved from 0.21070 to 0.20819, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2131 - val_loss: 0.2082\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2104\n","Epoch 44: val_loss improved from 0.20819 to 0.20571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2104 - val_loss: 0.2057\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2077\n","Epoch 45: val_loss improved from 0.20571 to 0.20326, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2077 - val_loss: 0.2033\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2050\n","Epoch 46: val_loss improved from 0.20326 to 0.20085, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2050 - val_loss: 0.2008\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2024\n","Epoch 47: val_loss improved from 0.20085 to 0.19847, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2024 - val_loss: 0.1985\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1998\n","Epoch 48: val_loss improved from 0.19847 to 0.19612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1998 - val_loss: 0.1961\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1972\n","Epoch 49: val_loss improved from 0.19612 to 0.19381, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1972 - val_loss: 0.1938\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1947\n","Epoch 50: val_loss improved from 0.19381 to 0.19152, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1947 - val_loss: 0.1915\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1922\n","Epoch 51: val_loss improved from 0.19152 to 0.18927, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1922 - val_loss: 0.1893\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1897\n","Epoch 52: val_loss improved from 0.18927 to 0.18704, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1897 - val_loss: 0.1870\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1873\n","Epoch 53: val_loss improved from 0.18704 to 0.18485, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1873 - val_loss: 0.1849\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1849\n","Epoch 54: val_loss improved from 0.18485 to 0.18269, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1849 - val_loss: 0.1827\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1825\n","Epoch 55: val_loss improved from 0.18269 to 0.18055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1825 - val_loss: 0.1806\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1802\n","Epoch 56: val_loss improved from 0.18055 to 0.17845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1802 - val_loss: 0.1784\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1778\n","Epoch 57: val_loss improved from 0.17845 to 0.17637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1778 - val_loss: 0.1764\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1756\n","Epoch 58: val_loss improved from 0.17637 to 0.17433, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1756 - val_loss: 0.1743\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1733\n","Epoch 59: val_loss improved from 0.17433 to 0.17231, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1733 - val_loss: 0.1723\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1711\n","Epoch 60: val_loss improved from 0.17231 to 0.17032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1711 - val_loss: 0.1703\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1689\n","Epoch 61: val_loss improved from 0.17032 to 0.16835, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1689 - val_loss: 0.1684\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1667\n","Epoch 62: val_loss improved from 0.16835 to 0.16642, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1667 - val_loss: 0.1664\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1646\n","Epoch 63: val_loss improved from 0.16642 to 0.16451, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1646 - val_loss: 0.1645\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1625\n","Epoch 64: val_loss improved from 0.16451 to 0.16262, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1625 - val_loss: 0.1626\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1604\n","Epoch 65: val_loss improved from 0.16262 to 0.16076, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1604 - val_loss: 0.1608\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1583\n","Epoch 66: val_loss improved from 0.16076 to 0.15893, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1583 - val_loss: 0.1589\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1563\n","Epoch 67: val_loss improved from 0.15893 to 0.15713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1563 - val_loss: 0.1571\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1543\n","Epoch 68: val_loss improved from 0.15713 to 0.15535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1543 - val_loss: 0.1553\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1523\n","Epoch 69: val_loss improved from 0.15535 to 0.15359, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1523 - val_loss: 0.1536\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1504\n","Epoch 70: val_loss improved from 0.15359 to 0.15186, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1504 - val_loss: 0.1519\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1484\n","Epoch 71: val_loss improved from 0.15186 to 0.15015, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1484 - val_loss: 0.1502\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1465\n","Epoch 72: val_loss improved from 0.15015 to 0.14847, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1465 - val_loss: 0.1485\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1447\n","Epoch 73: val_loss improved from 0.14847 to 0.14681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1447 - val_loss: 0.1468\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1428\n","Epoch 74: val_loss improved from 0.14681 to 0.14517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1428 - val_loss: 0.1452\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1410\n","Epoch 75: val_loss improved from 0.14517 to 0.14356, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1410 - val_loss: 0.1436\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1392\n","Epoch 76: val_loss improved from 0.14356 to 0.14197, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1392 - val_loss: 0.1420\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1374\n","Epoch 77: val_loss improved from 0.14197 to 0.14040, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1374 - val_loss: 0.1404\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1356\n","Epoch 78: val_loss improved from 0.14040 to 0.13886, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1356 - val_loss: 0.1389\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1339\n","Epoch 79: val_loss improved from 0.13886 to 0.13734, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1339 - val_loss: 0.1373\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1322\n","Epoch 80: val_loss improved from 0.13734 to 0.13584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1322 - val_loss: 0.1358\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1305\n","Epoch 81: val_loss improved from 0.13584 to 0.13436, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1305 - val_loss: 0.1344\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1289\n","Epoch 82: val_loss improved from 0.13436 to 0.13290, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1289 - val_loss: 0.1329\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1272\n","Epoch 83: val_loss improved from 0.13290 to 0.13147, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1272 - val_loss: 0.1315\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1256\n","Epoch 84: val_loss improved from 0.13147 to 0.13005, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1256 - val_loss: 0.1301\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1240\n","Epoch 85: val_loss improved from 0.13005 to 0.12866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1240 - val_loss: 0.1287\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1224\n","Epoch 86: val_loss improved from 0.12866 to 0.12728, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1224 - val_loss: 0.1273\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1209\n","Epoch 87: val_loss improved from 0.12728 to 0.12593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1209 - val_loss: 0.1259\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1193\n","Epoch 88: val_loss improved from 0.12593 to 0.12460, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1193 - val_loss: 0.1246\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1178\n","Epoch 89: val_loss improved from 0.12460 to 0.12328, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1178 - val_loss: 0.1233\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1163\n","Epoch 90: val_loss improved from 0.12328 to 0.12199, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1163 - val_loss: 0.1220\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1149\n","Epoch 91: val_loss improved from 0.12199 to 0.12072, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1149 - val_loss: 0.1207\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1134\n","Epoch 92: val_loss improved from 0.12072 to 0.11946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1134 - val_loss: 0.1195\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1120\n","Epoch 93: val_loss improved from 0.11946 to 0.11823, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1120 - val_loss: 0.1182\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1106\n","Epoch 94: val_loss improved from 0.11823 to 0.11701, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1106 - val_loss: 0.1170\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1092\n","Epoch 95: val_loss improved from 0.11701 to 0.11581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1092 - val_loss: 0.1158\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1078\n","Epoch 96: val_loss improved from 0.11581 to 0.11463, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1078 - val_loss: 0.1146\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1065\n","Epoch 97: val_loss improved from 0.11463 to 0.11347, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1065 - val_loss: 0.1135\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1052\n","Epoch 98: val_loss improved from 0.11347 to 0.11232, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1052 - val_loss: 0.1123\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1038\n","Epoch 99: val_loss improved from 0.11232 to 0.11120, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1038 - val_loss: 0.1112\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1026\n","Epoch 100: val_loss improved from 0.11120 to 0.11009, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1026 - val_loss: 0.1101\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1013\n","Epoch 101: val_loss improved from 0.11009 to 0.10899, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1013 - val_loss: 0.1090\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1000\n","Epoch 102: val_loss improved from 0.10899 to 0.10792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1000 - val_loss: 0.1079\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0988\n","Epoch 103: val_loss improved from 0.10792 to 0.10686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0988 - val_loss: 0.1069\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0976\n","Epoch 104: val_loss improved from 0.10686 to 0.10582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0976 - val_loss: 0.1058\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0964\n","Epoch 105: val_loss improved from 0.10582 to 0.10480, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0964 - val_loss: 0.1048\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0952\n","Epoch 106: val_loss improved from 0.10480 to 0.10379, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0952 - val_loss: 0.1038\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0940\n","Epoch 107: val_loss improved from 0.10379 to 0.10279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0940 - val_loss: 0.1028\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0929\n","Epoch 108: val_loss improved from 0.10279 to 0.10182, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0929 - val_loss: 0.1018\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0917\n","Epoch 109: val_loss improved from 0.10182 to 0.10086, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0917 - val_loss: 0.1009\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0906\n","Epoch 110: val_loss improved from 0.10086 to 0.09991, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0906 - val_loss: 0.0999\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0895\n","Epoch 111: val_loss improved from 0.09991 to 0.09898, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0895 - val_loss: 0.0990\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0884\n","Epoch 112: val_loss improved from 0.09898 to 0.09807, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0884 - val_loss: 0.0981\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0874\n","Epoch 113: val_loss improved from 0.09807 to 0.09717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0874 - val_loss: 0.0972\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0863\n","Epoch 114: val_loss improved from 0.09717 to 0.09628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0863 - val_loss: 0.0963\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0853\n","Epoch 115: val_loss improved from 0.09628 to 0.09541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0853 - val_loss: 0.0954\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0842\n","Epoch 116: val_loss improved from 0.09541 to 0.09456, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0842 - val_loss: 0.0946\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0832\n","Epoch 117: val_loss improved from 0.09456 to 0.09372, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0832 - val_loss: 0.0937\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0822\n","Epoch 118: val_loss improved from 0.09372 to 0.09289, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0822 - val_loss: 0.0929\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0813\n","Epoch 119: val_loss improved from 0.09289 to 0.09208, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0813 - val_loss: 0.0921\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0803\n","Epoch 120: val_loss improved from 0.09208 to 0.09128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0803 - val_loss: 0.0913\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0794\n","Epoch 121: val_loss improved from 0.09128 to 0.09049, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0794 - val_loss: 0.0905\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0784\n","Epoch 122: val_loss improved from 0.09049 to 0.08972, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0784 - val_loss: 0.0897\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0775\n","Epoch 123: val_loss improved from 0.08972 to 0.08896, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0775 - val_loss: 0.0890\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0766\n","Epoch 124: val_loss improved from 0.08896 to 0.08822, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0766 - val_loss: 0.0882\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0757\n","Epoch 125: val_loss improved from 0.08822 to 0.08748, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0757 - val_loss: 0.0875\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0749\n","Epoch 126: val_loss improved from 0.08748 to 0.08676, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0749 - val_loss: 0.0868\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0740\n","Epoch 127: val_loss improved from 0.08676 to 0.08605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0740 - val_loss: 0.0861\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0731\n","Epoch 128: val_loss improved from 0.08605 to 0.08536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0731 - val_loss: 0.0854\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0723\n","Epoch 129: val_loss improved from 0.08536 to 0.08468, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0723 - val_loss: 0.0847\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0715\n","Epoch 130: val_loss improved from 0.08468 to 0.08401, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0715 - val_loss: 0.0840\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0707\n","Epoch 131: val_loss improved from 0.08401 to 0.08335, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0707 - val_loss: 0.0833\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0699\n","Epoch 132: val_loss improved from 0.08335 to 0.08270, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0699 - val_loss: 0.0827\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0691\n","Epoch 133: val_loss improved from 0.08270 to 0.08207, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0691 - val_loss: 0.0821\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0683\n","Epoch 134: val_loss improved from 0.08207 to 0.08144, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0683 - val_loss: 0.0814\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0676\n","Epoch 135: val_loss improved from 0.08144 to 0.08083, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0676 - val_loss: 0.0808\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0668\n","Epoch 136: val_loss improved from 0.08083 to 0.08023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0668 - val_loss: 0.0802\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0661\n","Epoch 137: val_loss improved from 0.08023 to 0.07964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0661 - val_loss: 0.0796\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0654\n","Epoch 138: val_loss improved from 0.07964 to 0.07906, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0654 - val_loss: 0.0791\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0647\n","Epoch 139: val_loss improved from 0.07906 to 0.07849, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0647 - val_loss: 0.0785\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0640\n","Epoch 140: val_loss improved from 0.07849 to 0.07794, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0640 - val_loss: 0.0779\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0633\n","Epoch 141: val_loss improved from 0.07794 to 0.07739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0633 - val_loss: 0.0774\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0626\n","Epoch 142: val_loss improved from 0.07739 to 0.07685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0626 - val_loss: 0.0769\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0620\n","Epoch 143: val_loss improved from 0.07685 to 0.07633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0620 - val_loss: 0.0763\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0613\n","Epoch 144: val_loss improved from 0.07633 to 0.07581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0613 - val_loss: 0.0758\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0607\n","Epoch 145: val_loss improved from 0.07581 to 0.07530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0607 - val_loss: 0.0753\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0600\n","Epoch 146: val_loss improved from 0.07530 to 0.07481, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0600 - val_loss: 0.0748\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0594\n","Epoch 147: val_loss improved from 0.07481 to 0.07432, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0594 - val_loss: 0.0743\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0588\n","Epoch 148: val_loss improved from 0.07432 to 0.07384, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0588 - val_loss: 0.0738\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0582\n","Epoch 149: val_loss improved from 0.07384 to 0.07337, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0582 - val_loss: 0.0734\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0576\n","Epoch 150: val_loss improved from 0.07337 to 0.07292, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0576 - val_loss: 0.0729\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0570\n","Epoch 151: val_loss improved from 0.07292 to 0.07247, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0570 - val_loss: 0.0725\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0565\n","Epoch 152: val_loss improved from 0.07247 to 0.07202, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0565 - val_loss: 0.0720\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0559\n","Epoch 153: val_loss improved from 0.07202 to 0.07159, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0559 - val_loss: 0.0716\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0554\n","Epoch 154: val_loss improved from 0.07159 to 0.07117, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0554 - val_loss: 0.0712\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0548\n","Epoch 155: val_loss improved from 0.07117 to 0.07075, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0548 - val_loss: 0.0708\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0543\n","Epoch 156: val_loss improved from 0.07075 to 0.07035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0543 - val_loss: 0.0703\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0538\n","Epoch 157: val_loss improved from 0.07035 to 0.06995, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0538 - val_loss: 0.0700\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0533\n","Epoch 158: val_loss improved from 0.06995 to 0.06956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0533 - val_loss: 0.0696\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0528\n","Epoch 159: val_loss improved from 0.06956 to 0.06918, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0528 - val_loss: 0.0692\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0523\n","Epoch 160: val_loss improved from 0.06918 to 0.06880, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0523 - val_loss: 0.0688\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0518\n","Epoch 161: val_loss improved from 0.06880 to 0.06844, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0518 - val_loss: 0.0684\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0513\n","Epoch 162: val_loss improved from 0.06844 to 0.06808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0513 - val_loss: 0.0681\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0508\n","Epoch 163: val_loss improved from 0.06808 to 0.06773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0508 - val_loss: 0.0677\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0504\n","Epoch 164: val_loss improved from 0.06773 to 0.06739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0504 - val_loss: 0.0674\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0499\n","Epoch 165: val_loss improved from 0.06739 to 0.06705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0499 - val_loss: 0.0670\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0495\n","Epoch 166: val_loss improved from 0.06705 to 0.06672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0495 - val_loss: 0.0667\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0490\n","Epoch 167: val_loss improved from 0.06672 to 0.06640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0490 - val_loss: 0.0664\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0486\n","Epoch 168: val_loss improved from 0.06640 to 0.06608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0486 - val_loss: 0.0661\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0482\n","Epoch 169: val_loss improved from 0.06608 to 0.06578, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0482 - val_loss: 0.0658\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0478\n","Epoch 170: val_loss improved from 0.06578 to 0.06548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0478 - val_loss: 0.0655\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 171: val_loss improved from 0.06548 to 0.06518, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0474 - val_loss: 0.0652\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0470\n","Epoch 172: val_loss improved from 0.06518 to 0.06489, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0470 - val_loss: 0.0649\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0466\n","Epoch 173: val_loss improved from 0.06489 to 0.06461, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0466 - val_loss: 0.0646\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 174: val_loss improved from 0.06461 to 0.06434, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0462 - val_loss: 0.0643\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 175: val_loss improved from 0.06434 to 0.06407, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0458 - val_loss: 0.0641\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 176: val_loss improved from 0.06407 to 0.06380, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0455 - val_loss: 0.0638\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0451\n","Epoch 177: val_loss improved from 0.06380 to 0.06355, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0451 - val_loss: 0.0635\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0448\n","Epoch 178: val_loss improved from 0.06355 to 0.06330, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0448 - val_loss: 0.0633\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 179: val_loss improved from 0.06330 to 0.06305, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.0444 - val_loss: 0.0631\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0441\n","Epoch 180: val_loss improved from 0.06305 to 0.06281, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0441 - val_loss: 0.0628\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0437\n","Epoch 181: val_loss improved from 0.06281 to 0.06258, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0437 - val_loss: 0.0626\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 182: val_loss improved from 0.06258 to 0.06235, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0434 - val_loss: 0.0623\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0431\n","Epoch 183: val_loss improved from 0.06235 to 0.06212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0431 - val_loss: 0.0621\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 184: val_loss improved from 0.06212 to 0.06191, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0428 - val_loss: 0.0619\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0425\n","Epoch 185: val_loss improved from 0.06191 to 0.06169, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0425 - val_loss: 0.0617\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0422\n","Epoch 186: val_loss improved from 0.06169 to 0.06149, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0422 - val_loss: 0.0615\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 187: val_loss improved from 0.06149 to 0.06128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0419 - val_loss: 0.0613\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 188: val_loss improved from 0.06128 to 0.06109, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0416 - val_loss: 0.0611\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 189: val_loss improved from 0.06109 to 0.06089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0413 - val_loss: 0.0609\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 190: val_loss improved from 0.06089 to 0.06071, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0410 - val_loss: 0.0607\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 191: val_loss improved from 0.06071 to 0.06052, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0407 - val_loss: 0.0605\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0405\n","Epoch 192: val_loss improved from 0.06052 to 0.06034, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 136ms/step - loss: 0.0405 - val_loss: 0.0603\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 193: val_loss improved from 0.06034 to 0.06017, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0402 - val_loss: 0.0602\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 194: val_loss improved from 0.06017 to 0.06000, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0399 - val_loss: 0.0600\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0397\n","Epoch 195: val_loss improved from 0.06000 to 0.05983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0397 - val_loss: 0.0598\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 196: val_loss improved from 0.05983 to 0.05967, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0394 - val_loss: 0.0597\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 197: val_loss improved from 0.05967 to 0.05952, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0392 - val_loss: 0.0595\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 198: val_loss improved from 0.05952 to 0.05936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0390 - val_loss: 0.0594\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 199: val_loss improved from 0.05936 to 0.05921, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0387 - val_loss: 0.0592\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 200: val_loss improved from 0.05921 to 0.05907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0385 - val_loss: 0.0591\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 201: val_loss improved from 0.05907 to 0.05893, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0383 - val_loss: 0.0589\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 202: val_loss improved from 0.05893 to 0.05879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0381 - val_loss: 0.0588\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 203: val_loss improved from 0.05879 to 0.05865, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0378 - val_loss: 0.0587\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 204: val_loss improved from 0.05865 to 0.05852, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0376 - val_loss: 0.0585\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 205: val_loss improved from 0.05852 to 0.05840, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0374 - val_loss: 0.0584\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 206: val_loss improved from 0.05840 to 0.05827, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0372 - val_loss: 0.0583\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 207: val_loss improved from 0.05827 to 0.05815, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0370 - val_loss: 0.0582\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 208: val_loss improved from 0.05815 to 0.05804, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0368 - val_loss: 0.0580\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 209: val_loss improved from 0.05804 to 0.05792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0366 - val_loss: 0.0579\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 210: val_loss improved from 0.05792 to 0.05781, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0365 - val_loss: 0.0578\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 211: val_loss improved from 0.05781 to 0.05771, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0363 - val_loss: 0.0577\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 212: val_loss improved from 0.05771 to 0.05760, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0361 - val_loss: 0.0576\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 213: val_loss improved from 0.05760 to 0.05750, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0359 - val_loss: 0.0575\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 214: val_loss improved from 0.05750 to 0.05740, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0357 - val_loss: 0.0574\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 215: val_loss improved from 0.05740 to 0.05731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0356 - val_loss: 0.0573\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 216: val_loss improved from 0.05731 to 0.05721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0354 - val_loss: 0.0572\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 217: val_loss improved from 0.05721 to 0.05712, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0353 - val_loss: 0.0571\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 218: val_loss improved from 0.05712 to 0.05704, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0351 - val_loss: 0.0570\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 219: val_loss improved from 0.05704 to 0.05695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0349 - val_loss: 0.0570\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 220: val_loss improved from 0.05695 to 0.05687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0348 - val_loss: 0.0569\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 221: val_loss improved from 0.05687 to 0.05679, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0346 - val_loss: 0.0568\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 222: val_loss improved from 0.05679 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0345 - val_loss: 0.0567\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 223: val_loss improved from 0.05671 to 0.05664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0344 - val_loss: 0.0566\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 224: val_loss improved from 0.05664 to 0.05657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0342 - val_loss: 0.0566\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 225: val_loss improved from 0.05657 to 0.05650, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0341 - val_loss: 0.0565\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 226: val_loss improved from 0.05650 to 0.05643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0340 - val_loss: 0.0564\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 227: val_loss improved from 0.05643 to 0.05636, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0338 - val_loss: 0.0564\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 228: val_loss improved from 0.05636 to 0.05630, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0337 - val_loss: 0.0563\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 229: val_loss improved from 0.05630 to 0.05624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0336 - val_loss: 0.0562\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 230: val_loss improved from 0.05624 to 0.05618, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0335 - val_loss: 0.0562\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 231: val_loss improved from 0.05618 to 0.05612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0333 - val_loss: 0.0561\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 232: val_loss improved from 0.05612 to 0.05607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0332 - val_loss: 0.0561\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 233: val_loss improved from 0.05607 to 0.05601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0331 - val_loss: 0.0560\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 234: val_loss improved from 0.05601 to 0.05596, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0330 - val_loss: 0.0560\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 235: val_loss improved from 0.05596 to 0.05591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0329 - val_loss: 0.0559\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 236: val_loss improved from 0.05591 to 0.05586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0328 - val_loss: 0.0559\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 237: val_loss improved from 0.05586 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0327 - val_loss: 0.0558\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 238: val_loss improved from 0.05582 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0326 - val_loss: 0.0558\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 239: val_loss improved from 0.05577 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0325 - val_loss: 0.0557\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 240: val_loss improved from 0.05573 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0324 - val_loss: 0.0557\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 241: val_loss improved from 0.05569 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0323 - val_loss: 0.0556\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 242: val_loss improved from 0.05565 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0322 - val_loss: 0.0556\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 243: val_loss improved from 0.05561 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0321 - val_loss: 0.0556\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 244: val_loss improved from 0.05557 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0320 - val_loss: 0.0555\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 245: val_loss improved from 0.05554 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0319 - val_loss: 0.0555\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 246: val_loss improved from 0.05550 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0318 - val_loss: 0.0555\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 247: val_loss improved from 0.05547 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0318 - val_loss: 0.0554\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 248: val_loss improved from 0.05544 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0317 - val_loss: 0.0554\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 249: val_loss improved from 0.05541 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0316 - val_loss: 0.0554\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 250: val_loss improved from 0.05538 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0315 - val_loss: 0.0554\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 251: val_loss improved from 0.05535 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0314 - val_loss: 0.0553\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 252: val_loss improved from 0.05533 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0314 - val_loss: 0.0553\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 253: val_loss improved from 0.05530 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0313 - val_loss: 0.0553\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 254: val_loss improved from 0.05528 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0312 - val_loss: 0.0553\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 255: val_loss improved from 0.05525 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0312 - val_loss: 0.0552\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 256: val_loss improved from 0.05523 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0311 - val_loss: 0.0552\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 257: val_loss improved from 0.05521 to 0.05519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0310 - val_loss: 0.0552\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 258: val_loss improved from 0.05519 to 0.05517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0310 - val_loss: 0.0552\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 259: val_loss improved from 0.05517 to 0.05515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0309 - val_loss: 0.0552\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 260: val_loss improved from 0.05515 to 0.05514, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0308 - val_loss: 0.0551\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 261: val_loss improved from 0.05514 to 0.05512, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0308 - val_loss: 0.0551\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 262: val_loss improved from 0.05512 to 0.05511, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0307 - val_loss: 0.0551\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 263: val_loss improved from 0.05511 to 0.05509, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0307 - val_loss: 0.0551\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 264: val_loss improved from 0.05509 to 0.05508, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0306 - val_loss: 0.0551\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 265: val_loss improved from 0.05508 to 0.05507, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0305 - val_loss: 0.0551\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 266: val_loss improved from 0.05507 to 0.05505, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0305 - val_loss: 0.0551\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 267: val_loss improved from 0.05505 to 0.05504, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0304 - val_loss: 0.0550\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 268: val_loss improved from 0.05504 to 0.05503, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0304 - val_loss: 0.0550\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 269: val_loss improved from 0.05503 to 0.05502, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0303 - val_loss: 0.0550\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 270: val_loss improved from 0.05502 to 0.05501, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0303 - val_loss: 0.0550\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 271: val_loss improved from 0.05501 to 0.05501, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0302 - val_loss: 0.0550\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 272: val_loss improved from 0.05501 to 0.05500, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0302 - val_loss: 0.0550\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 273: val_loss improved from 0.05500 to 0.05499, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0301 - val_loss: 0.0550\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 274: val_loss improved from 0.05499 to 0.05499, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0301 - val_loss: 0.0550\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 275: val_loss improved from 0.05499 to 0.05498, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0301 - val_loss: 0.0550\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 276: val_loss improved from 0.05498 to 0.05497, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0300 - val_loss: 0.0550\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 277: val_loss improved from 0.05497 to 0.05497, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0300 - val_loss: 0.0550\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 278: val_loss improved from 0.05497 to 0.05497, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0299 - val_loss: 0.0550\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 279: val_loss improved from 0.05497 to 0.05496, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0299 - val_loss: 0.0550\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 280: val_loss improved from 0.05496 to 0.05496, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0299 - val_loss: 0.0550\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 281: val_loss improved from 0.05496 to 0.05496, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0298 - val_loss: 0.0550\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 282: val_loss improved from 0.05496 to 0.05495, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0298 - val_loss: 0.0550\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 283: val_loss improved from 0.05495 to 0.05495, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0297 - val_loss: 0.0550\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 284: val_loss improved from 0.05495 to 0.05495, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0297 - val_loss: 0.0550\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 285: val_loss improved from 0.05495 to 0.05495, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0297 - val_loss: 0.0550\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 286: val_loss improved from 0.05495 to 0.05495, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0296 - val_loss: 0.0550\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 287: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0296 - val_loss: 0.0550\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 288: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0296 - val_loss: 0.0550\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 289: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0295 - val_loss: 0.0550\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 290: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0295 - val_loss: 0.0550\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 291: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0295 - val_loss: 0.0550\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 292: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0294 - val_loss: 0.0550\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 293: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0294 - val_loss: 0.0550\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 294: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0294 - val_loss: 0.0550\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 295: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0294 - val_loss: 0.0550\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 296: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0293 - val_loss: 0.0550\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 297: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0293 - val_loss: 0.0550\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 298: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0293 - val_loss: 0.0550\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 299: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - val_loss: 0.0550\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 300: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0292 - val_loss: 0.0550\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 301: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0292 - val_loss: 0.0550\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 302: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0292 - val_loss: 0.0550\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 303: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0292 - val_loss: 0.0550\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 304: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0291 - val_loss: 0.0550\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 305: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0291 - val_loss: 0.0550\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 306: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0550\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 307: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0550\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 308: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0550\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 309: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0290 - val_loss: 0.0550\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 310: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0290 - val_loss: 0.0550\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 311: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0290 - val_loss: 0.0550\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 312: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0290 - val_loss: 0.0551\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 313: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0290 - val_loss: 0.0551\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 314: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0289 - val_loss: 0.0551\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 315: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0289 - val_loss: 0.0551\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 316: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0289 - val_loss: 0.0551\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 317: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0289 - val_loss: 0.0551\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 318: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0289 - val_loss: 0.0551\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 319: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - val_loss: 0.0551\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 320: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - val_loss: 0.0551\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 321: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0288 - val_loss: 0.0551\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 322: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0288 - val_loss: 0.0551\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 323: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0288 - val_loss: 0.0551\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 324: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0288 - val_loss: 0.0551\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 325: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0288 - val_loss: 0.0551\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 326: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0288 - val_loss: 0.0552\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 327: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0288 - val_loss: 0.0552\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 328: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 329: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 330: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 331: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 332: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 333: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 334: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 335: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0287 - val_loss: 0.0552\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 336: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0286 - val_loss: 0.0552\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 337: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0286 - val_loss: 0.0552\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 338: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 339: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 340: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 341: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 342: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 343: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 344: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 345: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 346: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0286 - val_loss: 0.0553\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 347: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0285 - val_loss: 0.0553\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 348: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0285 - val_loss: 0.0553\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 349: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 350: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 351: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 352: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 353: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 354: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 355: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 356: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 357: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 358: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 359: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0285 - val_loss: 0.0554\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 360: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0284 - val_loss: 0.0554\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 361: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 362/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 362: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 363/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 363: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 364/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 364: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 365/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 365: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 366/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 366: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 367/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 367: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 368/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 368: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 369/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 369: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 370/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 370: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 371/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 371: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 372/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 372: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 373/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 373: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 119ms/step - loss: 0.0284 - val_loss: 0.0555\n","Epoch 374/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 374: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 375/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 375: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 376/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 376: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 377/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 377: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 378/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 378: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 379/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 379: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0283 - val_loss: 0.0556\n","Epoch 380/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 380: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0283 - val_loss: 0.0556\n","Epoch 381/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 381: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0283 - val_loss: 0.0556\n","Epoch 382/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 382: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0283 - val_loss: 0.0556\n","Epoch 383/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 383: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0283 - val_loss: 0.0556\n","Epoch 384/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 384: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0283 - val_loss: 0.0556\n","Epoch 385/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 385: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0283 - val_loss: 0.0556\n","Epoch 386/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 386: val_loss did not improve from 0.05495\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0283 - val_loss: 0.0556\n","1/1 [==============================] - 0s 154ms/step - loss: 0.0737\n","loss_and_metrics : 0.07374504208564758\n","1/1 [==============================] - 0s 197ms/step\n"]}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))"]}]}