{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSKaER9m23wdCxRPEF/Zsq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4ACadmPh2Fgl","executionInfo":{"status":"ok","timestamp":1695011895465,"user_tz":-540,"elapsed":42624,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"2f7efcca-ced0-47b9-ed84-0c04d89565b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3731\n","Epoch 1: val_loss improved from inf to 0.35525, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 729ms/step - loss: 0.3731 - val_loss: 0.3552\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3684\n","Epoch 2: val_loss improved from 0.35525 to 0.35086, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.3684 - val_loss: 0.3509\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3637\n","Epoch 3: val_loss improved from 0.35086 to 0.34654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.3637 - val_loss: 0.3465\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3591\n","Epoch 4: val_loss improved from 0.34654 to 0.34228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.3591 - val_loss: 0.3423\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3545\n","Epoch 5: val_loss improved from 0.34228 to 0.33809, saving model to hl5_0100.h5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 52ms/step - loss: 0.3545 - val_loss: 0.3381\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3501\n","Epoch 6: val_loss improved from 0.33809 to 0.33395, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.3501 - val_loss: 0.3340\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3456\n","Epoch 7: val_loss improved from 0.33395 to 0.32988, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3456 - val_loss: 0.3299\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3413\n","Epoch 8: val_loss improved from 0.32988 to 0.32586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.3413 - val_loss: 0.3259\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3370\n","Epoch 9: val_loss improved from 0.32586 to 0.32191, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.3370 - val_loss: 0.3219\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3328\n","Epoch 10: val_loss improved from 0.32191 to 0.31801, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.3328 - val_loss: 0.3180\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3286\n","Epoch 11: val_loss improved from 0.31801 to 0.31416, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.3286 - val_loss: 0.3142\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3245\n","Epoch 12: val_loss improved from 0.31416 to 0.31037, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.3245 - val_loss: 0.3104\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3204\n","Epoch 13: val_loss improved from 0.31037 to 0.30664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.3204 - val_loss: 0.3066\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3164\n","Epoch 14: val_loss improved from 0.30664 to 0.30295, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.3164 - val_loss: 0.3030\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3124\n","Epoch 15: val_loss improved from 0.30295 to 0.29932, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.3124 - val_loss: 0.2993\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3085\n","Epoch 16: val_loss improved from 0.29932 to 0.29574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.3085 - val_loss: 0.2957\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3047\n","Epoch 17: val_loss improved from 0.29574 to 0.29221, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.3047 - val_loss: 0.2922\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3009\n","Epoch 18: val_loss improved from 0.29221 to 0.28873, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.3009 - val_loss: 0.2887\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2972\n","Epoch 19: val_loss improved from 0.28873 to 0.28530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2972 - val_loss: 0.2853\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2935\n","Epoch 20: val_loss improved from 0.28530 to 0.28191, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2935 - val_loss: 0.2819\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2898\n","Epoch 21: val_loss improved from 0.28191 to 0.27857, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.2898 - val_loss: 0.2786\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2862\n","Epoch 22: val_loss improved from 0.27857 to 0.27528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2862 - val_loss: 0.2753\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2827\n","Epoch 23: val_loss improved from 0.27528 to 0.27203, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2827 - val_loss: 0.2720\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2792\n","Epoch 24: val_loss improved from 0.27203 to 0.26883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2792 - val_loss: 0.2688\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2757\n","Epoch 25: val_loss improved from 0.26883 to 0.26567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2757 - val_loss: 0.2657\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2723\n","Epoch 26: val_loss improved from 0.26567 to 0.26255, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.2723 - val_loss: 0.2626\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2689\n","Epoch 27: val_loss improved from 0.26255 to 0.25948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.2689 - val_loss: 0.2595\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2656\n","Epoch 28: val_loss improved from 0.25948 to 0.25644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2656 - val_loss: 0.2564\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2623\n","Epoch 29: val_loss improved from 0.25644 to 0.25345, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2623 - val_loss: 0.2534\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2591\n","Epoch 30: val_loss improved from 0.25345 to 0.25050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.2591 - val_loss: 0.2505\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2559\n","Epoch 31: val_loss improved from 0.25050 to 0.24758, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2559 - val_loss: 0.2476\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2527\n","Epoch 32: val_loss improved from 0.24758 to 0.24471, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.2527 - val_loss: 0.2447\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2496\n","Epoch 33: val_loss improved from 0.24471 to 0.24188, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 40ms/step - loss: 0.2496 - val_loss: 0.2419\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2466\n","Epoch 34: val_loss improved from 0.24188 to 0.23908, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.2466 - val_loss: 0.2391\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2435\n","Epoch 35: val_loss improved from 0.23908 to 0.23632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.2435 - val_loss: 0.2363\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2405\n","Epoch 36: val_loss improved from 0.23632 to 0.23360, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.2405 - val_loss: 0.2336\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2376\n","Epoch 37: val_loss improved from 0.23360 to 0.23091, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.2376 - val_loss: 0.2309\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2346\n","Epoch 38: val_loss improved from 0.23091 to 0.22826, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2346 - val_loss: 0.2283\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2318\n","Epoch 39: val_loss improved from 0.22826 to 0.22564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.2318 - val_loss: 0.2256\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2289\n","Epoch 40: val_loss improved from 0.22564 to 0.22306, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2289 - val_loss: 0.2231\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2261\n","Epoch 41: val_loss improved from 0.22306 to 0.22051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2261 - val_loss: 0.2205\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2233\n","Epoch 42: val_loss improved from 0.22051 to 0.21800, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.2233 - val_loss: 0.2180\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2206\n","Epoch 43: val_loss improved from 0.21800 to 0.21552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.2206 - val_loss: 0.2155\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2179\n","Epoch 44: val_loss improved from 0.21552 to 0.21307, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2179 - val_loss: 0.2131\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2152\n","Epoch 45: val_loss improved from 0.21307 to 0.21066, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.2152 - val_loss: 0.2107\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2126\n","Epoch 46: val_loss improved from 0.21066 to 0.20827, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.2126 - val_loss: 0.2083\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 47: val_loss improved from 0.20827 to 0.20592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 173ms/step - loss: 0.2100 - val_loss: 0.2059\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2074\n","Epoch 48: val_loss improved from 0.20592 to 0.20360, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2074 - val_loss: 0.2036\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2048\n","Epoch 49: val_loss improved from 0.20360 to 0.20131, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2048 - val_loss: 0.2013\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2023\n","Epoch 50: val_loss improved from 0.20131 to 0.19905, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.2023 - val_loss: 0.1990\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1999\n","Epoch 51: val_loss improved from 0.19905 to 0.19682, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1999 - val_loss: 0.1968\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1974\n","Epoch 52: val_loss improved from 0.19682 to 0.19462, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1974 - val_loss: 0.1946\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1950\n","Epoch 53: val_loss improved from 0.19462 to 0.19244, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1950 - val_loss: 0.1924\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1926\n","Epoch 54: val_loss improved from 0.19244 to 0.19030, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1926 - val_loss: 0.1903\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1902\n","Epoch 55: val_loss improved from 0.19030 to 0.18819, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1902 - val_loss: 0.1882\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1879\n","Epoch 56: val_loss improved from 0.18819 to 0.18610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1879 - val_loss: 0.1861\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1856\n","Epoch 57: val_loss improved from 0.18610 to 0.18404, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1856 - val_loss: 0.1840\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1834\n","Epoch 58: val_loss improved from 0.18404 to 0.18201, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 155ms/step - loss: 0.1834 - val_loss: 0.1820\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1811\n","Epoch 59: val_loss improved from 0.18201 to 0.18000, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 277ms/step - loss: 0.1811 - val_loss: 0.1800\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1789\n","Epoch 60: val_loss improved from 0.18000 to 0.17802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 147ms/step - loss: 0.1789 - val_loss: 0.1780\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1767\n","Epoch 61: val_loss improved from 0.17802 to 0.17607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1767 - val_loss: 0.1761\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1746\n","Epoch 62: val_loss improved from 0.17607 to 0.17414, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1746 - val_loss: 0.1741\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1724\n","Epoch 63: val_loss improved from 0.17414 to 0.17224, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1724 - val_loss: 0.1722\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1703\n","Epoch 64: val_loss improved from 0.17224 to 0.17037, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1703 - val_loss: 0.1704\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1682\n","Epoch 65: val_loss improved from 0.17037 to 0.16852, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1682 - val_loss: 0.1685\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1662\n","Epoch 66: val_loss improved from 0.16852 to 0.16669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1662 - val_loss: 0.1667\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1642\n","Epoch 67: val_loss improved from 0.16669 to 0.16489, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1642 - val_loss: 0.1649\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1622\n","Epoch 68: val_loss improved from 0.16489 to 0.16311, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1622 - val_loss: 0.1631\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1602\n","Epoch 69: val_loss improved from 0.16311 to 0.16136, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.1602 - val_loss: 0.1614\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1582\n","Epoch 70: val_loss improved from 0.16136 to 0.15963, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 219ms/step - loss: 0.1582 - val_loss: 0.1596\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1563\n","Epoch 71: val_loss improved from 0.15963 to 0.15792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 163ms/step - loss: 0.1563 - val_loss: 0.1579\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1544\n","Epoch 72: val_loss improved from 0.15792 to 0.15624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1544 - val_loss: 0.1562\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1525\n","Epoch 73: val_loss improved from 0.15624 to 0.15458, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1525 - val_loss: 0.1546\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1507\n","Epoch 74: val_loss improved from 0.15458 to 0.15294, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1507 - val_loss: 0.1529\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1488\n","Epoch 75: val_loss improved from 0.15294 to 0.15132, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1488 - val_loss: 0.1513\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1470\n","Epoch 76: val_loss improved from 0.15132 to 0.14973, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 39ms/step - loss: 0.1470 - val_loss: 0.1497\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1453\n","Epoch 77: val_loss improved from 0.14973 to 0.14815, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 40ms/step - loss: 0.1453 - val_loss: 0.1482\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1435\n","Epoch 78: val_loss improved from 0.14815 to 0.14660, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 40ms/step - loss: 0.1435 - val_loss: 0.1466\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1418\n","Epoch 79: val_loss improved from 0.14660 to 0.14507, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1418 - val_loss: 0.1451\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1400\n","Epoch 80: val_loss improved from 0.14507 to 0.14356, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1400 - val_loss: 0.1436\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1383\n","Epoch 81: val_loss improved from 0.14356 to 0.14207, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1383 - val_loss: 0.1421\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1367\n","Epoch 82: val_loss improved from 0.14207 to 0.14061, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.1367 - val_loss: 0.1406\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1350\n","Epoch 83: val_loss improved from 0.14061 to 0.13916, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1350 - val_loss: 0.1392\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1334\n","Epoch 84: val_loss improved from 0.13916 to 0.13773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1334 - val_loss: 0.1377\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1318\n","Epoch 85: val_loss improved from 0.13773 to 0.13632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1318 - val_loss: 0.1363\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1302\n","Epoch 86: val_loss improved from 0.13632 to 0.13493, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1302 - val_loss: 0.1349\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1286\n","Epoch 87: val_loss improved from 0.13493 to 0.13356, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1286 - val_loss: 0.1336\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1271\n","Epoch 88: val_loss improved from 0.13356 to 0.13221, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1271 - val_loss: 0.1322\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1255\n","Epoch 89: val_loss improved from 0.13221 to 0.13088, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.1255 - val_loss: 0.1309\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1240\n","Epoch 90: val_loss improved from 0.13088 to 0.12957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1240 - val_loss: 0.1296\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1225\n","Epoch 91: val_loss improved from 0.12957 to 0.12828, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.1225 - val_loss: 0.1283\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1211\n","Epoch 92: val_loss improved from 0.12828 to 0.12700, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1211 - val_loss: 0.1270\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1196\n","Epoch 93: val_loss improved from 0.12700 to 0.12574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1196 - val_loss: 0.1257\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1182\n","Epoch 94: val_loss improved from 0.12574 to 0.12450, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1182 - val_loss: 0.1245\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1168\n","Epoch 95: val_loss improved from 0.12450 to 0.12328, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1168 - val_loss: 0.1233\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1154\n","Epoch 96: val_loss improved from 0.12328 to 0.12208, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.1154 - val_loss: 0.1221\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1140\n","Epoch 97: val_loss improved from 0.12208 to 0.12089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.1140 - val_loss: 0.1209\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1126\n","Epoch 98: val_loss improved from 0.12089 to 0.11972, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 41ms/step - loss: 0.1126 - val_loss: 0.1197\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1113\n","Epoch 99: val_loss improved from 0.11972 to 0.11857, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.1113 - val_loss: 0.1186\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1100\n","Epoch 100: val_loss improved from 0.11857 to 0.11743, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1100 - val_loss: 0.1174\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1087\n","Epoch 101: val_loss improved from 0.11743 to 0.11631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 40ms/step - loss: 0.1087 - val_loss: 0.1163\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1074\n","Epoch 102: val_loss improved from 0.11631 to 0.11521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1074 - val_loss: 0.1152\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1061\n","Epoch 103: val_loss improved from 0.11521 to 0.11412, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.1061 - val_loss: 0.1141\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1049\n","Epoch 104: val_loss improved from 0.11412 to 0.11305, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.1049 - val_loss: 0.1130\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1036\n","Epoch 105: val_loss improved from 0.11305 to 0.11199, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.1036 - val_loss: 0.1120\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1024\n","Epoch 106: val_loss improved from 0.11199 to 0.11095, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.1024 - val_loss: 0.1110\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1012\n","Epoch 107: val_loss improved from 0.11095 to 0.10993, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.1012 - val_loss: 0.1099\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1000\n","Epoch 108: val_loss improved from 0.10993 to 0.10892, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.1000 - val_loss: 0.1089\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0989\n","Epoch 109: val_loss improved from 0.10892 to 0.10792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0989 - val_loss: 0.1079\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0977\n","Epoch 110: val_loss improved from 0.10792 to 0.10694, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0977 - val_loss: 0.1069\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0966\n","Epoch 111: val_loss improved from 0.10694 to 0.10598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0966 - val_loss: 0.1060\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0954\n","Epoch 112: val_loss improved from 0.10598 to 0.10503, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0954 - val_loss: 0.1050\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0943\n","Epoch 113: val_loss improved from 0.10503 to 0.10409, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0943 - val_loss: 0.1041\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0932\n","Epoch 114: val_loss improved from 0.10409 to 0.10317, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0932 - val_loss: 0.1032\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0922\n","Epoch 115: val_loss improved from 0.10317 to 0.10226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0922 - val_loss: 0.1023\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0911\n","Epoch 116: val_loss improved from 0.10226 to 0.10137, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0911 - val_loss: 0.1014\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0901\n","Epoch 117: val_loss improved from 0.10137 to 0.10049, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0901 - val_loss: 0.1005\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0890\n","Epoch 118: val_loss improved from 0.10049 to 0.09962, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0890 - val_loss: 0.0996\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0880\n","Epoch 119: val_loss improved from 0.09962 to 0.09877, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0880 - val_loss: 0.0988\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0870\n","Epoch 120: val_loss improved from 0.09877 to 0.09793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0870 - val_loss: 0.0979\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0860\n","Epoch 121: val_loss improved from 0.09793 to 0.09710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0860 - val_loss: 0.0971\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0850\n","Epoch 122: val_loss improved from 0.09710 to 0.09629, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0850 - val_loss: 0.0963\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0841\n","Epoch 123: val_loss improved from 0.09629 to 0.09549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0841 - val_loss: 0.0955\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0831\n","Epoch 124: val_loss improved from 0.09549 to 0.09470, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0831 - val_loss: 0.0947\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0822\n","Epoch 125: val_loss improved from 0.09470 to 0.09393, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0822 - val_loss: 0.0939\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0813\n","Epoch 126: val_loss improved from 0.09393 to 0.09317, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0813 - val_loss: 0.0932\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0804\n","Epoch 127: val_loss improved from 0.09317 to 0.09241, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0804 - val_loss: 0.0924\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0795\n","Epoch 128: val_loss improved from 0.09241 to 0.09168, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0795 - val_loss: 0.0917\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0786\n","Epoch 129: val_loss improved from 0.09168 to 0.09095, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0786 - val_loss: 0.0909\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0777\n","Epoch 130: val_loss improved from 0.09095 to 0.09023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0777 - val_loss: 0.0902\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0769\n","Epoch 131: val_loss improved from 0.09023 to 0.08953, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0769 - val_loss: 0.0895\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0760\n","Epoch 132: val_loss improved from 0.08953 to 0.08884, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0760 - val_loss: 0.0888\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0752\n","Epoch 133: val_loss improved from 0.08884 to 0.08816, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0752 - val_loss: 0.0882\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0744\n","Epoch 134: val_loss improved from 0.08816 to 0.08749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0744 - val_loss: 0.0875\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0736\n","Epoch 135: val_loss improved from 0.08749 to 0.08683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0736 - val_loss: 0.0868\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0728\n","Epoch 136: val_loss improved from 0.08683 to 0.08619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0728 - val_loss: 0.0862\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0720\n","Epoch 137: val_loss improved from 0.08619 to 0.08555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0720 - val_loss: 0.0855\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0712\n","Epoch 138: val_loss improved from 0.08555 to 0.08492, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0712 - val_loss: 0.0849\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0705\n","Epoch 139: val_loss improved from 0.08492 to 0.08431, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0705 - val_loss: 0.0843\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0697\n","Epoch 140: val_loss improved from 0.08431 to 0.08370, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0697 - val_loss: 0.0837\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0690\n","Epoch 141: val_loss improved from 0.08370 to 0.08311, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0690 - val_loss: 0.0831\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0683\n","Epoch 142: val_loss improved from 0.08311 to 0.08253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0683 - val_loss: 0.0825\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0675\n","Epoch 143: val_loss improved from 0.08253 to 0.08195, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0675 - val_loss: 0.0820\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0668\n","Epoch 144: val_loss improved from 0.08195 to 0.08139, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0668 - val_loss: 0.0814\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0661\n","Epoch 145: val_loss improved from 0.08139 to 0.08083, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0661 - val_loss: 0.0808\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0655\n","Epoch 146: val_loss improved from 0.08083 to 0.08029, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0655 - val_loss: 0.0803\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0648\n","Epoch 147: val_loss improved from 0.08029 to 0.07975, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0648 - val_loss: 0.0798\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0641\n","Epoch 148: val_loss improved from 0.07975 to 0.07923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0641 - val_loss: 0.0792\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0635\n","Epoch 149: val_loss improved from 0.07923 to 0.07871, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0635 - val_loss: 0.0787\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0628\n","Epoch 150: val_loss improved from 0.07871 to 0.07820, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0628 - val_loss: 0.0782\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0622\n","Epoch 151: val_loss improved from 0.07820 to 0.07771, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0622 - val_loss: 0.0777\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0616\n","Epoch 152: val_loss improved from 0.07771 to 0.07722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0616 - val_loss: 0.0772\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0610\n","Epoch 153: val_loss improved from 0.07722 to 0.07674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0610 - val_loss: 0.0767\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0604\n","Epoch 154: val_loss improved from 0.07674 to 0.07626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0604 - val_loss: 0.0763\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0598\n","Epoch 155: val_loss improved from 0.07626 to 0.07580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0598 - val_loss: 0.0758\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0592\n","Epoch 156: val_loss improved from 0.07580 to 0.07535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0592 - val_loss: 0.0753\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0586\n","Epoch 157: val_loss improved from 0.07535 to 0.07490, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0586 - val_loss: 0.0749\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0581\n","Epoch 158: val_loss improved from 0.07490 to 0.07446, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0581 - val_loss: 0.0745\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0575\n","Epoch 159: val_loss improved from 0.07446 to 0.07403, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0575 - val_loss: 0.0740\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0569\n","Epoch 160: val_loss improved from 0.07403 to 0.07361, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0569 - val_loss: 0.0736\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0564\n","Epoch 161: val_loss improved from 0.07361 to 0.07319, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0564 - val_loss: 0.0732\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0559\n","Epoch 162: val_loss improved from 0.07319 to 0.07279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0559 - val_loss: 0.0728\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0554\n","Epoch 163: val_loss improved from 0.07279 to 0.07239, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0554 - val_loss: 0.0724\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0548\n","Epoch 164: val_loss improved from 0.07239 to 0.07200, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0548 - val_loss: 0.0720\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0543\n","Epoch 165: val_loss improved from 0.07200 to 0.07161, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0543 - val_loss: 0.0716\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0538\n","Epoch 166: val_loss improved from 0.07161 to 0.07124, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0538 - val_loss: 0.0712\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0534\n","Epoch 167: val_loss improved from 0.07124 to 0.07087, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0534 - val_loss: 0.0709\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0529\n","Epoch 168: val_loss improved from 0.07087 to 0.07050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0529 - val_loss: 0.0705\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 169: val_loss improved from 0.07050 to 0.07015, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0524 - val_loss: 0.0701\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0519\n","Epoch 170: val_loss improved from 0.07015 to 0.06980, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0519 - val_loss: 0.0698\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 171: val_loss improved from 0.06980 to 0.06946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0515 - val_loss: 0.0695\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0510\n","Epoch 172: val_loss improved from 0.06946 to 0.06913, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0510 - val_loss: 0.0691\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0506\n","Epoch 173: val_loss improved from 0.06913 to 0.06880, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0506 - val_loss: 0.0688\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0502\n","Epoch 174: val_loss improved from 0.06880 to 0.06848, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0502 - val_loss: 0.0685\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 175: val_loss improved from 0.06848 to 0.06816, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0497 - val_loss: 0.0682\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0493\n","Epoch 176: val_loss improved from 0.06816 to 0.06785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0493 - val_loss: 0.0679\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0489\n","Epoch 177: val_loss improved from 0.06785 to 0.06755, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0489 - val_loss: 0.0675\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0485\n","Epoch 178: val_loss improved from 0.06755 to 0.06725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0485 - val_loss: 0.0673\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0481\n","Epoch 179: val_loss improved from 0.06725 to 0.06696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0481 - val_loss: 0.0670\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0477\n","Epoch 180: val_loss improved from 0.06696 to 0.06668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0477 - val_loss: 0.0667\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0473\n","Epoch 181: val_loss improved from 0.06668 to 0.06640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0473 - val_loss: 0.0664\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0470\n","Epoch 182: val_loss improved from 0.06640 to 0.06613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0470 - val_loss: 0.0661\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0466\n","Epoch 183: val_loss improved from 0.06613 to 0.06586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0466 - val_loss: 0.0659\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 184: val_loss improved from 0.06586 to 0.06560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0462 - val_loss: 0.0656\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0459\n","Epoch 185: val_loss improved from 0.06560 to 0.06534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0459 - val_loss: 0.0653\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 186: val_loss improved from 0.06534 to 0.06509, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0455 - val_loss: 0.0651\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0452\n","Epoch 187: val_loss improved from 0.06509 to 0.06485, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0452 - val_loss: 0.0648\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0448\n","Epoch 188: val_loss improved from 0.06485 to 0.06461, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0448 - val_loss: 0.0646\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0445\n","Epoch 189: val_loss improved from 0.06461 to 0.06437, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0445 - val_loss: 0.0644\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 190: val_loss improved from 0.06437 to 0.06414, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0442 - val_loss: 0.0641\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0438\n","Epoch 191: val_loss improved from 0.06414 to 0.06392, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0438 - val_loss: 0.0639\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 192: val_loss improved from 0.06392 to 0.06370, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0435 - val_loss: 0.0637\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 193: val_loss improved from 0.06370 to 0.06348, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0432 - val_loss: 0.0635\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 194: val_loss improved from 0.06348 to 0.06327, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0429 - val_loss: 0.0633\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0426\n","Epoch 195: val_loss improved from 0.06327 to 0.06306, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0426 - val_loss: 0.0631\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0423\n","Epoch 196: val_loss improved from 0.06306 to 0.06286, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0423 - val_loss: 0.0629\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 197: val_loss improved from 0.06286 to 0.06267, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0420 - val_loss: 0.0627\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 198: val_loss improved from 0.06267 to 0.06247, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0417 - val_loss: 0.0625\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 199: val_loss improved from 0.06247 to 0.06229, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0414 - val_loss: 0.0623\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0412\n","Epoch 200: val_loss improved from 0.06229 to 0.06210, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0412 - val_loss: 0.0621\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 201: val_loss improved from 0.06210 to 0.06192, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0409 - val_loss: 0.0619\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 202: val_loss improved from 0.06192 to 0.06175, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0406 - val_loss: 0.0617\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0404\n","Epoch 203: val_loss improved from 0.06175 to 0.06158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0404 - val_loss: 0.0616\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0401\n","Epoch 204: val_loss improved from 0.06158 to 0.06141, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0401 - val_loss: 0.0614\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 205: val_loss improved from 0.06141 to 0.06125, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0399 - val_loss: 0.0612\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 206: val_loss improved from 0.06125 to 0.06109, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0396 - val_loss: 0.0611\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 207: val_loss improved from 0.06109 to 0.06093, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0394 - val_loss: 0.0609\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 208: val_loss improved from 0.06093 to 0.06078, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0392 - val_loss: 0.0608\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 209: val_loss improved from 0.06078 to 0.06063, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0389 - val_loss: 0.0606\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 210: val_loss improved from 0.06063 to 0.06049, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0387 - val_loss: 0.0605\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 211: val_loss improved from 0.06049 to 0.06034, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0385 - val_loss: 0.0603\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 212: val_loss improved from 0.06034 to 0.06021, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0383 - val_loss: 0.0602\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 213: val_loss improved from 0.06021 to 0.06007, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0380 - val_loss: 0.0601\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 214: val_loss improved from 0.06007 to 0.05994, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0378 - val_loss: 0.0599\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 215: val_loss improved from 0.05994 to 0.05981, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0376 - val_loss: 0.0598\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 216: val_loss improved from 0.05981 to 0.05969, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0374 - val_loss: 0.0597\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 217: val_loss improved from 0.05969 to 0.05957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0372 - val_loss: 0.0596\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 218: val_loss improved from 0.05957 to 0.05945, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0370 - val_loss: 0.0594\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 219: val_loss improved from 0.05945 to 0.05933, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0368 - val_loss: 0.0593\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 220: val_loss improved from 0.05933 to 0.05922, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0366 - val_loss: 0.0592\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 221: val_loss improved from 0.05922 to 0.05911, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0365 - val_loss: 0.0591\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 222: val_loss improved from 0.05911 to 0.05901, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0363 - val_loss: 0.0590\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 223: val_loss improved from 0.05901 to 0.05890, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0361 - val_loss: 0.0589\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 224: val_loss improved from 0.05890 to 0.05880, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0359 - val_loss: 0.0588\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 225: val_loss improved from 0.05880 to 0.05870, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0358 - val_loss: 0.0587\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 226: val_loss improved from 0.05870 to 0.05861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0356 - val_loss: 0.0586\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 227: val_loss improved from 0.05861 to 0.05851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0354 - val_loss: 0.0585\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 228: val_loss improved from 0.05851 to 0.05842, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0353 - val_loss: 0.0584\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 229: val_loss improved from 0.05842 to 0.05833, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0351 - val_loss: 0.0583\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 230: val_loss improved from 0.05833 to 0.05825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0349 - val_loss: 0.0582\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 231: val_loss improved from 0.05825 to 0.05817, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0348 - val_loss: 0.0582\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 232: val_loss improved from 0.05817 to 0.05808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0346 - val_loss: 0.0581\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 233: val_loss improved from 0.05808 to 0.05801, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0345 - val_loss: 0.0580\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 234: val_loss improved from 0.05801 to 0.05793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0344 - val_loss: 0.0579\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 235: val_loss improved from 0.05793 to 0.05785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0342 - val_loss: 0.0579\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 236: val_loss improved from 0.05785 to 0.05778, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0341 - val_loss: 0.0578\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 237: val_loss improved from 0.05778 to 0.05771, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0339 - val_loss: 0.0577\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 238: val_loss improved from 0.05771 to 0.05764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0338 - val_loss: 0.0576\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 239: val_loss improved from 0.05764 to 0.05758, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0337 - val_loss: 0.0576\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 240: val_loss improved from 0.05758 to 0.05751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0336 - val_loss: 0.0575\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 241: val_loss improved from 0.05751 to 0.05745, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0334 - val_loss: 0.0575\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 242: val_loss improved from 0.05745 to 0.05739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0333 - val_loss: 0.0574\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 243: val_loss improved from 0.05739 to 0.05733, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0332 - val_loss: 0.0573\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 244: val_loss improved from 0.05733 to 0.05728, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0331 - val_loss: 0.0573\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 245: val_loss improved from 0.05728 to 0.05722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0330 - val_loss: 0.0572\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 246: val_loss improved from 0.05722 to 0.05717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0328 - val_loss: 0.0572\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 247: val_loss improved from 0.05717 to 0.05712, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0327 - val_loss: 0.0571\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 248: val_loss improved from 0.05712 to 0.05707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0326 - val_loss: 0.0571\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 249: val_loss improved from 0.05707 to 0.05702, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0325 - val_loss: 0.0570\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 250: val_loss improved from 0.05702 to 0.05697, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0324 - val_loss: 0.0570\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 251: val_loss improved from 0.05697 to 0.05693, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0323 - val_loss: 0.0569\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 252: val_loss improved from 0.05693 to 0.05689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0322 - val_loss: 0.0569\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 253: val_loss improved from 0.05689 to 0.05684, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0321 - val_loss: 0.0568\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 254: val_loss improved from 0.05684 to 0.05680, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0320 - val_loss: 0.0568\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 255: val_loss improved from 0.05680 to 0.05676, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0319 - val_loss: 0.0568\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 256: val_loss improved from 0.05676 to 0.05673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0318 - val_loss: 0.0567\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 257: val_loss improved from 0.05673 to 0.05669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0318 - val_loss: 0.0567\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 258: val_loss improved from 0.05669 to 0.05666, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0317 - val_loss: 0.0567\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 259: val_loss improved from 0.05666 to 0.05662, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0316 - val_loss: 0.0566\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 260: val_loss improved from 0.05662 to 0.05659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0315 - val_loss: 0.0566\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 261: val_loss improved from 0.05659 to 0.05656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0314 - val_loss: 0.0566\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 262: val_loss improved from 0.05656 to 0.05653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0313 - val_loss: 0.0565\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 263: val_loss improved from 0.05653 to 0.05650, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0313 - val_loss: 0.0565\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 264: val_loss improved from 0.05650 to 0.05647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0312 - val_loss: 0.0565\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 265: val_loss improved from 0.05647 to 0.05644, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0311 - val_loss: 0.0564\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 266: val_loss improved from 0.05644 to 0.05642, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0310 - val_loss: 0.0564\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 267: val_loss improved from 0.05642 to 0.05639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0310 - val_loss: 0.0564\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 268: val_loss improved from 0.05639 to 0.05637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0309 - val_loss: 0.0564\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 269: val_loss improved from 0.05637 to 0.05635, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0308 - val_loss: 0.0563\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 270: val_loss improved from 0.05635 to 0.05633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0307 - val_loss: 0.0563\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 271: val_loss improved from 0.05633 to 0.05631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0307 - val_loss: 0.0563\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 272: val_loss improved from 0.05631 to 0.05629, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0306 - val_loss: 0.0563\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 273: val_loss improved from 0.05629 to 0.05627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0305 - val_loss: 0.0563\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 274: val_loss improved from 0.05627 to 0.05625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0305 - val_loss: 0.0563\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 275: val_loss improved from 0.05625 to 0.05623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0304 - val_loss: 0.0562\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 276: val_loss improved from 0.05623 to 0.05622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0304 - val_loss: 0.0562\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 277: val_loss improved from 0.05622 to 0.05620, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0303 - val_loss: 0.0562\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 278: val_loss improved from 0.05620 to 0.05619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0302 - val_loss: 0.0562\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 279: val_loss improved from 0.05619 to 0.05617, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0302 - val_loss: 0.0562\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 280: val_loss improved from 0.05617 to 0.05616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0301 - val_loss: 0.0562\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 281: val_loss improved from 0.05616 to 0.05615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0301 - val_loss: 0.0561\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 282: val_loss improved from 0.05615 to 0.05614, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0300 - val_loss: 0.0561\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 283: val_loss improved from 0.05614 to 0.05613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0300 - val_loss: 0.0561\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 284: val_loss improved from 0.05613 to 0.05612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0299 - val_loss: 0.0561\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 285: val_loss improved from 0.05612 to 0.05611, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0299 - val_loss: 0.0561\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 286: val_loss improved from 0.05611 to 0.05610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0298 - val_loss: 0.0561\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 287: val_loss improved from 0.05610 to 0.05609, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0298 - val_loss: 0.0561\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 288: val_loss improved from 0.05609 to 0.05608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0297 - val_loss: 0.0561\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 289: val_loss improved from 0.05608 to 0.05608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0297 - val_loss: 0.0561\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 290: val_loss improved from 0.05608 to 0.05607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0296 - val_loss: 0.0561\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 291: val_loss improved from 0.05607 to 0.05606, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0296 - val_loss: 0.0561\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 292: val_loss improved from 0.05606 to 0.05606, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0296 - val_loss: 0.0561\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 293: val_loss improved from 0.05606 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0295 - val_loss: 0.0561\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 294: val_loss improved from 0.05605 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0295 - val_loss: 0.0560\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 295: val_loss improved from 0.05605 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0294 - val_loss: 0.0560\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 296: val_loss improved from 0.05605 to 0.05604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0294 - val_loss: 0.0560\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 297: val_loss improved from 0.05604 to 0.05604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0294 - val_loss: 0.0560\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 298: val_loss improved from 0.05604 to 0.05604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0293 - val_loss: 0.0560\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 299: val_loss improved from 0.05604 to 0.05604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0293 - val_loss: 0.0560\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 300: val_loss improved from 0.05604 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0293 - val_loss: 0.0560\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 301: val_loss improved from 0.05603 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0292 - val_loss: 0.0560\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 302: val_loss improved from 0.05603 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0292 - val_loss: 0.0560\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 303: val_loss improved from 0.05603 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0291 - val_loss: 0.0560\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 304: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0291 - val_loss: 0.0560\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 305: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0291 - val_loss: 0.0560\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 306: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0291 - val_loss: 0.0560\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 307: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0290 - val_loss: 0.0560\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 308: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0290 - val_loss: 0.0560\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 309: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0290 - val_loss: 0.0560\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 310: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0289 - val_loss: 0.0560\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 311: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0289 - val_loss: 0.0560\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 312: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0289 - val_loss: 0.0560\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 313: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0289 - val_loss: 0.0560\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 314: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0288 - val_loss: 0.0561\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 315: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0288 - val_loss: 0.0561\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 316: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0288 - val_loss: 0.0561\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 317: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0287 - val_loss: 0.0561\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 318: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0287 - val_loss: 0.0561\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 319: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0287 - val_loss: 0.0561\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 320: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0287 - val_loss: 0.0561\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 321: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0287 - val_loss: 0.0561\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 322: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0286 - val_loss: 0.0561\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 323: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0286 - val_loss: 0.0561\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 324: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0286 - val_loss: 0.0561\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 325: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0286 - val_loss: 0.0561\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 326: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0286 - val_loss: 0.0561\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 327: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0285 - val_loss: 0.0561\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 328: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0285 - val_loss: 0.0561\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 329: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0285 - val_loss: 0.0561\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 330: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0285 - val_loss: 0.0561\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 331: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0285 - val_loss: 0.0561\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 332: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0284 - val_loss: 0.0561\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 333: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0284 - val_loss: 0.0562\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 334: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0284 - val_loss: 0.0562\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 335: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0284 - val_loss: 0.0562\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 336: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0284 - val_loss: 0.0562\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 337: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0284 - val_loss: 0.0562\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 338: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0283 - val_loss: 0.0562\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 339: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0283 - val_loss: 0.0562\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 340: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0283 - val_loss: 0.0562\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 341: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0283 - val_loss: 0.0562\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 342: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0283 - val_loss: 0.0562\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 343: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0283 - val_loss: 0.0562\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 344: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0283 - val_loss: 0.0562\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 345: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0282 - val_loss: 0.0562\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 346: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0282 - val_loss: 0.0563\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 347: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0282 - val_loss: 0.0563\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 348: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0282 - val_loss: 0.0563\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 349: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0282 - val_loss: 0.0563\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 350: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0282 - val_loss: 0.0563\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 351: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0282 - val_loss: 0.0563\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 352: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0282 - val_loss: 0.0563\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 353: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0281 - val_loss: 0.0563\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 354: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0281 - val_loss: 0.0563\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 355: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0281 - val_loss: 0.0563\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 356: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0281 - val_loss: 0.0563\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 357: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0281 - val_loss: 0.0563\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 358: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0281 - val_loss: 0.0564\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 359: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0281 - val_loss: 0.0564\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 360: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0281 - val_loss: 0.0564\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 361: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0281 - val_loss: 0.0564\n","Epoch 362/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 362: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0281 - val_loss: 0.0564\n","Epoch 363/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 363: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0280 - val_loss: 0.0564\n","Epoch 364/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 364: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0280 - val_loss: 0.0564\n","Epoch 365/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 365: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0280 - val_loss: 0.0564\n","Epoch 366/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 366: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0280 - val_loss: 0.0564\n","Epoch 367/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 367: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0280 - val_loss: 0.0564\n","Epoch 368/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 368: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0280 - val_loss: 0.0564\n","Epoch 369/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 369: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0280 - val_loss: 0.0564\n","Epoch 370/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 370: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 371/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 371: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 372/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 372: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 373/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 373: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 374/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 374: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0280 - val_loss: 0.0565\n","Epoch 375/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 375: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 376/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 376: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 377/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 377: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 378/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 378: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 379/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 379: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 380/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 380: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 381/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 381: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0279 - val_loss: 0.0565\n","Epoch 382/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 382: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 383/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 383: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 27ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 384/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 384: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 385/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 385: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 386/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 386: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 387/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 387: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 388/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 388: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 389/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 389: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 32ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 390/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 390: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0279 - val_loss: 0.0566\n","Epoch 391/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 391: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0278 - val_loss: 0.0566\n","Epoch 392/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 392: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0278 - val_loss: 0.0566\n","Epoch 393/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 393: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 35ms/step - loss: 0.0278 - val_loss: 0.0566\n","Epoch 394/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 394: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 395/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 395: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 28ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 396/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 396: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 397/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 397: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 34ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 398/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 398: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 30ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 399/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 399: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 400/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 400: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 29ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 401/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 401: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 31ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 402/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 402: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 33ms/step - loss: 0.0278 - val_loss: 0.0567\n","Epoch 403/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 403: val_loss did not improve from 0.05603\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0278 - val_loss: 0.0567\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0740\n","loss_and_metrics : 0.0740312784910202\n","1/1 [==============================] - 0s 76ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"elu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}