{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMiUUlaioiG+yPBFfoeHZzO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5vG9jEOv65-p","executionInfo":{"status":"ok","timestamp":1695118417482,"user_tz":-540,"elapsed":57518,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"187f0906-5330-467c-a3ac-02c942a1ca00"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3669\n","Epoch 1: val_loss improved from inf to 0.34947, saving model to hl5_0100.h5\n","1/1 [==============================] - 3s 3s/step - loss: 0.3669 - val_loss: 0.3495\n","Epoch 2/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3630\n","Epoch 2: val_loss improved from 0.34947 to 0.34592, saving model to hl5_0100.h5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 188ms/step - loss: 0.3630 - val_loss: 0.3459\n","Epoch 3/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3592\n","Epoch 3: val_loss improved from 0.34592 to 0.34244, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 160ms/step - loss: 0.3592 - val_loss: 0.3424\n","Epoch 4/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3555\n","Epoch 4: val_loss improved from 0.34244 to 0.33901, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 173ms/step - loss: 0.3555 - val_loss: 0.3390\n","Epoch 5/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3518\n","Epoch 5: val_loss improved from 0.33901 to 0.33563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3518 - val_loss: 0.3356\n","Epoch 6/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3482\n","Epoch 6: val_loss improved from 0.33563 to 0.33231, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3482 - val_loss: 0.3323\n","Epoch 7/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3446\n","Epoch 7: val_loss improved from 0.33231 to 0.32903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3446 - val_loss: 0.3290\n","Epoch 8/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3411\n","Epoch 8: val_loss improved from 0.32903 to 0.32579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3411 - val_loss: 0.3258\n","Epoch 9/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3377\n","Epoch 9: val_loss improved from 0.32579 to 0.32259, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3377 - val_loss: 0.3226\n","Epoch 10/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3342\n","Epoch 10: val_loss improved from 0.32259 to 0.31942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3342 - val_loss: 0.3194\n","Epoch 11/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3308\n","Epoch 11: val_loss improved from 0.31942 to 0.31628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3308 - val_loss: 0.3163\n","Epoch 12/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3275\n","Epoch 12: val_loss improved from 0.31628 to 0.31318, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.3275 - val_loss: 0.3132\n","Epoch 13/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3242\n","Epoch 13: val_loss improved from 0.31318 to 0.31011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3242 - val_loss: 0.3101\n","Epoch 14/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3209\n","Epoch 14: val_loss improved from 0.31011 to 0.30707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3209 - val_loss: 0.3071\n","Epoch 15/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3176\n","Epoch 15: val_loss improved from 0.30707 to 0.30406, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3176 - val_loss: 0.3041\n","Epoch 16/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3144\n","Epoch 16: val_loss improved from 0.30406 to 0.30107, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3144 - val_loss: 0.3011\n","Epoch 17/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3112\n","Epoch 17: val_loss improved from 0.30107 to 0.29811, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3112 - val_loss: 0.2981\n","Epoch 18/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3080\n","Epoch 18: val_loss improved from 0.29811 to 0.29519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.3080 - val_loss: 0.2952\n","Epoch 19/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3048\n","Epoch 19: val_loss improved from 0.29519 to 0.29229, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.3048 - val_loss: 0.2923\n","Epoch 20/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3017\n","Epoch 20: val_loss improved from 0.29229 to 0.28943, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.3017 - val_loss: 0.2894\n","Epoch 21/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2986\n","Epoch 21: val_loss improved from 0.28943 to 0.28660, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2986 - val_loss: 0.2866\n","Epoch 22/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2956\n","Epoch 22: val_loss improved from 0.28660 to 0.28380, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2956 - val_loss: 0.2838\n","Epoch 23/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2926\n","Epoch 23: val_loss improved from 0.28380 to 0.28104, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2926 - val_loss: 0.2810\n","Epoch 24/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2896\n","Epoch 24: val_loss improved from 0.28104 to 0.27831, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2896 - val_loss: 0.2783\n","Epoch 25/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2867\n","Epoch 25: val_loss improved from 0.27831 to 0.27561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.2867 - val_loss: 0.2756\n","Epoch 26/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2837\n","Epoch 26: val_loss improved from 0.27561 to 0.27294, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2837 - val_loss: 0.2729\n","Epoch 27/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2809\n","Epoch 27: val_loss improved from 0.27294 to 0.27030, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2809 - val_loss: 0.2703\n","Epoch 28/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2780\n","Epoch 28: val_loss improved from 0.27030 to 0.26769, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2780 - val_loss: 0.2677\n","Epoch 29/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2752\n","Epoch 29: val_loss improved from 0.26769 to 0.26511, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.2752 - val_loss: 0.2651\n","Epoch 30/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2724\n","Epoch 30: val_loss improved from 0.26511 to 0.26256, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2724 - val_loss: 0.2626\n","Epoch 31/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2697\n","Epoch 31: val_loss improved from 0.26256 to 0.26004, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2697 - val_loss: 0.2600\n","Epoch 32/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2669\n","Epoch 32: val_loss improved from 0.26004 to 0.25755, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2669 - val_loss: 0.2576\n","Epoch 33/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2642\n","Epoch 33: val_loss improved from 0.25755 to 0.25509, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2642 - val_loss: 0.2551\n","Epoch 34/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2616\n","Epoch 34: val_loss improved from 0.25509 to 0.25265, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2616 - val_loss: 0.2527\n","Epoch 35/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2589\n","Epoch 35: val_loss improved from 0.25265 to 0.25025, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2589 - val_loss: 0.2502\n","Epoch 36/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2563\n","Epoch 36: val_loss improved from 0.25025 to 0.24787, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2563 - val_loss: 0.2479\n","Epoch 37/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2538\n","Epoch 37: val_loss improved from 0.24787 to 0.24551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.2538 - val_loss: 0.2455\n","Epoch 38/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2512\n","Epoch 38: val_loss improved from 0.24551 to 0.24319, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.2512 - val_loss: 0.2432\n","Epoch 39/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2487\n","Epoch 39: val_loss improved from 0.24319 to 0.24089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.2487 - val_loss: 0.2409\n","Epoch 40/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2462\n","Epoch 40: val_loss improved from 0.24089 to 0.23861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2462 - val_loss: 0.2386\n","Epoch 41/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2437\n","Epoch 41: val_loss improved from 0.23861 to 0.23636, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.2437 - val_loss: 0.2364\n","Epoch 42/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2413\n","Epoch 42: val_loss improved from 0.23636 to 0.23414, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2413 - val_loss: 0.2341\n","Epoch 43/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2389\n","Epoch 43: val_loss improved from 0.23414 to 0.23194, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2389 - val_loss: 0.2319\n","Epoch 44/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2365\n","Epoch 44: val_loss improved from 0.23194 to 0.22977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2365 - val_loss: 0.2298\n","Epoch 45/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2341\n","Epoch 45: val_loss improved from 0.22977 to 0.22762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2341 - val_loss: 0.2276\n","Epoch 46/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2318\n","Epoch 46: val_loss improved from 0.22762 to 0.22550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2318 - val_loss: 0.2255\n","Epoch 47/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2295\n","Epoch 47: val_loss improved from 0.22550 to 0.22339, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2295 - val_loss: 0.2234\n","Epoch 48/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2272\n","Epoch 48: val_loss improved from 0.22339 to 0.22132, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2272 - val_loss: 0.2213\n","Epoch 49/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2249\n","Epoch 49: val_loss improved from 0.22132 to 0.21926, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2249 - val_loss: 0.2193\n","Epoch 50/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2227\n","Epoch 50: val_loss improved from 0.21926 to 0.21723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2227 - val_loss: 0.2172\n","Epoch 51/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2205\n","Epoch 51: val_loss improved from 0.21723 to 0.21523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2205 - val_loss: 0.2152\n","Epoch 52/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2183\n","Epoch 52: val_loss improved from 0.21523 to 0.21324, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.2183 - val_loss: 0.2132\n","Epoch 53/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2161\n","Epoch 53: val_loss improved from 0.21324 to 0.21128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.2161 - val_loss: 0.2113\n","Epoch 54/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2140\n","Epoch 54: val_loss improved from 0.21128 to 0.20934, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.2140 - val_loss: 0.2093\n","Epoch 55/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2119\n","Epoch 55: val_loss improved from 0.20934 to 0.20742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.2119 - val_loss: 0.2074\n","Epoch 56/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2098\n","Epoch 56: val_loss improved from 0.20742 to 0.20553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.2098 - val_loss: 0.2055\n","Epoch 57/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2077\n","Epoch 57: val_loss improved from 0.20553 to 0.20365, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.2077 - val_loss: 0.2037\n","Epoch 58/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2056\n","Epoch 58: val_loss improved from 0.20365 to 0.20180, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.2056 - val_loss: 0.2018\n","Epoch 59/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2036\n","Epoch 59: val_loss improved from 0.20180 to 0.19997, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.2036 - val_loss: 0.2000\n","Epoch 60/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2016\n","Epoch 60: val_loss improved from 0.19997 to 0.19816, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2016 - val_loss: 0.1982\n","Epoch 61/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1996\n","Epoch 61: val_loss improved from 0.19816 to 0.19637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.1996 - val_loss: 0.1964\n","Epoch 62/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1977\n","Epoch 62: val_loss improved from 0.19637 to 0.19460, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.1977 - val_loss: 0.1946\n","Epoch 63/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1957\n","Epoch 63: val_loss improved from 0.19460 to 0.19285, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1957 - val_loss: 0.1928\n","Epoch 64/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1938\n","Epoch 64: val_loss improved from 0.19285 to 0.19112, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1938 - val_loss: 0.1911\n","Epoch 65/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1919\n","Epoch 65: val_loss improved from 0.19112 to 0.18941, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.1919 - val_loss: 0.1894\n","Epoch 66/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1900\n","Epoch 66: val_loss improved from 0.18941 to 0.18772, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1900 - val_loss: 0.1877\n","Epoch 67/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1882\n","Epoch 67: val_loss improved from 0.18772 to 0.18604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1882 - val_loss: 0.1860\n","Epoch 68/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1863\n","Epoch 68: val_loss improved from 0.18604 to 0.18439, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1863 - val_loss: 0.1844\n","Epoch 69/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1845\n","Epoch 69: val_loss improved from 0.18439 to 0.18276, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1845 - val_loss: 0.1828\n","Epoch 70/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1827\n","Epoch 70: val_loss improved from 0.18276 to 0.18114, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.1827 - val_loss: 0.1811\n","Epoch 71/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1809\n","Epoch 71: val_loss improved from 0.18114 to 0.17955, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.1809 - val_loss: 0.1795\n","Epoch 72/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1792\n","Epoch 72: val_loss improved from 0.17955 to 0.17797, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1792 - val_loss: 0.1780\n","Epoch 73/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1774\n","Epoch 73: val_loss improved from 0.17797 to 0.17641, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1774 - val_loss: 0.1764\n","Epoch 74/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1757\n","Epoch 74: val_loss improved from 0.17641 to 0.17487, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1757 - val_loss: 0.1749\n","Epoch 75/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1740\n","Epoch 75: val_loss improved from 0.17487 to 0.17335, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1740 - val_loss: 0.1733\n","Epoch 76/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1723\n","Epoch 76: val_loss improved from 0.17335 to 0.17184, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1723 - val_loss: 0.1718\n","Epoch 77/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1706\n","Epoch 77: val_loss improved from 0.17184 to 0.17035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.1706 - val_loss: 0.1704\n","Epoch 78/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1690\n","Epoch 78: val_loss improved from 0.17035 to 0.16888, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1690 - val_loss: 0.1689\n","Epoch 79/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1674\n","Epoch 79: val_loss improved from 0.16888 to 0.16742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.1674 - val_loss: 0.1674\n","Epoch 80/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1658\n","Epoch 80: val_loss improved from 0.16742 to 0.16599, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.1658 - val_loss: 0.1660\n","Epoch 81/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1642\n","Epoch 81: val_loss improved from 0.16599 to 0.16457, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.1642 - val_loss: 0.1646\n","Epoch 82/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1626\n","Epoch 82: val_loss improved from 0.16457 to 0.16316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1626 - val_loss: 0.1632\n","Epoch 83/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1610\n","Epoch 83: val_loss improved from 0.16316 to 0.16177, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.1610 - val_loss: 0.1618\n","Epoch 84/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1595\n","Epoch 84: val_loss improved from 0.16177 to 0.16040, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1595 - val_loss: 0.1604\n","Epoch 85/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1580\n","Epoch 85: val_loss improved from 0.16040 to 0.15904, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1580 - val_loss: 0.1590\n","Epoch 86/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1564\n","Epoch 86: val_loss improved from 0.15904 to 0.15770, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1564 - val_loss: 0.1577\n","Epoch 87/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1550\n","Epoch 87: val_loss improved from 0.15770 to 0.15637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1550 - val_loss: 0.1564\n","Epoch 88/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1535\n","Epoch 88: val_loss improved from 0.15637 to 0.15506, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1535 - val_loss: 0.1551\n","Epoch 89/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1520\n","Epoch 89: val_loss improved from 0.15506 to 0.15377, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1520 - val_loss: 0.1538\n","Epoch 90/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1506\n","Epoch 90: val_loss improved from 0.15377 to 0.15249, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1506 - val_loss: 0.1525\n","Epoch 91/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1491\n","Epoch 91: val_loss improved from 0.15249 to 0.15122, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1491 - val_loss: 0.1512\n","Epoch 92/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1477\n","Epoch 92: val_loss improved from 0.15122 to 0.14997, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1477 - val_loss: 0.1500\n","Epoch 93/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1463\n","Epoch 93: val_loss improved from 0.14997 to 0.14874, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1463 - val_loss: 0.1487\n","Epoch 94/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1449\n","Epoch 94: val_loss improved from 0.14874 to 0.14752, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1449 - val_loss: 0.1475\n","Epoch 95/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1436\n","Epoch 95: val_loss improved from 0.14752 to 0.14631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1436 - val_loss: 0.1463\n","Epoch 96/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1422\n","Epoch 96: val_loss improved from 0.14631 to 0.14511, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1422 - val_loss: 0.1451\n","Epoch 97/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1409\n","Epoch 97: val_loss improved from 0.14511 to 0.14394, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1409 - val_loss: 0.1439\n","Epoch 98/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1396\n","Epoch 98: val_loss improved from 0.14394 to 0.14277, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1396 - val_loss: 0.1428\n","Epoch 99/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1383\n","Epoch 99: val_loss improved from 0.14277 to 0.14162, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1383 - val_loss: 0.1416\n","Epoch 100/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1370\n","Epoch 100: val_loss improved from 0.14162 to 0.14048, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1370 - val_loss: 0.1405\n","Epoch 101/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1357\n","Epoch 101: val_loss improved from 0.14048 to 0.13936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1357 - val_loss: 0.1394\n","Epoch 102/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1344\n","Epoch 102: val_loss improved from 0.13936 to 0.13825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1344 - val_loss: 0.1382\n","Epoch 103/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1332\n","Epoch 103: val_loss improved from 0.13825 to 0.13715, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1332 - val_loss: 0.1371\n","Epoch 104/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1319\n","Epoch 104: val_loss improved from 0.13715 to 0.13606, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1319 - val_loss: 0.1361\n","Epoch 105/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1307\n","Epoch 105: val_loss improved from 0.13606 to 0.13499, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1307 - val_loss: 0.1350\n","Epoch 106/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1295\n","Epoch 106: val_loss improved from 0.13499 to 0.13393, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1295 - val_loss: 0.1339\n","Epoch 107/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1283\n","Epoch 107: val_loss improved from 0.13393 to 0.13288, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1283 - val_loss: 0.1329\n","Epoch 108/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1271\n","Epoch 108: val_loss improved from 0.13288 to 0.13185, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1271 - val_loss: 0.1318\n","Epoch 109/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1259\n","Epoch 109: val_loss improved from 0.13185 to 0.13082, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1259 - val_loss: 0.1308\n","Epoch 110/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1248\n","Epoch 110: val_loss improved from 0.13082 to 0.12981, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1248 - val_loss: 0.1298\n","Epoch 111/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1236\n","Epoch 111: val_loss improved from 0.12981 to 0.12882, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1236 - val_loss: 0.1288\n","Epoch 112/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1225\n","Epoch 112: val_loss improved from 0.12882 to 0.12783, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1225 - val_loss: 0.1278\n","Epoch 113/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1214\n","Epoch 113: val_loss improved from 0.12783 to 0.12685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1214 - val_loss: 0.1269\n","Epoch 114/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1203\n","Epoch 114: val_loss improved from 0.12685 to 0.12589, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1203 - val_loss: 0.1259\n","Epoch 115/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1192\n","Epoch 115: val_loss improved from 0.12589 to 0.12494, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1192 - val_loss: 0.1249\n","Epoch 116/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1181\n","Epoch 116: val_loss improved from 0.12494 to 0.12400, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1181 - val_loss: 0.1240\n","Epoch 117/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1170\n","Epoch 117: val_loss improved from 0.12400 to 0.12307, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1170 - val_loss: 0.1231\n","Epoch 118/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1160\n","Epoch 118: val_loss improved from 0.12307 to 0.12215, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1160 - val_loss: 0.1222\n","Epoch 119/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1149\n","Epoch 119: val_loss improved from 0.12215 to 0.12124, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1149 - val_loss: 0.1212\n","Epoch 120/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1139\n","Epoch 120: val_loss improved from 0.12124 to 0.12035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1139 - val_loss: 0.1203\n","Epoch 121/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1129\n","Epoch 121: val_loss improved from 0.12035 to 0.11946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1129 - val_loss: 0.1195\n","Epoch 122/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1119\n","Epoch 122: val_loss improved from 0.11946 to 0.11859, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1119 - val_loss: 0.1186\n","Epoch 123/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1109\n","Epoch 123: val_loss improved from 0.11859 to 0.11772, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1109 - val_loss: 0.1177\n","Epoch 124/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1099\n","Epoch 124: val_loss improved from 0.11772 to 0.11687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1099 - val_loss: 0.1169\n","Epoch 125/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1089\n","Epoch 125: val_loss improved from 0.11687 to 0.11603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1089 - val_loss: 0.1160\n","Epoch 126/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1079\n","Epoch 126: val_loss improved from 0.11603 to 0.11519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1079 - val_loss: 0.1152\n","Epoch 127/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1070\n","Epoch 127: val_loss improved from 0.11519 to 0.11437, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1070 - val_loss: 0.1144\n","Epoch 128/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1060\n","Epoch 128: val_loss improved from 0.11437 to 0.11356, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1060 - val_loss: 0.1136\n","Epoch 129/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1051\n","Epoch 129: val_loss improved from 0.11356 to 0.11275, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1051 - val_loss: 0.1128\n","Epoch 130/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1042\n","Epoch 130: val_loss improved from 0.11275 to 0.11196, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1042 - val_loss: 0.1120\n","Epoch 131/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1032\n","Epoch 131: val_loss improved from 0.11196 to 0.11117, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1032 - val_loss: 0.1112\n","Epoch 132/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1023\n","Epoch 132: val_loss improved from 0.11117 to 0.11040, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1023 - val_loss: 0.1104\n","Epoch 133/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1014\n","Epoch 133: val_loss improved from 0.11040 to 0.10963, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1014 - val_loss: 0.1096\n","Epoch 134/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1006\n","Epoch 134: val_loss improved from 0.10963 to 0.10888, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1006 - val_loss: 0.1089\n","Epoch 135/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0997\n","Epoch 135: val_loss improved from 0.10888 to 0.10813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0997 - val_loss: 0.1081\n","Epoch 136/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0988\n","Epoch 136: val_loss improved from 0.10813 to 0.10739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0988 - val_loss: 0.1074\n","Epoch 137/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0980\n","Epoch 137: val_loss improved from 0.10739 to 0.10667, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0980 - val_loss: 0.1067\n","Epoch 138/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0971\n","Epoch 138: val_loss improved from 0.10667 to 0.10595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0971 - val_loss: 0.1059\n","Epoch 139/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0963\n","Epoch 139: val_loss improved from 0.10595 to 0.10524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0963 - val_loss: 0.1052\n","Epoch 140/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0955\n","Epoch 140: val_loss improved from 0.10524 to 0.10453, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0955 - val_loss: 0.1045\n","Epoch 141/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0946\n","Epoch 141: val_loss improved from 0.10453 to 0.10384, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0946 - val_loss: 0.1038\n","Epoch 142/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0938\n","Epoch 142: val_loss improved from 0.10384 to 0.10316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0938 - val_loss: 0.1032\n","Epoch 143/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0930\n","Epoch 143: val_loss improved from 0.10316 to 0.10248, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0930 - val_loss: 0.1025\n","Epoch 144/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0923\n","Epoch 144: val_loss improved from 0.10248 to 0.10181, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0923 - val_loss: 0.1018\n","Epoch 145/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0915\n","Epoch 145: val_loss improved from 0.10181 to 0.10115, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0915 - val_loss: 0.1012\n","Epoch 146/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0907\n","Epoch 146: val_loss improved from 0.10115 to 0.10050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0907 - val_loss: 0.1005\n","Epoch 147/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0899\n","Epoch 147: val_loss improved from 0.10050 to 0.09986, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0899 - val_loss: 0.0999\n","Epoch 148/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0892\n","Epoch 148: val_loss improved from 0.09986 to 0.09922, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0892 - val_loss: 0.0992\n","Epoch 149/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0884\n","Epoch 149: val_loss improved from 0.09922 to 0.09860, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0884 - val_loss: 0.0986\n","Epoch 150/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0877\n","Epoch 150: val_loss improved from 0.09860 to 0.09798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0877 - val_loss: 0.0980\n","Epoch 151/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0870\n","Epoch 151: val_loss improved from 0.09798 to 0.09736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0870 - val_loss: 0.0974\n","Epoch 152/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0863\n","Epoch 152: val_loss improved from 0.09736 to 0.09676, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0863 - val_loss: 0.0968\n","Epoch 153/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0855\n","Epoch 153: val_loss improved from 0.09676 to 0.09616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0855 - val_loss: 0.0962\n","Epoch 154/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0848\n","Epoch 154: val_loss improved from 0.09616 to 0.09558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0848 - val_loss: 0.0956\n","Epoch 155/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0841\n","Epoch 155: val_loss improved from 0.09558 to 0.09499, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0841 - val_loss: 0.0950\n","Epoch 156/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0835\n","Epoch 156: val_loss improved from 0.09499 to 0.09442, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0835 - val_loss: 0.0944\n","Epoch 157/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0828\n","Epoch 157: val_loss improved from 0.09442 to 0.09385, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0828 - val_loss: 0.0939\n","Epoch 158/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0821\n","Epoch 158: val_loss improved from 0.09385 to 0.09329, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0821 - val_loss: 0.0933\n","Epoch 159/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0814\n","Epoch 159: val_loss improved from 0.09329 to 0.09274, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0814 - val_loss: 0.0927\n","Epoch 160/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0808\n","Epoch 160: val_loss improved from 0.09274 to 0.09219, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0808 - val_loss: 0.0922\n","Epoch 161/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0801\n","Epoch 161: val_loss improved from 0.09219 to 0.09165, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0801 - val_loss: 0.0917\n","Epoch 162/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0795\n","Epoch 162: val_loss improved from 0.09165 to 0.09112, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0795 - val_loss: 0.0911\n","Epoch 163/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0789\n","Epoch 163: val_loss improved from 0.09112 to 0.09060, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0789 - val_loss: 0.0906\n","Epoch 164/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0782\n","Epoch 164: val_loss improved from 0.09060 to 0.09008, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0782 - val_loss: 0.0901\n","Epoch 165/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0776\n","Epoch 165: val_loss improved from 0.09008 to 0.08957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0776 - val_loss: 0.0896\n","Epoch 166/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0770\n","Epoch 166: val_loss improved from 0.08957 to 0.08906, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0770 - val_loss: 0.0891\n","Epoch 167/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0764\n","Epoch 167: val_loss improved from 0.08906 to 0.08856, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0764 - val_loss: 0.0886\n","Epoch 168/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0758\n","Epoch 168: val_loss improved from 0.08856 to 0.08807, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0758 - val_loss: 0.0881\n","Epoch 169/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0752\n","Epoch 169: val_loss improved from 0.08807 to 0.08758, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0752 - val_loss: 0.0876\n","Epoch 170/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0746\n","Epoch 170: val_loss improved from 0.08758 to 0.08710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0746 - val_loss: 0.0871\n","Epoch 171/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0740\n","Epoch 171: val_loss improved from 0.08710 to 0.08663, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0740 - val_loss: 0.0866\n","Epoch 172/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0735\n","Epoch 172: val_loss improved from 0.08663 to 0.08616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0735 - val_loss: 0.0862\n","Epoch 173/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0729\n","Epoch 173: val_loss improved from 0.08616 to 0.08570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0729 - val_loss: 0.0857\n","Epoch 174/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0724\n","Epoch 174: val_loss improved from 0.08570 to 0.08525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0724 - val_loss: 0.0852\n","Epoch 175/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0718\n","Epoch 175: val_loss improved from 0.08525 to 0.08480, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0718 - val_loss: 0.0848\n","Epoch 176/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0713\n","Epoch 176: val_loss improved from 0.08480 to 0.08435, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0713 - val_loss: 0.0844\n","Epoch 177/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0707\n","Epoch 177: val_loss improved from 0.08435 to 0.08391, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0707 - val_loss: 0.0839\n","Epoch 178/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0702\n","Epoch 178: val_loss improved from 0.08391 to 0.08348, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0702 - val_loss: 0.0835\n","Epoch 179/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0697\n","Epoch 179: val_loss improved from 0.08348 to 0.08306, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0697 - val_loss: 0.0831\n","Epoch 180/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0691\n","Epoch 180: val_loss improved from 0.08306 to 0.08263, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0691 - val_loss: 0.0826\n","Epoch 181/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0686\n","Epoch 181: val_loss improved from 0.08263 to 0.08222, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0686 - val_loss: 0.0822\n","Epoch 182/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0681\n","Epoch 182: val_loss improved from 0.08222 to 0.08181, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0681 - val_loss: 0.0818\n","Epoch 183/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0676\n","Epoch 183: val_loss improved from 0.08181 to 0.08140, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0676 - val_loss: 0.0814\n","Epoch 184/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0671\n","Epoch 184: val_loss improved from 0.08140 to 0.08101, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0671 - val_loss: 0.0810\n","Epoch 185/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0666\n","Epoch 185: val_loss improved from 0.08101 to 0.08061, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0666 - val_loss: 0.0806\n","Epoch 186/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0661\n","Epoch 186: val_loss improved from 0.08061 to 0.08022, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0661 - val_loss: 0.0802\n","Epoch 187/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0657\n","Epoch 187: val_loss improved from 0.08022 to 0.07984, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0657 - val_loss: 0.0798\n","Epoch 188/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0652\n","Epoch 188: val_loss improved from 0.07984 to 0.07946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0652 - val_loss: 0.0795\n","Epoch 189/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0647\n","Epoch 189: val_loss improved from 0.07946 to 0.07909, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0647 - val_loss: 0.0791\n","Epoch 190/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0643\n","Epoch 190: val_loss improved from 0.07909 to 0.07872, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0643 - val_loss: 0.0787\n","Epoch 191/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0638\n","Epoch 191: val_loss improved from 0.07872 to 0.07836, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0638 - val_loss: 0.0784\n","Epoch 192/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0634\n","Epoch 192: val_loss improved from 0.07836 to 0.07800, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0634 - val_loss: 0.0780\n","Epoch 193/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0629\n","Epoch 193: val_loss improved from 0.07800 to 0.07764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0629 - val_loss: 0.0776\n","Epoch 194/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0625\n","Epoch 194: val_loss improved from 0.07764 to 0.07729, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0625 - val_loss: 0.0773\n","Epoch 195/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0620\n","Epoch 195: val_loss improved from 0.07729 to 0.07695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0620 - val_loss: 0.0769\n","Epoch 196/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0616\n","Epoch 196: val_loss improved from 0.07695 to 0.07661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0616 - val_loss: 0.0766\n","Epoch 197/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0612\n","Epoch 197: val_loss improved from 0.07661 to 0.07627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0612 - val_loss: 0.0763\n","Epoch 198/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0608\n","Epoch 198: val_loss improved from 0.07627 to 0.07594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0608 - val_loss: 0.0759\n","Epoch 199/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0604\n","Epoch 199: val_loss improved from 0.07594 to 0.07562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0604 - val_loss: 0.0756\n","Epoch 200/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0599\n","Epoch 200: val_loss improved from 0.07562 to 0.07530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0599 - val_loss: 0.0753\n","Epoch 201/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0595\n","Epoch 201: val_loss improved from 0.07530 to 0.07498, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0595 - val_loss: 0.0750\n","Epoch 202/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0591\n","Epoch 202: val_loss improved from 0.07498 to 0.07467, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0591 - val_loss: 0.0747\n","Epoch 203/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0587\n","Epoch 203: val_loss improved from 0.07467 to 0.07436, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0587 - val_loss: 0.0744\n","Epoch 204/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0584\n","Epoch 204: val_loss improved from 0.07436 to 0.07405, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0584 - val_loss: 0.0741\n","Epoch 205/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0580\n","Epoch 205: val_loss improved from 0.07405 to 0.07375, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0580 - val_loss: 0.0738\n","Epoch 206/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0576\n","Epoch 206: val_loss improved from 0.07375 to 0.07346, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0576 - val_loss: 0.0735\n","Epoch 207/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0572\n","Epoch 207: val_loss improved from 0.07346 to 0.07316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0572 - val_loss: 0.0732\n","Epoch 208/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0568\n","Epoch 208: val_loss improved from 0.07316 to 0.07287, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0568 - val_loss: 0.0729\n","Epoch 209/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0565\n","Epoch 209: val_loss improved from 0.07287 to 0.07259, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0565 - val_loss: 0.0726\n","Epoch 210/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0561\n","Epoch 210: val_loss improved from 0.07259 to 0.07231, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0561 - val_loss: 0.0723\n","Epoch 211/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0558\n","Epoch 211: val_loss improved from 0.07231 to 0.07203, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0558 - val_loss: 0.0720\n","Epoch 212/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0554\n","Epoch 212: val_loss improved from 0.07203 to 0.07176, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0554 - val_loss: 0.0718\n","Epoch 213/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0551\n","Epoch 213: val_loss improved from 0.07176 to 0.07149, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0551 - val_loss: 0.0715\n","Epoch 214/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0547\n","Epoch 214: val_loss improved from 0.07149 to 0.07123, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0547 - val_loss: 0.0712\n","Epoch 215/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0544\n","Epoch 215: val_loss improved from 0.07123 to 0.07097, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0544 - val_loss: 0.0710\n","Epoch 216/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0540\n","Epoch 216: val_loss improved from 0.07097 to 0.07071, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0540 - val_loss: 0.0707\n","Epoch 217/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0537\n","Epoch 217: val_loss improved from 0.07071 to 0.07045, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0537 - val_loss: 0.0705\n","Epoch 218/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0534\n","Epoch 218: val_loss improved from 0.07045 to 0.07020, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0534 - val_loss: 0.0702\n","Epoch 219/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0530\n","Epoch 219: val_loss improved from 0.07020 to 0.06996, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0530 - val_loss: 0.0700\n","Epoch 220/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0527\n","Epoch 220: val_loss improved from 0.06996 to 0.06971, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 137ms/step - loss: 0.0527 - val_loss: 0.0697\n","Epoch 221/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 221: val_loss improved from 0.06971 to 0.06947, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0524 - val_loss: 0.0695\n","Epoch 222/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0521\n","Epoch 222: val_loss improved from 0.06947 to 0.06924, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0521 - val_loss: 0.0692\n","Epoch 223/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0518\n","Epoch 223: val_loss improved from 0.06924 to 0.06900, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0518 - val_loss: 0.0690\n","Epoch 224/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 224: val_loss improved from 0.06900 to 0.06877, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0515 - val_loss: 0.0688\n","Epoch 225/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0512\n","Epoch 225: val_loss improved from 0.06877 to 0.06855, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.0512 - val_loss: 0.0685\n","Epoch 226/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0509\n","Epoch 226: val_loss improved from 0.06855 to 0.06832, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0509 - val_loss: 0.0683\n","Epoch 227/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0506\n","Epoch 227: val_loss improved from 0.06832 to 0.06810, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0506 - val_loss: 0.0681\n","Epoch 228/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0503\n","Epoch 228: val_loss improved from 0.06810 to 0.06789, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0503 - val_loss: 0.0679\n","Epoch 229/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0500\n","Epoch 229: val_loss improved from 0.06789 to 0.06767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.0500 - val_loss: 0.0677\n","Epoch 230/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 230: val_loss improved from 0.06767 to 0.06746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0497 - val_loss: 0.0675\n","Epoch 231/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 231: val_loss improved from 0.06746 to 0.06725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0494 - val_loss: 0.0673\n","Epoch 232/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0492\n","Epoch 232: val_loss improved from 0.06725 to 0.06705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0492 - val_loss: 0.0670\n","Epoch 233/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0489\n","Epoch 233: val_loss improved from 0.06705 to 0.06685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0489 - val_loss: 0.0668\n","Epoch 234/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0486\n","Epoch 234: val_loss improved from 0.06685 to 0.06665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0486 - val_loss: 0.0666\n","Epoch 235/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0483\n","Epoch 235: val_loss improved from 0.06665 to 0.06645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0483 - val_loss: 0.0665\n","Epoch 236/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0481\n","Epoch 236: val_loss improved from 0.06645 to 0.06626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0481 - val_loss: 0.0663\n","Epoch 237/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0478\n","Epoch 237: val_loss improved from 0.06626 to 0.06607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0478 - val_loss: 0.0661\n","Epoch 238/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0476\n","Epoch 238: val_loss improved from 0.06607 to 0.06588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0476 - val_loss: 0.0659\n","Epoch 239/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0473\n","Epoch 239: val_loss improved from 0.06588 to 0.06569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0473 - val_loss: 0.0657\n","Epoch 240/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0471\n","Epoch 240: val_loss improved from 0.06569 to 0.06551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0471 - val_loss: 0.0655\n","Epoch 241/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0468\n","Epoch 241: val_loss improved from 0.06551 to 0.06533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0468 - val_loss: 0.0653\n","Epoch 242/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0466\n","Epoch 242: val_loss improved from 0.06533 to 0.06516, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0466 - val_loss: 0.0652\n","Epoch 243/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0463\n","Epoch 243: val_loss improved from 0.06516 to 0.06498, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0463 - val_loss: 0.0650\n","Epoch 244/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0461\n","Epoch 244: val_loss improved from 0.06498 to 0.06481, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0461 - val_loss: 0.0648\n","Epoch 245/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0459\n","Epoch 245: val_loss improved from 0.06481 to 0.06464, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0459 - val_loss: 0.0646\n","Epoch 246/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0456\n","Epoch 246: val_loss improved from 0.06464 to 0.06448, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0456 - val_loss: 0.0645\n","Epoch 247/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 247: val_loss improved from 0.06448 to 0.06431, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0454 - val_loss: 0.0643\n","Epoch 248/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0452\n","Epoch 248: val_loss improved from 0.06431 to 0.06415, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0452 - val_loss: 0.0642\n","Epoch 249/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0449\n","Epoch 249: val_loss improved from 0.06415 to 0.06399, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0449 - val_loss: 0.0640\n","Epoch 250/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0447\n","Epoch 250: val_loss improved from 0.06399 to 0.06383, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0447 - val_loss: 0.0638\n","Epoch 251/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0445\n","Epoch 251: val_loss improved from 0.06383 to 0.06368, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0445 - val_loss: 0.0637\n","Epoch 252/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0443\n","Epoch 252: val_loss improved from 0.06368 to 0.06353, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0443 - val_loss: 0.0635\n","Epoch 253/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0441\n","Epoch 253: val_loss improved from 0.06353 to 0.06338, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0441 - val_loss: 0.0634\n","Epoch 254/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0439\n","Epoch 254: val_loss improved from 0.06338 to 0.06323, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0439 - val_loss: 0.0632\n","Epoch 255/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0437\n","Epoch 255: val_loss improved from 0.06323 to 0.06309, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0437 - val_loss: 0.0631\n","Epoch 256/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 256: val_loss improved from 0.06309 to 0.06294, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0435 - val_loss: 0.0629\n","Epoch 257/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 257: val_loss improved from 0.06294 to 0.06280, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0432 - val_loss: 0.0628\n","Epoch 258/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 258: val_loss improved from 0.06280 to 0.06266, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0430 - val_loss: 0.0627\n","Epoch 259/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0429\n","Epoch 259: val_loss improved from 0.06266 to 0.06253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0429 - val_loss: 0.0625\n","Epoch 260/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 260: val_loss improved from 0.06253 to 0.06239, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0427 - val_loss: 0.0624\n","Epoch 261/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0425\n","Epoch 261: val_loss improved from 0.06239 to 0.06226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0425 - val_loss: 0.0623\n","Epoch 262/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0423\n","Epoch 262: val_loss improved from 0.06226 to 0.06213, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0423 - val_loss: 0.0621\n","Epoch 263/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 263: val_loss improved from 0.06213 to 0.06200, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0421 - val_loss: 0.0620\n","Epoch 264/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 264: val_loss improved from 0.06200 to 0.06188, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0419 - val_loss: 0.0619\n","Epoch 265/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 265: val_loss improved from 0.06188 to 0.06175, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0417 - val_loss: 0.0618\n","Epoch 266/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0415\n","Epoch 266: val_loss improved from 0.06175 to 0.06163, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0415 - val_loss: 0.0616\n","Epoch 267/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 267: val_loss improved from 0.06163 to 0.06151, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0414 - val_loss: 0.0615\n","Epoch 268/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0412\n","Epoch 268: val_loss improved from 0.06151 to 0.06139, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0412 - val_loss: 0.0614\n","Epoch 269/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 269: val_loss improved from 0.06139 to 0.06128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0410 - val_loss: 0.0613\n","Epoch 270/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 270: val_loss improved from 0.06128 to 0.06116, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0408 - val_loss: 0.0612\n","Epoch 271/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 271: val_loss improved from 0.06116 to 0.06105, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0407 - val_loss: 0.0610\n","Epoch 272/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0405\n","Epoch 272: val_loss improved from 0.06105 to 0.06094, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0405 - val_loss: 0.0609\n","Epoch 273/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 273: val_loss improved from 0.06094 to 0.06083, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0403 - val_loss: 0.0608\n","Epoch 274/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 274: val_loss improved from 0.06083 to 0.06072, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0402 - val_loss: 0.0607\n","Epoch 275/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0400\n","Epoch 275: val_loss improved from 0.06072 to 0.06062, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0400 - val_loss: 0.0606\n","Epoch 276/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 276: val_loss improved from 0.06062 to 0.06051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0399 - val_loss: 0.0605\n","Epoch 277/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0397\n","Epoch 277: val_loss improved from 0.06051 to 0.06041, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0397 - val_loss: 0.0604\n","Epoch 278/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 278: val_loss improved from 0.06041 to 0.06031, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0395 - val_loss: 0.0603\n","Epoch 279/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 279: val_loss improved from 0.06031 to 0.06021, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0394 - val_loss: 0.0602\n","Epoch 280/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 280: val_loss improved from 0.06021 to 0.06011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0392 - val_loss: 0.0601\n","Epoch 281/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 281: val_loss improved from 0.06011 to 0.06002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0391 - val_loss: 0.0600\n","Epoch 282/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 282: val_loss improved from 0.06002 to 0.05992, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0389 - val_loss: 0.0599\n","Epoch 283/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0388\n","Epoch 283: val_loss improved from 0.05992 to 0.05983, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0388 - val_loss: 0.0598\n","Epoch 284/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 284: val_loss improved from 0.05983 to 0.05974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0387 - val_loss: 0.0597\n","Epoch 285/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 285: val_loss improved from 0.05974 to 0.05965, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0385 - val_loss: 0.0597\n","Epoch 286/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 286: val_loss improved from 0.05965 to 0.05956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0384 - val_loss: 0.0596\n","Epoch 287/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 287: val_loss improved from 0.05956 to 0.05948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0382 - val_loss: 0.0595\n","Epoch 288/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 288: val_loss improved from 0.05948 to 0.05939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0381 - val_loss: 0.0594\n","Epoch 289/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 289: val_loss improved from 0.05939 to 0.05931, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0380 - val_loss: 0.0593\n","Epoch 290/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 290: val_loss improved from 0.05931 to 0.05922, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0378 - val_loss: 0.0592\n","Epoch 291/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 291: val_loss improved from 0.05922 to 0.05914, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0377 - val_loss: 0.0591\n","Epoch 292/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 292: val_loss improved from 0.05914 to 0.05906, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0376 - val_loss: 0.0591\n","Epoch 293/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 293: val_loss improved from 0.05906 to 0.05899, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0374 - val_loss: 0.0590\n","Epoch 294/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 294: val_loss improved from 0.05899 to 0.05891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0373 - val_loss: 0.0589\n","Epoch 295/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 295: val_loss improved from 0.05891 to 0.05883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0372 - val_loss: 0.0588\n","Epoch 296/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 296: val_loss improved from 0.05883 to 0.05876, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0371 - val_loss: 0.0588\n","Epoch 297/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 297: val_loss improved from 0.05876 to 0.05869, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0370 - val_loss: 0.0587\n","Epoch 298/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 298: val_loss improved from 0.05869 to 0.05861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0368 - val_loss: 0.0586\n","Epoch 299/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 299: val_loss improved from 0.05861 to 0.05854, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0367 - val_loss: 0.0585\n","Epoch 300/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 300: val_loss improved from 0.05854 to 0.05847, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0366 - val_loss: 0.0585\n","Epoch 301/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 301: val_loss improved from 0.05847 to 0.05841, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0365 - val_loss: 0.0584\n","Epoch 302/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 302: val_loss improved from 0.05841 to 0.05834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0364 - val_loss: 0.0583\n","Epoch 303/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 303: val_loss improved from 0.05834 to 0.05827, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0363 - val_loss: 0.0583\n","Epoch 304/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 304: val_loss improved from 0.05827 to 0.05821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0362 - val_loss: 0.0582\n","Epoch 305/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 305: val_loss improved from 0.05821 to 0.05815, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0360 - val_loss: 0.0581\n","Epoch 306/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 306: val_loss improved from 0.05815 to 0.05808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0359 - val_loss: 0.0581\n","Epoch 307/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 307: val_loss improved from 0.05808 to 0.05802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0358 - val_loss: 0.0580\n","Epoch 308/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 308: val_loss improved from 0.05802 to 0.05796, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0357 - val_loss: 0.0580\n","Epoch 309/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 309: val_loss improved from 0.05796 to 0.05790, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0356 - val_loss: 0.0579\n","Epoch 310/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 310: val_loss improved from 0.05790 to 0.05785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0355 - val_loss: 0.0578\n","Epoch 311/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 311: val_loss improved from 0.05785 to 0.05779, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0354 - val_loss: 0.0578\n","Epoch 312/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 312: val_loss improved from 0.05779 to 0.05773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0353 - val_loss: 0.0577\n","Epoch 313/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 313: val_loss improved from 0.05773 to 0.05768, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0352 - val_loss: 0.0577\n","Epoch 314/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 314: val_loss improved from 0.05768 to 0.05763, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0351 - val_loss: 0.0576\n","Epoch 315/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 315: val_loss improved from 0.05763 to 0.05757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0350 - val_loss: 0.0576\n","Epoch 316/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 316: val_loss improved from 0.05757 to 0.05752, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0349 - val_loss: 0.0575\n","Epoch 317/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 317: val_loss improved from 0.05752 to 0.05747, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0348 - val_loss: 0.0575\n","Epoch 318/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 318: val_loss improved from 0.05747 to 0.05742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0348 - val_loss: 0.0574\n","Epoch 319/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 319: val_loss improved from 0.05742 to 0.05737, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0347 - val_loss: 0.0574\n","Epoch 320/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 320: val_loss improved from 0.05737 to 0.05732, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0346 - val_loss: 0.0573\n","Epoch 321/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 321: val_loss improved from 0.05732 to 0.05728, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0345 - val_loss: 0.0573\n","Epoch 322/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 322: val_loss improved from 0.05728 to 0.05723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0344 - val_loss: 0.0572\n","Epoch 323/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 323: val_loss improved from 0.05723 to 0.05719, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0343 - val_loss: 0.0572\n","Epoch 324/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 324: val_loss improved from 0.05719 to 0.05714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0342 - val_loss: 0.0571\n","Epoch 325/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 325: val_loss improved from 0.05714 to 0.05710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0341 - val_loss: 0.0571\n","Epoch 326/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 326: val_loss improved from 0.05710 to 0.05705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0341 - val_loss: 0.0571\n","Epoch 327/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 327: val_loss improved from 0.05705 to 0.05701, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0340 - val_loss: 0.0570\n","Epoch 328/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 328: val_loss improved from 0.05701 to 0.05697, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0339 - val_loss: 0.0570\n","Epoch 329/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 329: val_loss improved from 0.05697 to 0.05693, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0338 - val_loss: 0.0569\n","Epoch 330/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 330: val_loss improved from 0.05693 to 0.05689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0337 - val_loss: 0.0569\n","Epoch 331/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 331: val_loss improved from 0.05689 to 0.05685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0337 - val_loss: 0.0569\n","Epoch 332/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 332: val_loss improved from 0.05685 to 0.05682, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0336 - val_loss: 0.0568\n","Epoch 333/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 333: val_loss improved from 0.05682 to 0.05678, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0335 - val_loss: 0.0568\n","Epoch 334/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 334: val_loss improved from 0.05678 to 0.05674, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0334 - val_loss: 0.0567\n","Epoch 335/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 335: val_loss improved from 0.05674 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0334 - val_loss: 0.0567\n","Epoch 336/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 336: val_loss improved from 0.05671 to 0.05667, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0333 - val_loss: 0.0567\n","Epoch 337/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 337: val_loss improved from 0.05667 to 0.05664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0332 - val_loss: 0.0566\n","Epoch 338/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 338: val_loss improved from 0.05664 to 0.05660, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0332 - val_loss: 0.0566\n","Epoch 339/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 339: val_loss improved from 0.05660 to 0.05657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0331 - val_loss: 0.0566\n","Epoch 340/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 340: val_loss improved from 0.05657 to 0.05654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0330 - val_loss: 0.0565\n","Epoch 341/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 341: val_loss improved from 0.05654 to 0.05651, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0329 - val_loss: 0.0565\n","Epoch 342/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 342: val_loss improved from 0.05651 to 0.05648, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0329 - val_loss: 0.0565\n","Epoch 343/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 343: val_loss improved from 0.05648 to 0.05645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0328 - val_loss: 0.0564\n","Epoch 344/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 344: val_loss improved from 0.05645 to 0.05642, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0328 - val_loss: 0.0564\n","Epoch 345/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 345: val_loss improved from 0.05642 to 0.05639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0327 - val_loss: 0.0564\n","Epoch 346/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 346: val_loss improved from 0.05639 to 0.05636, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0326 - val_loss: 0.0564\n","Epoch 347/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 347: val_loss improved from 0.05636 to 0.05633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0326 - val_loss: 0.0563\n","Epoch 348/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 348: val_loss improved from 0.05633 to 0.05630, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0325 - val_loss: 0.0563\n","Epoch 349/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 349: val_loss improved from 0.05630 to 0.05628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0324 - val_loss: 0.0563\n","Epoch 350/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 350: val_loss improved from 0.05628 to 0.05625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0324 - val_loss: 0.0563\n","Epoch 351/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 351: val_loss improved from 0.05625 to 0.05623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0323 - val_loss: 0.0562\n","Epoch 352/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 352: val_loss improved from 0.05623 to 0.05620, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0323 - val_loss: 0.0562\n","Epoch 353/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 353: val_loss improved from 0.05620 to 0.05618, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0322 - val_loss: 0.0562\n","Epoch 354/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 354: val_loss improved from 0.05618 to 0.05615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0321 - val_loss: 0.0562\n","Epoch 355/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 355: val_loss improved from 0.05615 to 0.05613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0321 - val_loss: 0.0561\n","Epoch 356/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 356: val_loss improved from 0.05613 to 0.05611, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0320 - val_loss: 0.0561\n","Epoch 357/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 357: val_loss improved from 0.05611 to 0.05609, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0320 - val_loss: 0.0561\n","Epoch 358/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 358: val_loss improved from 0.05609 to 0.05607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0319 - val_loss: 0.0561\n","Epoch 359/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 359: val_loss improved from 0.05607 to 0.05604, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0319 - val_loss: 0.0560\n","Epoch 360/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 360: val_loss improved from 0.05604 to 0.05602, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0318 - val_loss: 0.0560\n","Epoch 361/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 361: val_loss improved from 0.05602 to 0.05600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0318 - val_loss: 0.0560\n","Epoch 362/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 362: val_loss improved from 0.05600 to 0.05598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0317 - val_loss: 0.0560\n","Epoch 363/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 363: val_loss improved from 0.05598 to 0.05597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0317 - val_loss: 0.0560\n","Epoch 364/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 364: val_loss improved from 0.05597 to 0.05595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0316 - val_loss: 0.0559\n","Epoch 365/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 365: val_loss improved from 0.05595 to 0.05593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0316 - val_loss: 0.0559\n","Epoch 366/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 366: val_loss improved from 0.05593 to 0.05591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0315 - val_loss: 0.0559\n","Epoch 367/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 367: val_loss improved from 0.05591 to 0.05589, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0315 - val_loss: 0.0559\n","Epoch 368/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 368: val_loss improved from 0.05589 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0314 - val_loss: 0.0559\n","Epoch 369/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 369: val_loss improved from 0.05588 to 0.05586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0314 - val_loss: 0.0559\n","Epoch 370/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 370: val_loss improved from 0.05586 to 0.05584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0313 - val_loss: 0.0558\n","Epoch 371/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 371: val_loss improved from 0.05584 to 0.05583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0313 - val_loss: 0.0558\n","Epoch 372/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 372: val_loss improved from 0.05583 to 0.05581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0313 - val_loss: 0.0558\n","Epoch 373/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 373: val_loss improved from 0.05581 to 0.05580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0312 - val_loss: 0.0558\n","Epoch 374/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 374: val_loss improved from 0.05580 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.0312 - val_loss: 0.0558\n","Epoch 375/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 375: val_loss improved from 0.05579 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0311 - val_loss: 0.0558\n","Epoch 376/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 376: val_loss improved from 0.05577 to 0.05576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0311 - val_loss: 0.0558\n","Epoch 377/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 377: val_loss improved from 0.05576 to 0.05574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0310 - val_loss: 0.0557\n","Epoch 378/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 378: val_loss improved from 0.05574 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0310 - val_loss: 0.0557\n","Epoch 379/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 379: val_loss improved from 0.05573 to 0.05572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0310 - val_loss: 0.0557\n","Epoch 380/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 380: val_loss improved from 0.05572 to 0.05571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0309 - val_loss: 0.0557\n","Epoch 381/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 381: val_loss improved from 0.05571 to 0.05570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 143ms/step - loss: 0.0309 - val_loss: 0.0557\n","Epoch 382/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 382: val_loss improved from 0.05570 to 0.05568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0308 - val_loss: 0.0557\n","Epoch 383/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 383: val_loss improved from 0.05568 to 0.05567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 138ms/step - loss: 0.0308 - val_loss: 0.0557\n","Epoch 384/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 384: val_loss improved from 0.05567 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0308 - val_loss: 0.0557\n","Epoch 385/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 385: val_loss improved from 0.05566 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0307 - val_loss: 0.0557\n","Epoch 386/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 386: val_loss improved from 0.05565 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0307 - val_loss: 0.0556\n","Epoch 387/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 387: val_loss improved from 0.05564 to 0.05563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0307 - val_loss: 0.0556\n","Epoch 388/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 388: val_loss improved from 0.05563 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0306 - val_loss: 0.0556\n","Epoch 389/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 389: val_loss improved from 0.05562 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0306 - val_loss: 0.0556\n","Epoch 390/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 390: val_loss improved from 0.05561 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0306 - val_loss: 0.0556\n","Epoch 391/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 391: val_loss improved from 0.05561 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0305 - val_loss: 0.0556\n","Epoch 392/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 392: val_loss improved from 0.05560 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0305 - val_loss: 0.0556\n","Epoch 393/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 393: val_loss improved from 0.05559 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0305 - val_loss: 0.0556\n","Epoch 394/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 394: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0304 - val_loss: 0.0556\n","Epoch 395/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 395: val_loss improved from 0.05557 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0304 - val_loss: 0.0556\n","Epoch 396/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 396: val_loss improved from 0.05557 to 0.05556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0304 - val_loss: 0.0556\n","Epoch 397/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 397: val_loss improved from 0.05556 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0303 - val_loss: 0.0556\n","Epoch 398/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 398: val_loss improved from 0.05555 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0303 - val_loss: 0.0555\n","Epoch 399/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 399: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0303 - val_loss: 0.0555\n","Epoch 400/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 400: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0302 - val_loss: 0.0555\n","Epoch 401/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 401: val_loss improved from 0.05553 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0302 - val_loss: 0.0555\n","Epoch 402/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 402: val_loss improved from 0.05553 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0302 - val_loss: 0.0555\n","Epoch 403/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 403: val_loss improved from 0.05552 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0301 - val_loss: 0.0555\n","Epoch 404/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 404: val_loss improved from 0.05552 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0301 - val_loss: 0.0555\n","Epoch 405/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 405: val_loss improved from 0.05551 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0301 - val_loss: 0.0555\n","Epoch 406/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 406: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0301 - val_loss: 0.0555\n","Epoch 407/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 407: val_loss improved from 0.05550 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0300 - val_loss: 0.0555\n","Epoch 408/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 408: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0300 - val_loss: 0.0555\n","Epoch 409/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 409: val_loss improved from 0.05549 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0300 - val_loss: 0.0555\n","Epoch 410/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 410: val_loss improved from 0.05549 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0300 - val_loss: 0.0555\n","Epoch 411/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 411: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0299 - val_loss: 0.0555\n","Epoch 412/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 412: val_loss improved from 0.05548 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0299 - val_loss: 0.0555\n","Epoch 413/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 413: val_loss improved from 0.05548 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0299 - val_loss: 0.0555\n","Epoch 414/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 414: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0299 - val_loss: 0.0555\n","Epoch 415/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 415: val_loss improved from 0.05547 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0298 - val_loss: 0.0555\n","Epoch 416/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 416: val_loss improved from 0.05547 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0298 - val_loss: 0.0555\n","Epoch 417/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 417: val_loss improved from 0.05547 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0298 - val_loss: 0.0555\n","Epoch 418/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 418: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0298 - val_loss: 0.0555\n","Epoch 419/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 419: val_loss improved from 0.05546 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0297 - val_loss: 0.0555\n","Epoch 420/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 420: val_loss improved from 0.05546 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0297 - val_loss: 0.0555\n","Epoch 421/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 421: val_loss improved from 0.05546 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0297 - val_loss: 0.0555\n","Epoch 422/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 422: val_loss improved from 0.05546 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0297 - val_loss: 0.0555\n","Epoch 423/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 423: val_loss improved from 0.05546 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0296 - val_loss: 0.0555\n","Epoch 424/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 424: val_loss improved from 0.05546 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0296 - val_loss: 0.0555\n","Epoch 425/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 425: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0296 - val_loss: 0.0555\n","Epoch 426/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 426: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0296 - val_loss: 0.0555\n","Epoch 427/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 427: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0296 - val_loss: 0.0555\n","Epoch 428/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 428: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0295 - val_loss: 0.0555\n","Epoch 429/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 429: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0295 - val_loss: 0.0555\n","Epoch 430/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 430: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0295 - val_loss: 0.0555\n","Epoch 431/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 431: val_loss improved from 0.05545 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0295 - val_loss: 0.0555\n","Epoch 432/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 432: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0295 - val_loss: 0.0555\n","Epoch 433/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 433: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0294 - val_loss: 0.0555\n","Epoch 434/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 434: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0294 - val_loss: 0.0555\n","Epoch 435/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 435: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0294 - val_loss: 0.0555\n","Epoch 436/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 436: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0294 - val_loss: 0.0555\n","Epoch 437/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 437: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0294 - val_loss: 0.0555\n","Epoch 438/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 438: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0294 - val_loss: 0.0555\n","Epoch 439/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 439: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0293 - val_loss: 0.0555\n","Epoch 440/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 440: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0293 - val_loss: 0.0555\n","Epoch 441/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 441: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - val_loss: 0.0555\n","Epoch 442/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 442: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0293 - val_loss: 0.0555\n","Epoch 443/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 443: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0293 - val_loss: 0.0555\n","Epoch 444/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 444: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0293 - val_loss: 0.0555\n","Epoch 445/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 445: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0292 - val_loss: 0.0555\n","Epoch 446/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 446: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0292 - val_loss: 0.0555\n","Epoch 447/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 447: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0292 - val_loss: 0.0555\n","Epoch 448/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 448: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0292 - val_loss: 0.0555\n","Epoch 449/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 449: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0292 - val_loss: 0.0555\n","Epoch 450/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 450: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0292 - val_loss: 0.0555\n","Epoch 451/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 451: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0291 - val_loss: 0.0555\n","Epoch 452/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 452: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0291 - val_loss: 0.0555\n","Epoch 453/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 453: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0291 - val_loss: 0.0555\n","Epoch 454/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 454: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0291 - val_loss: 0.0555\n","Epoch 455/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 455: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0291 - val_loss: 0.0555\n","Epoch 456/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 456: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0291 - val_loss: 0.0555\n","Epoch 457/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 457: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0555\n","Epoch 458/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 458: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0290 - val_loss: 0.0555\n","Epoch 459/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 459: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0290 - val_loss: 0.0555\n","Epoch 460/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 460: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0290 - val_loss: 0.0555\n","Epoch 461/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 461: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0290 - val_loss: 0.0555\n","Epoch 462/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 462: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0290 - val_loss: 0.0555\n","Epoch 463/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 463: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0290 - val_loss: 0.0555\n","Epoch 464/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 464: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0290 - val_loss: 0.0555\n","Epoch 465/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 465: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0290 - val_loss: 0.0555\n","Epoch 466/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 466: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 467/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 467: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 468/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 468: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 469/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 469: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 470/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 470: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 471/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 471: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 472/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 472: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 473/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 473: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 474/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 474: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 190ms/step - loss: 0.0289 - val_loss: 0.0555\n","Epoch 475/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 475: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 476/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 476: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 477/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 477: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 142ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 478/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 478: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 479/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 479: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 480/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 480: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 481/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 481: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 482/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 482: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 483/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 483: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 484/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 484: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0288 - val_loss: 0.0556\n","Epoch 485/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 485: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 486/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 486: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 214ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 487/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 487: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 175ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 488/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 488: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 133ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 489/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 489: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 490/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 490: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 491/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 491: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 492/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 492: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 493/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 493: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 494/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 494: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 495/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 495: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 496/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 496: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 497/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 497: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 498/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 498: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 196ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 499/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 499: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 256ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 500/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 500: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 501/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 501: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 502/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 502: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 503/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 503: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 504/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 504: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 505/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 505: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 506/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 506: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 507/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 507: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 508/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 508: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 509/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 509: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 510/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 510: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 511/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 511: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 512/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 512: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 513/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 513: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 514/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 514: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 515/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 515: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 516/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 516: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 517/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 517: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 518/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 518: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 519/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 519: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 520/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 520: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 521/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 521: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 522/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 522: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 523/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 523: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 524/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 524: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 525/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 525: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 526/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 526: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 527/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 527: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 528/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 528: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 529/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 529: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0284 - val_loss: 0.0558\n","Epoch 530/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 530: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0284 - val_loss: 0.0558\n","Epoch 531/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 531: val_loss did not improve from 0.05545\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0284 - val_loss: 0.0558\n","1/1 [==============================] - 0s 195ms/step - loss: 0.0734\n","loss_and_metrics : 0.07342682778835297\n","1/1 [==============================] - 0s 187ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","#from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.layers import LeakyReLU\n","#from tensorflow.keras.layers import PReLU\n","#from tensorflow.keras.optimizers import Adam\n","#from tensorflow.keras.optimizers import Nadam\n","#from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"LeakyReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"LeakyReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"LeakyReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"LeakyReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"LeakyReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=1000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}