{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRkw/azczT4Pj0XoagHCeR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VvrGkKz1gA6D","executionInfo":{"status":"ok","timestamp":1695012648882,"user_tz":-540,"elapsed":45392,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"f0faf1dd-4f2e-4b62-84bf-37d42fcd4153"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3663\n","Epoch 1: val_loss improved from inf to 0.34908, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 0.3663 - val_loss: 0.3491\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3625\n","Epoch 2: val_loss improved from 0.34908 to 0.34562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.3625 - val_loss: 0.3456\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3588\n","Epoch 3: val_loss improved from 0.34562 to 0.34221, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.3588 - val_loss: 0.3422\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3552\n","Epoch 4: val_loss improved from 0.34221 to 0.33884, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3552 - val_loss: 0.3388\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3516\n","Epoch 5: val_loss improved from 0.33884 to 0.33551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.3516 - val_loss: 0.3355\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3480\n","Epoch 6: val_loss improved from 0.33551 to 0.33223, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.3480 - val_loss: 0.3322\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3445\n","Epoch 7: val_loss improved from 0.33223 to 0.32900, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3445 - val_loss: 0.3290\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3410\n","Epoch 8: val_loss improved from 0.32900 to 0.32580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.3410 - val_loss: 0.3258\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3376\n","Epoch 9: val_loss improved from 0.32580 to 0.32265, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.3376 - val_loss: 0.3226\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3342\n","Epoch 10: val_loss improved from 0.32265 to 0.31953, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.3342 - val_loss: 0.3195\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3309\n","Epoch 11: val_loss improved from 0.31953 to 0.31645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.3309 - val_loss: 0.3165\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3276\n","Epoch 12: val_loss improved from 0.31645 to 0.31341, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.3276 - val_loss: 0.3134\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3244\n","Epoch 13: val_loss improved from 0.31341 to 0.31040, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3244 - val_loss: 0.3104\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3211\n","Epoch 14: val_loss improved from 0.31040 to 0.30742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.3211 - val_loss: 0.3074\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3179\n","Epoch 15: val_loss improved from 0.30742 to 0.30447, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3179 - val_loss: 0.3045\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3148\n","Epoch 16: val_loss improved from 0.30447 to 0.30155, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3148 - val_loss: 0.3016\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3117\n","Epoch 17: val_loss improved from 0.30155 to 0.29867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3117 - val_loss: 0.2987\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3086\n","Epoch 18: val_loss improved from 0.29867 to 0.29581, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3086 - val_loss: 0.2958\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3055\n","Epoch 19: val_loss improved from 0.29581 to 0.29298, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3055 - val_loss: 0.2930\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3025\n","Epoch 20: val_loss improved from 0.29298 to 0.29018, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.3025 - val_loss: 0.2902\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2995\n","Epoch 21: val_loss improved from 0.29018 to 0.28741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2995 - val_loss: 0.2874\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2965\n","Epoch 22: val_loss improved from 0.28741 to 0.28467, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2965 - val_loss: 0.2847\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2935\n","Epoch 23: val_loss improved from 0.28467 to 0.28196, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2935 - val_loss: 0.2820\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2906\n","Epoch 24: val_loss improved from 0.28196 to 0.27928, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2906 - val_loss: 0.2793\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2877\n","Epoch 25: val_loss improved from 0.27928 to 0.27663, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2877 - val_loss: 0.2766\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2849\n","Epoch 26: val_loss improved from 0.27663 to 0.27401, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2849 - val_loss: 0.2740\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2820\n","Epoch 27: val_loss improved from 0.27401 to 0.27141, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2820 - val_loss: 0.2714\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2792\n","Epoch 28: val_loss improved from 0.27141 to 0.26885, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2792 - val_loss: 0.2688\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2765\n","Epoch 29: val_loss improved from 0.26885 to 0.26631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.2765 - val_loss: 0.2663\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2737\n","Epoch 30: val_loss improved from 0.26631 to 0.26381, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2737 - val_loss: 0.2638\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2710\n","Epoch 31: val_loss improved from 0.26381 to 0.26133, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2710 - val_loss: 0.2613\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2684\n","Epoch 32: val_loss improved from 0.26133 to 0.25888, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2684 - val_loss: 0.2589\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2657\n","Epoch 33: val_loss improved from 0.25888 to 0.25646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2657 - val_loss: 0.2565\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2631\n","Epoch 34: val_loss improved from 0.25646 to 0.25406, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2631 - val_loss: 0.2541\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2605\n","Epoch 35: val_loss improved from 0.25406 to 0.25169, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2605 - val_loss: 0.2517\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2579\n","Epoch 36: val_loss improved from 0.25169 to 0.24935, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2579 - val_loss: 0.2493\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2554\n","Epoch 37: val_loss improved from 0.24935 to 0.24703, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2554 - val_loss: 0.2470\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2529\n","Epoch 38: val_loss improved from 0.24703 to 0.24474, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2529 - val_loss: 0.2447\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2504\n","Epoch 39: val_loss improved from 0.24474 to 0.24247, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2504 - val_loss: 0.2425\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2479\n","Epoch 40: val_loss improved from 0.24247 to 0.24023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2479 - val_loss: 0.2402\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2455\n","Epoch 41: val_loss improved from 0.24023 to 0.23802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2455 - val_loss: 0.2380\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2431\n","Epoch 42: val_loss improved from 0.23802 to 0.23582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2431 - val_loss: 0.2358\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2407\n","Epoch 43: val_loss improved from 0.23582 to 0.23366, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2407 - val_loss: 0.2337\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2384\n","Epoch 44: val_loss improved from 0.23366 to 0.23151, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.2384 - val_loss: 0.2315\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2360\n","Epoch 45: val_loss improved from 0.23151 to 0.22940, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2360 - val_loss: 0.2294\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2337\n","Epoch 46: val_loss improved from 0.22940 to 0.22730, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2337 - val_loss: 0.2273\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2315\n","Epoch 47: val_loss improved from 0.22730 to 0.22523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2315 - val_loss: 0.2252\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2292\n","Epoch 48: val_loss improved from 0.22523 to 0.22318, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2292 - val_loss: 0.2232\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2270\n","Epoch 49: val_loss improved from 0.22318 to 0.22115, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2270 - val_loss: 0.2212\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2248\n","Epoch 50: val_loss improved from 0.22115 to 0.21915, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2248 - val_loss: 0.2191\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2226\n","Epoch 51: val_loss improved from 0.21915 to 0.21717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2226 - val_loss: 0.2172\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2204\n","Epoch 52: val_loss improved from 0.21717 to 0.21521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2204 - val_loss: 0.2152\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2183\n","Epoch 53: val_loss improved from 0.21521 to 0.21327, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2183 - val_loss: 0.2133\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2162\n","Epoch 54: val_loss improved from 0.21327 to 0.21135, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2162 - val_loss: 0.2114\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2141\n","Epoch 55: val_loss improved from 0.21135 to 0.20946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2141 - val_loss: 0.2095\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2120\n","Epoch 56: val_loss improved from 0.20946 to 0.20758, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2120 - val_loss: 0.2076\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2100\n","Epoch 57: val_loss improved from 0.20758 to 0.20573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2100 - val_loss: 0.2057\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2079\n","Epoch 58: val_loss improved from 0.20573 to 0.20389, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.2079 - val_loss: 0.2039\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2059\n","Epoch 59: val_loss improved from 0.20389 to 0.20208, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.2059 - val_loss: 0.2021\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2039\n","Epoch 60: val_loss improved from 0.20208 to 0.20029, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.2039 - val_loss: 0.2003\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2020\n","Epoch 61: val_loss improved from 0.20029 to 0.19851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2020 - val_loss: 0.1985\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2000\n","Epoch 62: val_loss improved from 0.19851 to 0.19676, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2000 - val_loss: 0.1968\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1981\n","Epoch 63: val_loss improved from 0.19676 to 0.19503, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1981 - val_loss: 0.1950\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1962\n","Epoch 64: val_loss improved from 0.19503 to 0.19331, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1962 - val_loss: 0.1933\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1943\n","Epoch 65: val_loss improved from 0.19331 to 0.19161, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1943 - val_loss: 0.1916\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1925\n","Epoch 66: val_loss improved from 0.19161 to 0.18994, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1925 - val_loss: 0.1899\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1906\n","Epoch 67: val_loss improved from 0.18994 to 0.18828, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1906 - val_loss: 0.1883\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1888\n","Epoch 68: val_loss improved from 0.18828 to 0.18664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1888 - val_loss: 0.1866\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1870\n","Epoch 69: val_loss improved from 0.18664 to 0.18501, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1870 - val_loss: 0.1850\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1852\n","Epoch 70: val_loss improved from 0.18501 to 0.18341, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1852 - val_loss: 0.1834\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1834\n","Epoch 71: val_loss improved from 0.18341 to 0.18182, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1834 - val_loss: 0.1818\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1817\n","Epoch 72: val_loss improved from 0.18182 to 0.18025, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1817 - val_loss: 0.1802\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1800\n","Epoch 73: val_loss improved from 0.18025 to 0.17870, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1800 - val_loss: 0.1787\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1782\n","Epoch 74: val_loss improved from 0.17870 to 0.17716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1782 - val_loss: 0.1772\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1766\n","Epoch 75: val_loss improved from 0.17716 to 0.17564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1766 - val_loss: 0.1756\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1749\n","Epoch 76: val_loss improved from 0.17564 to 0.17414, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1749 - val_loss: 0.1741\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1732\n","Epoch 77: val_loss improved from 0.17414 to 0.17266, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1732 - val_loss: 0.1727\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1716\n","Epoch 78: val_loss improved from 0.17266 to 0.17119, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1716 - val_loss: 0.1712\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1700\n","Epoch 79: val_loss improved from 0.17119 to 0.16974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1700 - val_loss: 0.1697\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1684\n","Epoch 80: val_loss improved from 0.16974 to 0.16831, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1684 - val_loss: 0.1683\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1668\n","Epoch 81: val_loss improved from 0.16831 to 0.16689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1668 - val_loss: 0.1669\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1652\n","Epoch 82: val_loss improved from 0.16689 to 0.16549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1652 - val_loss: 0.1655\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1636\n","Epoch 83: val_loss improved from 0.16549 to 0.16410, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1636 - val_loss: 0.1641\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1621\n","Epoch 84: val_loss improved from 0.16410 to 0.16273, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1621 - val_loss: 0.1627\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1606\n","Epoch 85: val_loss improved from 0.16273 to 0.16138, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1606 - val_loss: 0.1614\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1591\n","Epoch 86: val_loss improved from 0.16138 to 0.16004, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1591 - val_loss: 0.1600\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1576\n","Epoch 87: val_loss improved from 0.16004 to 0.15871, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1576 - val_loss: 0.1587\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1561\n","Epoch 88: val_loss improved from 0.15871 to 0.15740, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1561 - val_loss: 0.1574\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1547\n","Epoch 89: val_loss improved from 0.15740 to 0.15611, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1547 - val_loss: 0.1561\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1532\n","Epoch 90: val_loss improved from 0.15611 to 0.15483, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1532 - val_loss: 0.1548\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1518\n","Epoch 91: val_loss improved from 0.15483 to 0.15356, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1518 - val_loss: 0.1536\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1504\n","Epoch 92: val_loss improved from 0.15356 to 0.15231, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1504 - val_loss: 0.1523\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1490\n","Epoch 93: val_loss improved from 0.15231 to 0.15107, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1490 - val_loss: 0.1511\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1476\n","Epoch 94: val_loss improved from 0.15107 to 0.14985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1476 - val_loss: 0.1499\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1462\n","Epoch 95: val_loss improved from 0.14985 to 0.14864, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1462 - val_loss: 0.1486\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1449\n","Epoch 96: val_loss improved from 0.14864 to 0.14745, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1449 - val_loss: 0.1474\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1436\n","Epoch 97: val_loss improved from 0.14745 to 0.14627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1436 - val_loss: 0.1463\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1422\n","Epoch 98: val_loss improved from 0.14627 to 0.14510, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1422 - val_loss: 0.1451\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1409\n","Epoch 99: val_loss improved from 0.14510 to 0.14394, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1409 - val_loss: 0.1439\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1396\n","Epoch 100: val_loss improved from 0.14394 to 0.14280, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1396 - val_loss: 0.1428\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1383\n","Epoch 101: val_loss improved from 0.14280 to 0.14167, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1383 - val_loss: 0.1417\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1371\n","Epoch 102: val_loss improved from 0.14167 to 0.14056, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1371 - val_loss: 0.1406\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1358\n","Epoch 103: val_loss improved from 0.14056 to 0.13945, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1358 - val_loss: 0.1395\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1346\n","Epoch 104: val_loss improved from 0.13945 to 0.13836, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1346 - val_loss: 0.1384\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1334\n","Epoch 105: val_loss improved from 0.13836 to 0.13728, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1334 - val_loss: 0.1373\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1321\n","Epoch 106: val_loss improved from 0.13728 to 0.13622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1321 - val_loss: 0.1362\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1309\n","Epoch 107: val_loss improved from 0.13622 to 0.13517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1309 - val_loss: 0.1352\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1298\n","Epoch 108: val_loss improved from 0.13517 to 0.13412, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1298 - val_loss: 0.1341\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1286\n","Epoch 109: val_loss improved from 0.13412 to 0.13310, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1286 - val_loss: 0.1331\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1274\n","Epoch 110: val_loss improved from 0.13310 to 0.13208, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1274 - val_loss: 0.1321\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1263\n","Epoch 111: val_loss improved from 0.13208 to 0.13107, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1263 - val_loss: 0.1311\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1251\n","Epoch 112: val_loss improved from 0.13107 to 0.13008, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1251 - val_loss: 0.1301\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1240\n","Epoch 113: val_loss improved from 0.13008 to 0.12910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1240 - val_loss: 0.1291\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1229\n","Epoch 114: val_loss improved from 0.12910 to 0.12813, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1229 - val_loss: 0.1281\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1218\n","Epoch 115: val_loss improved from 0.12813 to 0.12717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1218 - val_loss: 0.1272\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1207\n","Epoch 116: val_loss improved from 0.12717 to 0.12622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1207 - val_loss: 0.1262\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1196\n","Epoch 117: val_loss improved from 0.12622 to 0.12528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1196 - val_loss: 0.1253\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1186\n","Epoch 118: val_loss improved from 0.12528 to 0.12436, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1186 - val_loss: 0.1244\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1175\n","Epoch 119: val_loss improved from 0.12436 to 0.12344, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1175 - val_loss: 0.1234\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1165\n","Epoch 120: val_loss improved from 0.12344 to 0.12254, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1165 - val_loss: 0.1225\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1154\n","Epoch 121: val_loss improved from 0.12254 to 0.12164, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1154 - val_loss: 0.1216\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1144\n","Epoch 122: val_loss improved from 0.12164 to 0.12076, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1144 - val_loss: 0.1208\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1134\n","Epoch 123: val_loss improved from 0.12076 to 0.11989, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1134 - val_loss: 0.1199\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1124\n","Epoch 124: val_loss improved from 0.11989 to 0.11903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1124 - val_loss: 0.1190\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1114\n","Epoch 125: val_loss improved from 0.11903 to 0.11817, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1114 - val_loss: 0.1182\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1105\n","Epoch 126: val_loss improved from 0.11817 to 0.11733, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1105 - val_loss: 0.1173\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1095\n","Epoch 127: val_loss improved from 0.11733 to 0.11650, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1095 - val_loss: 0.1165\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1085\n","Epoch 128: val_loss improved from 0.11650 to 0.11568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1085 - val_loss: 0.1157\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1076\n","Epoch 129: val_loss improved from 0.11568 to 0.11486, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.1076 - val_loss: 0.1149\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1067\n","Epoch 130: val_loss improved from 0.11486 to 0.11406, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1067 - val_loss: 0.1141\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1057\n","Epoch 131: val_loss improved from 0.11406 to 0.11327, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1057 - val_loss: 0.1133\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1048\n","Epoch 132: val_loss improved from 0.11327 to 0.11248, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1048 - val_loss: 0.1125\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1039\n","Epoch 133: val_loss improved from 0.11248 to 0.11171, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1039 - val_loss: 0.1117\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1030\n","Epoch 134: val_loss improved from 0.11171 to 0.11094, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1030 - val_loss: 0.1109\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1022\n","Epoch 135: val_loss improved from 0.11094 to 0.11019, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1022 - val_loss: 0.1102\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1013\n","Epoch 136: val_loss improved from 0.11019 to 0.10944, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1013 - val_loss: 0.1094\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1004\n","Epoch 137: val_loss improved from 0.10944 to 0.10870, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1004 - val_loss: 0.1087\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0996\n","Epoch 138: val_loss improved from 0.10870 to 0.10797, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0996 - val_loss: 0.1080\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0987\n","Epoch 139: val_loss improved from 0.10797 to 0.10725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0987 - val_loss: 0.1072\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0979\n","Epoch 140: val_loss improved from 0.10725 to 0.10654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0979 - val_loss: 0.1065\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0971\n","Epoch 141: val_loss improved from 0.10654 to 0.10583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0971 - val_loss: 0.1058\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0962\n","Epoch 142: val_loss improved from 0.10583 to 0.10514, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0962 - val_loss: 0.1051\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0954\n","Epoch 143: val_loss improved from 0.10514 to 0.10445, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0954 - val_loss: 0.1045\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0946\n","Epoch 144: val_loss improved from 0.10445 to 0.10377, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0946 - val_loss: 0.1038\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0938\n","Epoch 145: val_loss improved from 0.10377 to 0.10310, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0938 - val_loss: 0.1031\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0931\n","Epoch 146: val_loss improved from 0.10310 to 0.10244, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0931 - val_loss: 0.1024\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0923\n","Epoch 147: val_loss improved from 0.10244 to 0.10179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0923 - val_loss: 0.1018\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0915\n","Epoch 148: val_loss improved from 0.10179 to 0.10114, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0915 - val_loss: 0.1011\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0908\n","Epoch 149: val_loss improved from 0.10114 to 0.10050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0908 - val_loss: 0.1005\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0900\n","Epoch 150: val_loss improved from 0.10050 to 0.09987, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0900 - val_loss: 0.0999\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0893\n","Epoch 151: val_loss improved from 0.09987 to 0.09925, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0893 - val_loss: 0.0992\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0886\n","Epoch 152: val_loss improved from 0.09925 to 0.09863, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0886 - val_loss: 0.0986\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0878\n","Epoch 153: val_loss improved from 0.09863 to 0.09803, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0878 - val_loss: 0.0980\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0871\n","Epoch 154: val_loss improved from 0.09803 to 0.09743, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0871 - val_loss: 0.0974\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0864\n","Epoch 155: val_loss improved from 0.09743 to 0.09683, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0864 - val_loss: 0.0968\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0857\n","Epoch 156: val_loss improved from 0.09683 to 0.09625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0857 - val_loss: 0.0962\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0850\n","Epoch 157: val_loss improved from 0.09625 to 0.09567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0850 - val_loss: 0.0957\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0843\n","Epoch 158: val_loss improved from 0.09567 to 0.09510, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0843 - val_loss: 0.0951\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0837\n","Epoch 159: val_loss improved from 0.09510 to 0.09453, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0837 - val_loss: 0.0945\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0830\n","Epoch 160: val_loss improved from 0.09453 to 0.09398, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0830 - val_loss: 0.0940\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0823\n","Epoch 161: val_loss improved from 0.09398 to 0.09343, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0823 - val_loss: 0.0934\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0817\n","Epoch 162: val_loss improved from 0.09343 to 0.09288, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0817 - val_loss: 0.0929\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0810\n","Epoch 163: val_loss improved from 0.09288 to 0.09235, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0810 - val_loss: 0.0923\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0804\n","Epoch 164: val_loss improved from 0.09235 to 0.09182, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0804 - val_loss: 0.0918\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0798\n","Epoch 165: val_loss improved from 0.09182 to 0.09129, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0798 - val_loss: 0.0913\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0792\n","Epoch 166: val_loss improved from 0.09129 to 0.09077, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0792 - val_loss: 0.0908\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0785\n","Epoch 167: val_loss improved from 0.09077 to 0.09026, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0785 - val_loss: 0.0903\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0779\n","Epoch 168: val_loss improved from 0.09026 to 0.08976, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0779 - val_loss: 0.0898\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0773\n","Epoch 169: val_loss improved from 0.08976 to 0.08926, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0773 - val_loss: 0.0893\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0767\n","Epoch 170: val_loss improved from 0.08926 to 0.08877, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0767 - val_loss: 0.0888\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0761\n","Epoch 171: val_loss improved from 0.08877 to 0.08828, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0761 - val_loss: 0.0883\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0756\n","Epoch 172: val_loss improved from 0.08828 to 0.08780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0756 - val_loss: 0.0878\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0750\n","Epoch 173: val_loss improved from 0.08780 to 0.08733, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0750 - val_loss: 0.0873\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0744\n","Epoch 174: val_loss improved from 0.08733 to 0.08686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0744 - val_loss: 0.0869\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0739\n","Epoch 175: val_loss improved from 0.08686 to 0.08640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 118ms/step - loss: 0.0739 - val_loss: 0.0864\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0733\n","Epoch 176: val_loss improved from 0.08640 to 0.08595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.0733 - val_loss: 0.0859\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0727\n","Epoch 177: val_loss improved from 0.08595 to 0.08550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0727 - val_loss: 0.0855\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0722\n","Epoch 178: val_loss improved from 0.08550 to 0.08505, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0722 - val_loss: 0.0851\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0717\n","Epoch 179: val_loss improved from 0.08505 to 0.08461, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0717 - val_loss: 0.0846\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0711\n","Epoch 180: val_loss improved from 0.08461 to 0.08418, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0711 - val_loss: 0.0842\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0706\n","Epoch 181: val_loss improved from 0.08418 to 0.08375, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0706 - val_loss: 0.0838\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0701\n","Epoch 182: val_loss improved from 0.08375 to 0.08333, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0701 - val_loss: 0.0833\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0696\n","Epoch 183: val_loss improved from 0.08333 to 0.08291, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0696 - val_loss: 0.0829\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0691\n","Epoch 184: val_loss improved from 0.08291 to 0.08250, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0691 - val_loss: 0.0825\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0686\n","Epoch 185: val_loss improved from 0.08250 to 0.08209, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0686 - val_loss: 0.0821\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0681\n","Epoch 186: val_loss improved from 0.08209 to 0.08169, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0681 - val_loss: 0.0817\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0676\n","Epoch 187: val_loss improved from 0.08169 to 0.08130, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0676 - val_loss: 0.0813\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0671\n","Epoch 188: val_loss improved from 0.08130 to 0.08091, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0671 - val_loss: 0.0809\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0666\n","Epoch 189: val_loss improved from 0.08091 to 0.08052, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0666 - val_loss: 0.0805\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0661\n","Epoch 190: val_loss improved from 0.08052 to 0.08014, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0661 - val_loss: 0.0801\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0657\n","Epoch 191: val_loss improved from 0.08014 to 0.07977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0657 - val_loss: 0.0798\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0652\n","Epoch 192: val_loss improved from 0.07977 to 0.07939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0652 - val_loss: 0.0794\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0648\n","Epoch 193: val_loss improved from 0.07939 to 0.07903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0648 - val_loss: 0.0790\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0643\n","Epoch 194: val_loss improved from 0.07903 to 0.07867, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0643 - val_loss: 0.0787\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0639\n","Epoch 195: val_loss improved from 0.07867 to 0.07831, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0639 - val_loss: 0.0783\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0634\n","Epoch 196: val_loss improved from 0.07831 to 0.07796, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0634 - val_loss: 0.0780\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0630\n","Epoch 197: val_loss improved from 0.07796 to 0.07761, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0630 - val_loss: 0.0776\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0625\n","Epoch 198: val_loss improved from 0.07761 to 0.07727, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0625 - val_loss: 0.0773\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0621\n","Epoch 199: val_loss improved from 0.07727 to 0.07693, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0621 - val_loss: 0.0769\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0617\n","Epoch 200: val_loss improved from 0.07693 to 0.07660, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0617 - val_loss: 0.0766\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0613\n","Epoch 201: val_loss improved from 0.07660 to 0.07627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0613 - val_loss: 0.0763\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0609\n","Epoch 202: val_loss improved from 0.07627 to 0.07594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0609 - val_loss: 0.0759\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0605\n","Epoch 203: val_loss improved from 0.07594 to 0.07562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0605 - val_loss: 0.0756\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0601\n","Epoch 204: val_loss improved from 0.07562 to 0.07531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0601 - val_loss: 0.0753\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0597\n","Epoch 205: val_loss improved from 0.07531 to 0.07499, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0597 - val_loss: 0.0750\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0593\n","Epoch 206: val_loss improved from 0.07499 to 0.07469, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0593 - val_loss: 0.0747\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0589\n","Epoch 207: val_loss improved from 0.07469 to 0.07438, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0589 - val_loss: 0.0744\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0585\n","Epoch 208: val_loss improved from 0.07438 to 0.07408, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0585 - val_loss: 0.0741\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0581\n","Epoch 209: val_loss improved from 0.07408 to 0.07379, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0581 - val_loss: 0.0738\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0577\n","Epoch 210: val_loss improved from 0.07379 to 0.07350, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0577 - val_loss: 0.0735\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0574\n","Epoch 211: val_loss improved from 0.07350 to 0.07321, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0574 - val_loss: 0.0732\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0570\n","Epoch 212: val_loss improved from 0.07321 to 0.07292, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0570 - val_loss: 0.0729\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0567\n","Epoch 213: val_loss improved from 0.07292 to 0.07264, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0567 - val_loss: 0.0726\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0563\n","Epoch 214: val_loss improved from 0.07264 to 0.07237, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0563 - val_loss: 0.0724\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0559\n","Epoch 215: val_loss improved from 0.07237 to 0.07210, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0559 - val_loss: 0.0721\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0556\n","Epoch 216: val_loss improved from 0.07210 to 0.07183, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0556 - val_loss: 0.0718\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0552\n","Epoch 217: val_loss improved from 0.07183 to 0.07156, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0552 - val_loss: 0.0716\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0549\n","Epoch 218: val_loss improved from 0.07156 to 0.07130, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0549 - val_loss: 0.0713\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0546\n","Epoch 219: val_loss improved from 0.07130 to 0.07104, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0546 - val_loss: 0.0710\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0542\n","Epoch 220: val_loss improved from 0.07104 to 0.07079, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0542 - val_loss: 0.0708\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0539\n","Epoch 221: val_loss improved from 0.07079 to 0.07054, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0539 - val_loss: 0.0705\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0536\n","Epoch 222: val_loss improved from 0.07054 to 0.07029, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0536 - val_loss: 0.0703\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0533\n","Epoch 223: val_loss improved from 0.07029 to 0.07004, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0533 - val_loss: 0.0700\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0529\n","Epoch 224: val_loss improved from 0.07004 to 0.06980, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0529 - val_loss: 0.0698\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0526\n","Epoch 225: val_loss improved from 0.06980 to 0.06957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0526 - val_loss: 0.0696\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0523\n","Epoch 226: val_loss improved from 0.06957 to 0.06933, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0523 - val_loss: 0.0693\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0520\n","Epoch 227: val_loss improved from 0.06933 to 0.06910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0520 - val_loss: 0.0691\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0517\n","Epoch 228: val_loss improved from 0.06910 to 0.06887, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0517 - val_loss: 0.0689\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0514\n","Epoch 229: val_loss improved from 0.06887 to 0.06865, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0514 - val_loss: 0.0686\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0511\n","Epoch 230: val_loss improved from 0.06865 to 0.06843, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0511 - val_loss: 0.0684\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0508\n","Epoch 231: val_loss improved from 0.06843 to 0.06821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0508 - val_loss: 0.0682\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0505\n","Epoch 232: val_loss improved from 0.06821 to 0.06799, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0505 - val_loss: 0.0680\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0503\n","Epoch 233: val_loss improved from 0.06799 to 0.06778, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0503 - val_loss: 0.0678\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0500\n","Epoch 234: val_loss improved from 0.06778 to 0.06757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0500 - val_loss: 0.0676\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0497\n","Epoch 235: val_loss improved from 0.06757 to 0.06736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0497 - val_loss: 0.0674\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0494\n","Epoch 236: val_loss improved from 0.06736 to 0.06716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0494 - val_loss: 0.0672\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0492\n","Epoch 237: val_loss improved from 0.06716 to 0.06696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0492 - val_loss: 0.0670\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0489\n","Epoch 238: val_loss improved from 0.06696 to 0.06676, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0489 - val_loss: 0.0668\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0486\n","Epoch 239: val_loss improved from 0.06676 to 0.06657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0486 - val_loss: 0.0666\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0484\n","Epoch 240: val_loss improved from 0.06657 to 0.06638, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0484 - val_loss: 0.0664\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0481\n","Epoch 241: val_loss improved from 0.06638 to 0.06619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0481 - val_loss: 0.0662\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 242: val_loss improved from 0.06619 to 0.06600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0479 - val_loss: 0.0660\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0476\n","Epoch 243: val_loss improved from 0.06600 to 0.06582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0476 - val_loss: 0.0658\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 244: val_loss improved from 0.06582 to 0.06563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0474 - val_loss: 0.0656\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0471\n","Epoch 245: val_loss improved from 0.06563 to 0.06546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0471 - val_loss: 0.0655\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0469\n","Epoch 246: val_loss improved from 0.06546 to 0.06528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0469 - val_loss: 0.0653\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0466\n","Epoch 247: val_loss improved from 0.06528 to 0.06511, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0466 - val_loss: 0.0651\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0464\n","Epoch 248: val_loss improved from 0.06511 to 0.06493, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0464 - val_loss: 0.0649\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 249: val_loss improved from 0.06493 to 0.06477, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0462 - val_loss: 0.0648\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0459\n","Epoch 250: val_loss improved from 0.06477 to 0.06460, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0459 - val_loss: 0.0646\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0457\n","Epoch 251: val_loss improved from 0.06460 to 0.06444, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0457 - val_loss: 0.0644\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 252: val_loss improved from 0.06444 to 0.06428, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0455 - val_loss: 0.0643\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0452\n","Epoch 253: val_loss improved from 0.06428 to 0.06412, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0452 - val_loss: 0.0641\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0450\n","Epoch 254: val_loss improved from 0.06412 to 0.06396, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0450 - val_loss: 0.0640\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0448\n","Epoch 255: val_loss improved from 0.06396 to 0.06381, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0448 - val_loss: 0.0638\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 256: val_loss improved from 0.06381 to 0.06365, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0446 - val_loss: 0.0637\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 257: val_loss improved from 0.06365 to 0.06350, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0444 - val_loss: 0.0635\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 258: val_loss improved from 0.06350 to 0.06336, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0442 - val_loss: 0.0634\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 259: val_loss improved from 0.06336 to 0.06321, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0440 - val_loss: 0.0632\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0438\n","Epoch 260: val_loss improved from 0.06321 to 0.06307, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0438 - val_loss: 0.0631\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 261: val_loss improved from 0.06307 to 0.06293, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0436 - val_loss: 0.0629\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 262: val_loss improved from 0.06293 to 0.06279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0434 - val_loss: 0.0628\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 263: val_loss improved from 0.06279 to 0.06265, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0432 - val_loss: 0.0627\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 264: val_loss improved from 0.06265 to 0.06252, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0430 - val_loss: 0.0625\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 265: val_loss improved from 0.06252 to 0.06238, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0428 - val_loss: 0.0624\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0426\n","Epoch 266: val_loss improved from 0.06238 to 0.06225, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0426 - val_loss: 0.0623\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 267: val_loss improved from 0.06225 to 0.06212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0424 - val_loss: 0.0621\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0422\n","Epoch 268: val_loss improved from 0.06212 to 0.06200, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0422 - val_loss: 0.0620\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 269: val_loss improved from 0.06200 to 0.06187, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0420 - val_loss: 0.0619\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 270: val_loss improved from 0.06187 to 0.06175, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0419 - val_loss: 0.0617\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 271: val_loss improved from 0.06175 to 0.06163, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0417 - val_loss: 0.0616\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0415\n","Epoch 272: val_loss improved from 0.06163 to 0.06151, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0415 - val_loss: 0.0615\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 273: val_loss improved from 0.06151 to 0.06139, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0413 - val_loss: 0.0614\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0412\n","Epoch 274: val_loss improved from 0.06139 to 0.06128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0412 - val_loss: 0.0613\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0410\n","Epoch 275: val_loss improved from 0.06128 to 0.06116, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0410 - val_loss: 0.0612\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 276: val_loss improved from 0.06116 to 0.06105, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0408 - val_loss: 0.0611\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 277: val_loss improved from 0.06105 to 0.06094, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0407 - val_loss: 0.0609\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0405\n","Epoch 278: val_loss improved from 0.06094 to 0.06083, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0405 - val_loss: 0.0608\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0403\n","Epoch 279: val_loss improved from 0.06083 to 0.06072, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0403 - val_loss: 0.0607\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 280: val_loss improved from 0.06072 to 0.06062, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0402 - val_loss: 0.0606\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0400\n","Epoch 281: val_loss improved from 0.06062 to 0.06052, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0400 - val_loss: 0.0605\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 282: val_loss improved from 0.06052 to 0.06041, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0399 - val_loss: 0.0604\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0397\n","Epoch 283: val_loss improved from 0.06041 to 0.06031, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0397 - val_loss: 0.0603\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 284: val_loss improved from 0.06031 to 0.06021, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0395 - val_loss: 0.0602\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 285: val_loss improved from 0.06021 to 0.06012, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0394 - val_loss: 0.0601\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0393\n","Epoch 286: val_loss improved from 0.06012 to 0.06002, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0393 - val_loss: 0.0600\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 287: val_loss improved from 0.06002 to 0.05993, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0391 - val_loss: 0.0599\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 288: val_loss improved from 0.05993 to 0.05984, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0390 - val_loss: 0.0598\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0388\n","Epoch 289: val_loss improved from 0.05984 to 0.05974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0388 - val_loss: 0.0597\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 290: val_loss improved from 0.05974 to 0.05965, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0387 - val_loss: 0.0597\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 291: val_loss improved from 0.05965 to 0.05957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0385 - val_loss: 0.0596\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 292: val_loss improved from 0.05957 to 0.05948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0384 - val_loss: 0.0595\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 293: val_loss improved from 0.05948 to 0.05939, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0383 - val_loss: 0.0594\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 294: val_loss improved from 0.05939 to 0.05931, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0381 - val_loss: 0.0593\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 295: val_loss improved from 0.05931 to 0.05923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0380 - val_loss: 0.0592\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0379\n","Epoch 296: val_loss improved from 0.05923 to 0.05915, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0379 - val_loss: 0.0591\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 297: val_loss improved from 0.05915 to 0.05907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0377 - val_loss: 0.0591\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 298: val_loss improved from 0.05907 to 0.05899, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0376 - val_loss: 0.0590\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0375\n","Epoch 299: val_loss improved from 0.05899 to 0.05891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0375 - val_loss: 0.0589\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 300: val_loss improved from 0.05891 to 0.05884, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0374 - val_loss: 0.0588\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 301: val_loss improved from 0.05884 to 0.05876, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0373 - val_loss: 0.0588\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 302: val_loss improved from 0.05876 to 0.05869, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0371 - val_loss: 0.0587\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 303: val_loss improved from 0.05869 to 0.05862, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0370 - val_loss: 0.0586\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 304: val_loss improved from 0.05862 to 0.05854, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0369 - val_loss: 0.0585\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0368\n","Epoch 305: val_loss improved from 0.05854 to 0.05847, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0368 - val_loss: 0.0585\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 306: val_loss improved from 0.05847 to 0.05841, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0367 - val_loss: 0.0584\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 307: val_loss improved from 0.05841 to 0.05834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0366 - val_loss: 0.0583\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 308: val_loss improved from 0.05834 to 0.05827, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0364 - val_loss: 0.0583\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 309: val_loss improved from 0.05827 to 0.05821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0363 - val_loss: 0.0582\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 310: val_loss improved from 0.05821 to 0.05814, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0362 - val_loss: 0.0581\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 311: val_loss improved from 0.05814 to 0.05808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0361 - val_loss: 0.0581\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 312: val_loss improved from 0.05808 to 0.05802, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0360 - val_loss: 0.0580\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 313: val_loss improved from 0.05802 to 0.05796, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0359 - val_loss: 0.0580\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 314: val_loss improved from 0.05796 to 0.05790, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0358 - val_loss: 0.0579\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 315: val_loss improved from 0.05790 to 0.05784, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0357 - val_loss: 0.0578\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 316: val_loss improved from 0.05784 to 0.05778, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0356 - val_loss: 0.0578\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 317: val_loss improved from 0.05778 to 0.05773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0355 - val_loss: 0.0577\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 318: val_loss improved from 0.05773 to 0.05767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0354 - val_loss: 0.0577\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 319: val_loss improved from 0.05767 to 0.05761, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0353 - val_loss: 0.0576\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 320: val_loss improved from 0.05761 to 0.05756, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0352 - val_loss: 0.0576\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 321: val_loss improved from 0.05756 to 0.05751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0351 - val_loss: 0.0575\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 322: val_loss improved from 0.05751 to 0.05746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0350 - val_loss: 0.0575\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 323: val_loss improved from 0.05746 to 0.05741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0349 - val_loss: 0.0574\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 324: val_loss improved from 0.05741 to 0.05736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0348 - val_loss: 0.0574\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 325: val_loss improved from 0.05736 to 0.05731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0347 - val_loss: 0.0573\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 326: val_loss improved from 0.05731 to 0.05726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0347 - val_loss: 0.0573\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 327: val_loss improved from 0.05726 to 0.05721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0346 - val_loss: 0.0572\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 328: val_loss improved from 0.05721 to 0.05717, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0345 - val_loss: 0.0572\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 329: val_loss improved from 0.05717 to 0.05712, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0344 - val_loss: 0.0571\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 330: val_loss improved from 0.05712 to 0.05708, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0343 - val_loss: 0.0571\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 331: val_loss improved from 0.05708 to 0.05703, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.0342 - val_loss: 0.0570\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 332: val_loss improved from 0.05703 to 0.05699, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0342 - val_loss: 0.0570\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 333: val_loss improved from 0.05699 to 0.05695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0341 - val_loss: 0.0569\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 334: val_loss improved from 0.05695 to 0.05690, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0340 - val_loss: 0.0569\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 335: val_loss improved from 0.05690 to 0.05686, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0339 - val_loss: 0.0569\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 336: val_loss improved from 0.05686 to 0.05682, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0338 - val_loss: 0.0568\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 337: val_loss improved from 0.05682 to 0.05679, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0338 - val_loss: 0.0568\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 338: val_loss improved from 0.05679 to 0.05675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0337 - val_loss: 0.0567\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 339: val_loss improved from 0.05675 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0336 - val_loss: 0.0567\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 340: val_loss improved from 0.05671 to 0.05667, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0335 - val_loss: 0.0567\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 341: val_loss improved from 0.05667 to 0.05664, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0335 - val_loss: 0.0566\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 342: val_loss improved from 0.05664 to 0.05660, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0334 - val_loss: 0.0566\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 343: val_loss improved from 0.05660 to 0.05657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0333 - val_loss: 0.0566\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 344: val_loss improved from 0.05657 to 0.05653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0333 - val_loss: 0.0565\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 345: val_loss improved from 0.05653 to 0.05650, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0332 - val_loss: 0.0565\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 346: val_loss improved from 0.05650 to 0.05647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0331 - val_loss: 0.0565\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 347: val_loss improved from 0.05647 to 0.05643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0330 - val_loss: 0.0564\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 348: val_loss improved from 0.05643 to 0.05640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0330 - val_loss: 0.0564\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 349: val_loss improved from 0.05640 to 0.05637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0329 - val_loss: 0.0564\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 350: val_loss improved from 0.05637 to 0.05634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0329 - val_loss: 0.0563\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 351: val_loss improved from 0.05634 to 0.05631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0328 - val_loss: 0.0563\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 352: val_loss improved from 0.05631 to 0.05628, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0327 - val_loss: 0.0563\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 353: val_loss improved from 0.05628 to 0.05626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0327 - val_loss: 0.0563\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 354: val_loss improved from 0.05626 to 0.05623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0326 - val_loss: 0.0562\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 355: val_loss improved from 0.05623 to 0.05620, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0325 - val_loss: 0.0562\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 356: val_loss improved from 0.05620 to 0.05617, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0325 - val_loss: 0.0562\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 357: val_loss improved from 0.05617 to 0.05615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0324 - val_loss: 0.0561\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 358: val_loss improved from 0.05615 to 0.05612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0324 - val_loss: 0.0561\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 359: val_loss improved from 0.05612 to 0.05610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0323 - val_loss: 0.0561\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 360: val_loss improved from 0.05610 to 0.05607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0323 - val_loss: 0.0561\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 361: val_loss improved from 0.05607 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0322 - val_loss: 0.0560\n","Epoch 362/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 362: val_loss improved from 0.05605 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0321 - val_loss: 0.0560\n","Epoch 363/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 363: val_loss improved from 0.05603 to 0.05600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0321 - val_loss: 0.0560\n","Epoch 364/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 364: val_loss improved from 0.05600 to 0.05598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0320 - val_loss: 0.0560\n","Epoch 365/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 365: val_loss improved from 0.05598 to 0.05596, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0320 - val_loss: 0.0560\n","Epoch 366/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 366: val_loss improved from 0.05596 to 0.05594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0319 - val_loss: 0.0559\n","Epoch 367/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 367: val_loss improved from 0.05594 to 0.05592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0319 - val_loss: 0.0559\n","Epoch 368/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 368: val_loss improved from 0.05592 to 0.05590, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0318 - val_loss: 0.0559\n","Epoch 369/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 369: val_loss improved from 0.05590 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0318 - val_loss: 0.0559\n","Epoch 370/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 370: val_loss improved from 0.05588 to 0.05586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0317 - val_loss: 0.0559\n","Epoch 371/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 371: val_loss improved from 0.05586 to 0.05584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0317 - val_loss: 0.0558\n","Epoch 372/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 372: val_loss improved from 0.05584 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0316 - val_loss: 0.0558\n","Epoch 373/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 373: val_loss improved from 0.05582 to 0.05580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0316 - val_loss: 0.0558\n","Epoch 374/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 374: val_loss improved from 0.05580 to 0.05578, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0315 - val_loss: 0.0558\n","Epoch 375/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 375: val_loss improved from 0.05578 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0315 - val_loss: 0.0558\n","Epoch 376/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 376: val_loss improved from 0.05577 to 0.05575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0314 - val_loss: 0.0557\n","Epoch 377/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 377: val_loss improved from 0.05575 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0314 - val_loss: 0.0557\n","Epoch 378/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 378: val_loss improved from 0.05573 to 0.05572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0314 - val_loss: 0.0557\n","Epoch 379/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 379: val_loss improved from 0.05572 to 0.05570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0313 - val_loss: 0.0557\n","Epoch 380/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 380: val_loss improved from 0.05570 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0313 - val_loss: 0.0557\n","Epoch 381/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 381: val_loss improved from 0.05569 to 0.05567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0312 - val_loss: 0.0557\n","Epoch 382/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 382: val_loss improved from 0.05567 to 0.05566, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0312 - val_loss: 0.0557\n","Epoch 383/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 383: val_loss improved from 0.05566 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0311 - val_loss: 0.0556\n","Epoch 384/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 384: val_loss improved from 0.05564 to 0.05563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0311 - val_loss: 0.0556\n","Epoch 385/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 385: val_loss improved from 0.05563 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0311 - val_loss: 0.0556\n","Epoch 386/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 386: val_loss improved from 0.05562 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0310 - val_loss: 0.0556\n","Epoch 387/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 387: val_loss improved from 0.05560 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0310 - val_loss: 0.0556\n","Epoch 388/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 388: val_loss improved from 0.05559 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0309 - val_loss: 0.0556\n","Epoch 389/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 389: val_loss improved from 0.05558 to 0.05557, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0309 - val_loss: 0.0556\n","Epoch 390/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 390: val_loss improved from 0.05557 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0309 - val_loss: 0.0556\n","Epoch 391/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 391: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0308 - val_loss: 0.0555\n","Epoch 392/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 392: val_loss improved from 0.05554 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0308 - val_loss: 0.0555\n","Epoch 393/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 393: val_loss improved from 0.05553 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0308 - val_loss: 0.0555\n","Epoch 394/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 394: val_loss improved from 0.05552 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0307 - val_loss: 0.0555\n","Epoch 395/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 395: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0307 - val_loss: 0.0555\n","Epoch 396/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 396: val_loss improved from 0.05550 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0306 - val_loss: 0.0555\n","Epoch 397/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 397: val_loss improved from 0.05549 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0306 - val_loss: 0.0555\n","Epoch 398/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 398: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0306 - val_loss: 0.0555\n","Epoch 399/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 399: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0305 - val_loss: 0.0555\n","Epoch 400/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 400: val_loss improved from 0.05546 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0305 - val_loss: 0.0555\n","Epoch 401/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 401: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0305 - val_loss: 0.0554\n","Epoch 402/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 402: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0304 - val_loss: 0.0554\n","Epoch 403/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 403: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0304 - val_loss: 0.0554\n","Epoch 404/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 404: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0304 - val_loss: 0.0554\n","Epoch 405/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 405: val_loss improved from 0.05542 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 406/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 406: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 407/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 407: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 408/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 408: val_loss improved from 0.05540 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 409/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 409: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0302 - val_loss: 0.0554\n","Epoch 410/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 410: val_loss improved from 0.05539 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0302 - val_loss: 0.0554\n","Epoch 411/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 411: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0302 - val_loss: 0.0554\n","Epoch 412/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 412: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0301 - val_loss: 0.0554\n","Epoch 413/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 413: val_loss improved from 0.05537 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0301 - val_loss: 0.0554\n","Epoch 414/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 414: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0301 - val_loss: 0.0554\n","Epoch 415/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 415: val_loss improved from 0.05536 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0301 - val_loss: 0.0554\n","Epoch 416/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 416: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0300 - val_loss: 0.0554\n","Epoch 417/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 417: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 418/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 418: val_loss improved from 0.05535 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 419/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 419: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 420/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 420: val_loss improved from 0.05534 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0299 - val_loss: 0.0553\n","Epoch 421/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 421: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0299 - val_loss: 0.0553\n","Epoch 422/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 422: val_loss improved from 0.05533 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0299 - val_loss: 0.0553\n","Epoch 423/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 423: val_loss improved from 0.05533 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0299 - val_loss: 0.0553\n","Epoch 424/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 424: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0298 - val_loss: 0.0553\n","Epoch 425/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 425: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0298 - val_loss: 0.0553\n","Epoch 426/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 426: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0298 - val_loss: 0.0553\n","Epoch 427/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 427: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0298 - val_loss: 0.0553\n","Epoch 428/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 428: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0297 - val_loss: 0.0553\n","Epoch 429/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 429: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0297 - val_loss: 0.0553\n","Epoch 430/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 430: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0297 - val_loss: 0.0553\n","Epoch 431/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 431: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0297 - val_loss: 0.0553\n","Epoch 432/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 432: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0297 - val_loss: 0.0553\n","Epoch 433/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 433: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 434/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 434: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 435/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 435: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 436/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 436: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 437/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 437: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 438/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 438: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 439/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 439: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 440/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 440: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 441/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 441: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 442/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 442: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 443/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 443: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 444/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 444: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 445/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 445: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 446/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 446: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 447/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 447: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 448/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 448: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 449/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 449: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 450/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 450: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 451/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 451: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 452/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 452: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 453/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 453: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 454/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 454: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 455/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 455: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 456/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 456: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 457/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 457: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 458/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 458: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 459/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 459: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 460/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 460: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 461/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 461: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 462/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 462: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 463/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 463: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 464/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 464: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 465/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 465: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 466/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 466: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 467/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 467: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 468/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 468: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 469/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 469: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 470/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 470: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 471/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 471: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 472/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 472: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 473/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 473: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 474/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 474: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 475/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 475: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 476/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 476: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 477/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 477: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 478/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 478: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 479/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 479: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 480/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 480: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 481/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 481: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 482/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 482: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 483/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 483: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 484/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 484: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 485/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 485: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 486/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 486: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 487/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 487: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 488/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 488: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 489/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 489: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 490/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 490: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 491/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 491: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 492/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 492: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 493/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 493: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 494/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 494: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 495/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 495: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 496/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 496: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 497/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 497: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 498/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 498: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 499/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 499: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 500/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 500: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 501/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 501: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 502/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 502: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 503/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 503: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 504/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 504: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 505/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 505: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 506/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 506: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 507/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 507: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 508/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 508: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 509/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 509: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 510/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 510: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 511/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 511: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 512/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 512: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 513/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 513: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 514/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 514: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 515/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 515: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 516/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 516: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 517/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 517: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 518/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 518: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 519/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 519: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 520/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 520: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 521/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 521: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 522/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 522: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 523/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 523: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 524/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 524: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 525/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 525: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 526/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 526: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 527/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 527: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 528/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 528: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 529/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 529: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 530/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 530: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 531/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 531: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 532/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 532: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 533/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 533: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 534/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 534: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 535/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 535: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 536/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 536: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 537/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 537: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 538/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 538: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 283ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 539/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 539: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 186ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 540/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 540: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 138ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 541/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 541: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 165ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 542/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 542: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 543/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 543: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 544/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 544: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0284 - val_loss: 0.0556\n","1/1 [==============================] - 1s 577ms/step - loss: 0.0734\n","loss_and_metrics : 0.0733538493514061\n","1/1 [==============================] - 0s 115ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}