{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOz4LQZlKSKHm1/PvF6b8MK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UpzZKbJr11RK","executionInfo":{"status":"ok","timestamp":1695016878565,"user_tz":-540,"elapsed":57086,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"b6f120cc-3c41-4d31-d0e6-2a7b2b348262"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3673\n","Epoch 1: val_loss improved from inf to 0.34975, saving model to hl5_0100.h5\n","1/1 [==============================] - 2s 2s/step - loss: 0.3673 - val_loss: 0.3498\n","Epoch 2/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3634\n","Epoch 2: val_loss improved from 0.34975 to 0.34615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.3634 - val_loss: 0.3462\n","Epoch 3/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3595"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3: val_loss improved from 0.34615 to 0.34260, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.3595 - val_loss: 0.3426\n","Epoch 4/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3557\n","Epoch 4: val_loss improved from 0.34260 to 0.33910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.3557 - val_loss: 0.3391\n","Epoch 5/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3520\n","Epoch 5: val_loss improved from 0.33910 to 0.33569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.3520 - val_loss: 0.3357\n","Epoch 6/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3483\n","Epoch 6: val_loss improved from 0.33569 to 0.33232, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.3483 - val_loss: 0.3323\n","Epoch 7/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3447\n","Epoch 7: val_loss improved from 0.33232 to 0.32901, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.3447 - val_loss: 0.3290\n","Epoch 8/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3412\n","Epoch 8: val_loss improved from 0.32901 to 0.32574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.3412 - val_loss: 0.3257\n","Epoch 9/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3377\n","Epoch 9: val_loss improved from 0.32574 to 0.32253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.3377 - val_loss: 0.3225\n","Epoch 10/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3343\n","Epoch 10: val_loss improved from 0.32253 to 0.31936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.3343 - val_loss: 0.3194\n","Epoch 11/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3309\n","Epoch 11: val_loss improved from 0.31936 to 0.31624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.3309 - val_loss: 0.3162\n","Epoch 12/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3275\n","Epoch 12: val_loss improved from 0.31624 to 0.31315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.3275 - val_loss: 0.3132\n","Epoch 13/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3242\n","Epoch 13: val_loss improved from 0.31315 to 0.31011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.3242 - val_loss: 0.3101\n","Epoch 14/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3209\n","Epoch 14: val_loss improved from 0.31011 to 0.30711, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.3209 - val_loss: 0.3071\n","Epoch 15/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3177\n","Epoch 15: val_loss improved from 0.30711 to 0.30415, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.3177 - val_loss: 0.3041\n","Epoch 16/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3146\n","Epoch 16: val_loss improved from 0.30415 to 0.30122, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.3146 - val_loss: 0.3012\n","Epoch 17/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3114\n","Epoch 17: val_loss improved from 0.30122 to 0.29833, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.3114 - val_loss: 0.2983\n","Epoch 18/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3083\n","Epoch 18: val_loss improved from 0.29833 to 0.29547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.3083 - val_loss: 0.2955\n","Epoch 19/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3052\n","Epoch 19: val_loss improved from 0.29547 to 0.29265, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.3052 - val_loss: 0.2926\n","Epoch 20/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.3022\n","Epoch 20: val_loss improved from 0.29265 to 0.28986, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.3022 - val_loss: 0.2899\n","Epoch 21/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2992\n","Epoch 21: val_loss improved from 0.28986 to 0.28710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.2992 - val_loss: 0.2871\n","Epoch 22/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2962\n","Epoch 22: val_loss improved from 0.28710 to 0.28437, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.2962 - val_loss: 0.2844\n","Epoch 23/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2933\n","Epoch 23: val_loss improved from 0.28437 to 0.28168, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.2933 - val_loss: 0.2817\n","Epoch 24/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2904\n","Epoch 24: val_loss improved from 0.28168 to 0.27901, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.2904 - val_loss: 0.2790\n","Epoch 25/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2875\n","Epoch 25: val_loss improved from 0.27901 to 0.27637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.2875 - val_loss: 0.2764\n","Epoch 26/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2847\n","Epoch 26: val_loss improved from 0.27637 to 0.27377, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.2847 - val_loss: 0.2738\n","Epoch 27/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2819\n","Epoch 27: val_loss improved from 0.27377 to 0.27119, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.2819 - val_loss: 0.2712\n","Epoch 28/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2791\n","Epoch 28: val_loss improved from 0.27119 to 0.26864, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.2791 - val_loss: 0.2686\n","Epoch 29/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2764\n","Epoch 29: val_loss improved from 0.26864 to 0.26612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.2764 - val_loss: 0.2661\n","Epoch 30/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2736\n","Epoch 30: val_loss improved from 0.26612 to 0.26362, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.2736 - val_loss: 0.2636\n","Epoch 31/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2710\n","Epoch 31: val_loss improved from 0.26362 to 0.26116, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.2710 - val_loss: 0.2612\n","Epoch 32/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2683\n","Epoch 32: val_loss improved from 0.26116 to 0.25872, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.2683 - val_loss: 0.2587\n","Epoch 33/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2657\n","Epoch 33: val_loss improved from 0.25872 to 0.25630, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.2657 - val_loss: 0.2563\n","Epoch 34/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2631\n","Epoch 34: val_loss improved from 0.25630 to 0.25392, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.2631 - val_loss: 0.2539\n","Epoch 35/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2605\n","Epoch 35: val_loss improved from 0.25392 to 0.25156, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.2605 - val_loss: 0.2516\n","Epoch 36/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2579\n","Epoch 36: val_loss improved from 0.25156 to 0.24923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.2579 - val_loss: 0.2492\n","Epoch 37/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2554\n","Epoch 37: val_loss improved from 0.24923 to 0.24692, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.2554 - val_loss: 0.2469\n","Epoch 38/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2529\n","Epoch 38: val_loss improved from 0.24692 to 0.24464, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.2529 - val_loss: 0.2446\n","Epoch 39/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2504\n","Epoch 39: val_loss improved from 0.24464 to 0.24238, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.2504 - val_loss: 0.2424\n","Epoch 40/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2480\n","Epoch 40: val_loss improved from 0.24238 to 0.24015, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.2480 - val_loss: 0.2402\n","Epoch 41/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2456\n","Epoch 41: val_loss improved from 0.24015 to 0.23795, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.2456 - val_loss: 0.2379\n","Epoch 42/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2432\n","Epoch 42: val_loss improved from 0.23795 to 0.23576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.2432 - val_loss: 0.2358\n","Epoch 43/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2408\n","Epoch 43: val_loss improved from 0.23576 to 0.23360, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.2408 - val_loss: 0.2336\n","Epoch 44/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2385\n","Epoch 44: val_loss improved from 0.23360 to 0.23147, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.2385 - val_loss: 0.2315\n","Epoch 45/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2361\n","Epoch 45: val_loss improved from 0.23147 to 0.22935, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.2361 - val_loss: 0.2294\n","Epoch 46/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2339\n","Epoch 46: val_loss improved from 0.22935 to 0.22726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.2339 - val_loss: 0.2273\n","Epoch 47/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2316\n","Epoch 47: val_loss improved from 0.22726 to 0.22520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.2316 - val_loss: 0.2252\n","Epoch 48/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2293\n","Epoch 48: val_loss improved from 0.22520 to 0.22316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.2293 - val_loss: 0.2232\n","Epoch 49/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2271\n","Epoch 49: val_loss improved from 0.22316 to 0.22113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.2271 - val_loss: 0.2211\n","Epoch 50/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2249\n","Epoch 50: val_loss improved from 0.22113 to 0.21914, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.2249 - val_loss: 0.2191\n","Epoch 51/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2227\n","Epoch 51: val_loss improved from 0.21914 to 0.21716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.2227 - val_loss: 0.2172\n","Epoch 52/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2206\n","Epoch 52: val_loss improved from 0.21716 to 0.21520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.2206 - val_loss: 0.2152\n","Epoch 53/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2185\n","Epoch 53: val_loss improved from 0.21520 to 0.21327, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 140ms/step - loss: 0.2185 - val_loss: 0.2133\n","Epoch 54/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2163\n","Epoch 54: val_loss improved from 0.21327 to 0.21136, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 135ms/step - loss: 0.2163 - val_loss: 0.2114\n","Epoch 55/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2143\n","Epoch 55: val_loss improved from 0.21136 to 0.20947, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.2143 - val_loss: 0.2095\n","Epoch 56/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2122\n","Epoch 56: val_loss improved from 0.20947 to 0.20760, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.2122 - val_loss: 0.2076\n","Epoch 57/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2102\n","Epoch 57: val_loss improved from 0.20760 to 0.20575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.2102 - val_loss: 0.2058\n","Epoch 58/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2081\n","Epoch 58: val_loss improved from 0.20575 to 0.20392, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.2081 - val_loss: 0.2039\n","Epoch 59/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2061\n","Epoch 59: val_loss improved from 0.20392 to 0.20212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.2061 - val_loss: 0.2021\n","Epoch 60/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2042\n","Epoch 60: val_loss improved from 0.20212 to 0.20033, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.2042 - val_loss: 0.2003\n","Epoch 61/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2022\n","Epoch 61: val_loss improved from 0.20033 to 0.19856, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.2022 - val_loss: 0.1986\n","Epoch 62/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.2003\n","Epoch 62: val_loss improved from 0.19856 to 0.19681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 131ms/step - loss: 0.2003 - val_loss: 0.1968\n","Epoch 63/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1984\n","Epoch 63: val_loss improved from 0.19681 to 0.19509, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.1984 - val_loss: 0.1951\n","Epoch 64/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1965\n","Epoch 64: val_loss improved from 0.19509 to 0.19338, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.1965 - val_loss: 0.1934\n","Epoch 65/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1946\n","Epoch 65: val_loss improved from 0.19338 to 0.19169, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.1946 - val_loss: 0.1917\n","Epoch 66/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1927\n","Epoch 66: val_loss improved from 0.19169 to 0.19001, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.1927 - val_loss: 0.1900\n","Epoch 67/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1909\n","Epoch 67: val_loss improved from 0.19001 to 0.18836, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.1909 - val_loss: 0.1884\n","Epoch 68/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1891\n","Epoch 68: val_loss improved from 0.18836 to 0.18673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.1891 - val_loss: 0.1867\n","Epoch 69/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1873\n","Epoch 69: val_loss improved from 0.18673 to 0.18511, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.1873 - val_loss: 0.1851\n","Epoch 70/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1855\n","Epoch 70: val_loss improved from 0.18511 to 0.18351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.1855 - val_loss: 0.1835\n","Epoch 71/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1837\n","Epoch 71: val_loss improved from 0.18351 to 0.18193, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.1837 - val_loss: 0.1819\n","Epoch 72/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1820\n","Epoch 72: val_loss improved from 0.18193 to 0.18037, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.1820 - val_loss: 0.1804\n","Epoch 73/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1803\n","Epoch 73: val_loss improved from 0.18037 to 0.17883, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1803 - val_loss: 0.1788\n","Epoch 74/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1786\n","Epoch 74: val_loss improved from 0.17883 to 0.17730, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.1786 - val_loss: 0.1773\n","Epoch 75/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1769\n","Epoch 75: val_loss improved from 0.17730 to 0.17579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1769 - val_loss: 0.1758\n","Epoch 76/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1752\n","Epoch 76: val_loss improved from 0.17579 to 0.17429, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1752 - val_loss: 0.1743\n","Epoch 77/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1736\n","Epoch 77: val_loss improved from 0.17429 to 0.17282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1736 - val_loss: 0.1728\n","Epoch 78/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1719\n","Epoch 78: val_loss improved from 0.17282 to 0.17136, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.1719 - val_loss: 0.1714\n","Epoch 79/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1703\n","Epoch 79: val_loss improved from 0.17136 to 0.16991, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1703 - val_loss: 0.1699\n","Epoch 80/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1687\n","Epoch 80: val_loss improved from 0.16991 to 0.16848, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1687 - val_loss: 0.1685\n","Epoch 81/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1671\n","Epoch 81: val_loss improved from 0.16848 to 0.16707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.1671 - val_loss: 0.1671\n","Epoch 82/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1656\n","Epoch 82: val_loss improved from 0.16707 to 0.16568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1656 - val_loss: 0.1657\n","Epoch 83/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1640\n","Epoch 83: val_loss improved from 0.16568 to 0.16429, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1640 - val_loss: 0.1643\n","Epoch 84/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1625\n","Epoch 84: val_loss improved from 0.16429 to 0.16293, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1625 - val_loss: 0.1629\n","Epoch 85/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1610\n","Epoch 85: val_loss improved from 0.16293 to 0.16158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1610 - val_loss: 0.1616\n","Epoch 86/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1595\n","Epoch 86: val_loss improved from 0.16158 to 0.16025, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1595 - val_loss: 0.1602\n","Epoch 87/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1580\n","Epoch 87: val_loss improved from 0.16025 to 0.15893, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1580 - val_loss: 0.1589\n","Epoch 88/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1565\n","Epoch 88: val_loss improved from 0.15893 to 0.15762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1565 - val_loss: 0.1576\n","Epoch 89/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1551\n","Epoch 89: val_loss improved from 0.15762 to 0.15633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1551 - val_loss: 0.1563\n","Epoch 90/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1537\n","Epoch 90: val_loss improved from 0.15633 to 0.15506, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.1537 - val_loss: 0.1551\n","Epoch 91/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1522\n","Epoch 91: val_loss improved from 0.15506 to 0.15380, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.1522 - val_loss: 0.1538\n","Epoch 92/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1508\n","Epoch 92: val_loss improved from 0.15380 to 0.15255, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.1508 - val_loss: 0.1526\n","Epoch 93/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1494\n","Epoch 93: val_loss improved from 0.15255 to 0.15132, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1494 - val_loss: 0.1513\n","Epoch 94/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1481\n","Epoch 94: val_loss improved from 0.15132 to 0.15010, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1481 - val_loss: 0.1501\n","Epoch 95/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1467\n","Epoch 95: val_loss improved from 0.15010 to 0.14890, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1467 - val_loss: 0.1489\n","Epoch 96/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1454\n","Epoch 96: val_loss improved from 0.14890 to 0.14771, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1454 - val_loss: 0.1477\n","Epoch 97/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1440\n","Epoch 97: val_loss improved from 0.14771 to 0.14653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1440 - val_loss: 0.1465\n","Epoch 98/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1427\n","Epoch 98: val_loss improved from 0.14653 to 0.14536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1427 - val_loss: 0.1454\n","Epoch 99/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1414\n","Epoch 99: val_loss improved from 0.14536 to 0.14421, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1414 - val_loss: 0.1442\n","Epoch 100/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1401\n","Epoch 100: val_loss improved from 0.14421 to 0.14308, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1401 - val_loss: 0.1431\n","Epoch 101/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1388\n","Epoch 101: val_loss improved from 0.14308 to 0.14195, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.1388 - val_loss: 0.1420\n","Epoch 102/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1376\n","Epoch 102: val_loss improved from 0.14195 to 0.14084, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1376 - val_loss: 0.1408\n","Epoch 103/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1363\n","Epoch 103: val_loss improved from 0.14084 to 0.13974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1363 - val_loss: 0.1397\n","Epoch 104/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1351\n","Epoch 104: val_loss improved from 0.13974 to 0.13865, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1351 - val_loss: 0.1387\n","Epoch 105/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1339\n","Epoch 105: val_loss improved from 0.13865 to 0.13758, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1339 - val_loss: 0.1376\n","Epoch 106/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1327\n","Epoch 106: val_loss improved from 0.13758 to 0.13652, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.1327 - val_loss: 0.1365\n","Epoch 107/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1315\n","Epoch 107: val_loss improved from 0.13652 to 0.13547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.1315 - val_loss: 0.1355\n","Epoch 108/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1303\n","Epoch 108: val_loss improved from 0.13547 to 0.13443, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.1303 - val_loss: 0.1344\n","Epoch 109/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1291\n","Epoch 109: val_loss improved from 0.13443 to 0.13341, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1291 - val_loss: 0.1334\n","Epoch 110/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1280\n","Epoch 110: val_loss improved from 0.13341 to 0.13239, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1280 - val_loss: 0.1324\n","Epoch 111/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1268\n","Epoch 111: val_loss improved from 0.13239 to 0.13139, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.1268 - val_loss: 0.1314\n","Epoch 112/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1257\n","Epoch 112: val_loss improved from 0.13139 to 0.13040, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1257 - val_loss: 0.1304\n","Epoch 113/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1245\n","Epoch 113: val_loss improved from 0.13040 to 0.12942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.1245 - val_loss: 0.1294\n","Epoch 114/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1234\n","Epoch 114: val_loss improved from 0.12942 to 0.12845, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1234 - val_loss: 0.1285\n","Epoch 115/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1223\n","Epoch 115: val_loss improved from 0.12845 to 0.12749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1223 - val_loss: 0.1275\n","Epoch 116/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1213\n","Epoch 116: val_loss improved from 0.12749 to 0.12655, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1213 - val_loss: 0.1265\n","Epoch 117/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1202\n","Epoch 117: val_loss improved from 0.12655 to 0.12561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1202 - val_loss: 0.1256\n","Epoch 118/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1191\n","Epoch 118: val_loss improved from 0.12561 to 0.12469, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.1191 - val_loss: 0.1247\n","Epoch 119/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1181\n","Epoch 119: val_loss improved from 0.12469 to 0.12377, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.1181 - val_loss: 0.1238\n","Epoch 120/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1170\n","Epoch 120: val_loss improved from 0.12377 to 0.12287, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.1170 - val_loss: 0.1229\n","Epoch 121/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1160\n","Epoch 121: val_loss improved from 0.12287 to 0.12198, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1160 - val_loss: 0.1220\n","Epoch 122/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1150\n","Epoch 122: val_loss improved from 0.12198 to 0.12110, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.1150 - val_loss: 0.1211\n","Epoch 123/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1140\n","Epoch 123: val_loss improved from 0.12110 to 0.12022, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.1140 - val_loss: 0.1202\n","Epoch 124/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1130\n","Epoch 124: val_loss improved from 0.12022 to 0.11936, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1130 - val_loss: 0.1194\n","Epoch 125/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1120\n","Epoch 125: val_loss improved from 0.11936 to 0.11851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.1120 - val_loss: 0.1185\n","Epoch 126/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1110\n","Epoch 126: val_loss improved from 0.11851 to 0.11767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1110 - val_loss: 0.1177\n","Epoch 127/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1101\n","Epoch 127: val_loss improved from 0.11767 to 0.11684, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.1101 - val_loss: 0.1168\n","Epoch 128/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1091\n","Epoch 128: val_loss improved from 0.11684 to 0.11601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.1091 - val_loss: 0.1160\n","Epoch 129/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1082\n","Epoch 129: val_loss improved from 0.11601 to 0.11520, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1082 - val_loss: 0.1152\n","Epoch 130/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1072\n","Epoch 130: val_loss improved from 0.11520 to 0.11440, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1072 - val_loss: 0.1144\n","Epoch 131/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1063\n","Epoch 131: val_loss improved from 0.11440 to 0.11360, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.1063 - val_loss: 0.1136\n","Epoch 132/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1054\n","Epoch 132: val_loss improved from 0.11360 to 0.11282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1054 - val_loss: 0.1128\n","Epoch 133/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1045\n","Epoch 133: val_loss improved from 0.11282 to 0.11204, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1045 - val_loss: 0.1120\n","Epoch 134/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1036\n","Epoch 134: val_loss improved from 0.11204 to 0.11128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1036 - val_loss: 0.1113\n","Epoch 135/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1027\n","Epoch 135: val_loss improved from 0.11128 to 0.11052, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.1027 - val_loss: 0.1105\n","Epoch 136/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1018\n","Epoch 136: val_loss improved from 0.11052 to 0.10977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1018 - val_loss: 0.1098\n","Epoch 137/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1010\n","Epoch 137: val_loss improved from 0.10977 to 0.10903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.1010 - val_loss: 0.1090\n","Epoch 138/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.1001\n","Epoch 138: val_loss improved from 0.10903 to 0.10830, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.1001 - val_loss: 0.1083\n","Epoch 139/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0993\n","Epoch 139: val_loss improved from 0.10830 to 0.10758, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0993 - val_loss: 0.1076\n","Epoch 140/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0984\n","Epoch 140: val_loss improved from 0.10758 to 0.10687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0984 - val_loss: 0.1069\n","Epoch 141/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0976\n","Epoch 141: val_loss improved from 0.10687 to 0.10616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0976 - val_loss: 0.1062\n","Epoch 142/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0968\n","Epoch 142: val_loss improved from 0.10616 to 0.10547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0968 - val_loss: 0.1055\n","Epoch 143/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0960\n","Epoch 143: val_loss improved from 0.10547 to 0.10478, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0960 - val_loss: 0.1048\n","Epoch 144/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0952\n","Epoch 144: val_loss improved from 0.10478 to 0.10410, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0952 - val_loss: 0.1041\n","Epoch 145/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0944\n","Epoch 145: val_loss improved from 0.10410 to 0.10343, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0944 - val_loss: 0.1034\n","Epoch 146/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0936\n","Epoch 146: val_loss improved from 0.10343 to 0.10276, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0936 - val_loss: 0.1028\n","Epoch 147/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0928\n","Epoch 147: val_loss improved from 0.10276 to 0.10211, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0928 - val_loss: 0.1021\n","Epoch 148/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0921\n","Epoch 148: val_loss improved from 0.10211 to 0.10146, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0921 - val_loss: 0.1015\n","Epoch 149/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0913\n","Epoch 149: val_loss improved from 0.10146 to 0.10082, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0913 - val_loss: 0.1008\n","Epoch 150/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0906\n","Epoch 150: val_loss improved from 0.10082 to 0.10019, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0906 - val_loss: 0.1002\n","Epoch 151/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0898\n","Epoch 151: val_loss improved from 0.10019 to 0.09956, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0898 - val_loss: 0.0996\n","Epoch 152/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0891\n","Epoch 152: val_loss improved from 0.09956 to 0.09895, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0891 - val_loss: 0.0989\n","Epoch 153/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0884\n","Epoch 153: val_loss improved from 0.09895 to 0.09834, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0884 - val_loss: 0.0983\n","Epoch 154/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0877\n","Epoch 154: val_loss improved from 0.09834 to 0.09774, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0877 - val_loss: 0.0977\n","Epoch 155/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0870\n","Epoch 155: val_loss improved from 0.09774 to 0.09714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0870 - val_loss: 0.0971\n","Epoch 156/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0863\n","Epoch 156: val_loss improved from 0.09714 to 0.09656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0863 - val_loss: 0.0966\n","Epoch 157/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0856\n","Epoch 157: val_loss improved from 0.09656 to 0.09597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0856 - val_loss: 0.0960\n","Epoch 158/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0849\n","Epoch 158: val_loss improved from 0.09597 to 0.09540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0849 - val_loss: 0.0954\n","Epoch 159/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0842\n","Epoch 159: val_loss improved from 0.09540 to 0.09484, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0842 - val_loss: 0.0948\n","Epoch 160/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0835\n","Epoch 160: val_loss improved from 0.09484 to 0.09428, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0835 - val_loss: 0.0943\n","Epoch 161/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0829\n","Epoch 161: val_loss improved from 0.09428 to 0.09373, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0829 - val_loss: 0.0937\n","Epoch 162/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0822\n","Epoch 162: val_loss improved from 0.09373 to 0.09318, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0822 - val_loss: 0.0932\n","Epoch 163/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0816\n","Epoch 163: val_loss improved from 0.09318 to 0.09264, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0816 - val_loss: 0.0926\n","Epoch 164/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0809\n","Epoch 164: val_loss improved from 0.09264 to 0.09211, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0809 - val_loss: 0.0921\n","Epoch 165/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0803\n","Epoch 165: val_loss improved from 0.09211 to 0.09158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0803 - val_loss: 0.0916\n","Epoch 166/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0797\n","Epoch 166: val_loss improved from 0.09158 to 0.09106, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0797 - val_loss: 0.0911\n","Epoch 167/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0791\n","Epoch 167: val_loss improved from 0.09106 to 0.09055, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0791 - val_loss: 0.0906\n","Epoch 168/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0785\n","Epoch 168: val_loss improved from 0.09055 to 0.09005, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0785 - val_loss: 0.0900\n","Epoch 169/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0779\n","Epoch 169: val_loss improved from 0.09005 to 0.08954, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0779 - val_loss: 0.0895\n","Epoch 170/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0773\n","Epoch 170: val_loss improved from 0.08954 to 0.08905, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0773 - val_loss: 0.0891\n","Epoch 171/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0767\n","Epoch 171: val_loss improved from 0.08905 to 0.08856, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0767 - val_loss: 0.0886\n","Epoch 172/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0761\n","Epoch 172: val_loss improved from 0.08856 to 0.08808, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0761 - val_loss: 0.0881\n","Epoch 173/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0755\n","Epoch 173: val_loss improved from 0.08808 to 0.08760, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0755 - val_loss: 0.0876\n","Epoch 174/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0749\n","Epoch 174: val_loss improved from 0.08760 to 0.08713, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0749 - val_loss: 0.0871\n","Epoch 175/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0744\n","Epoch 175: val_loss improved from 0.08713 to 0.08667, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0744 - val_loss: 0.0867\n","Epoch 176/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0738\n","Epoch 176: val_loss improved from 0.08667 to 0.08621, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0738 - val_loss: 0.0862\n","Epoch 177/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0733\n","Epoch 177: val_loss improved from 0.08621 to 0.08576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0733 - val_loss: 0.0858\n","Epoch 178/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0727\n","Epoch 178: val_loss improved from 0.08576 to 0.08531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0727 - val_loss: 0.0853\n","Epoch 179/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0722\n","Epoch 179: val_loss improved from 0.08531 to 0.08487, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0722 - val_loss: 0.0849\n","Epoch 180/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0716\n","Epoch 180: val_loss improved from 0.08487 to 0.08444, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0716 - val_loss: 0.0844\n","Epoch 181/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0711\n","Epoch 181: val_loss improved from 0.08444 to 0.08401, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0711 - val_loss: 0.0840\n","Epoch 182/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0706\n","Epoch 182: val_loss improved from 0.08401 to 0.08358, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.0706 - val_loss: 0.0836\n","Epoch 183/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0701\n","Epoch 183: val_loss improved from 0.08358 to 0.08316, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 151ms/step - loss: 0.0701 - val_loss: 0.0832\n","Epoch 184/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0696\n","Epoch 184: val_loss improved from 0.08316 to 0.08275, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0696 - val_loss: 0.0827\n","Epoch 185/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0691\n","Epoch 185: val_loss improved from 0.08275 to 0.08234, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0691 - val_loss: 0.0823\n","Epoch 186/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0686\n","Epoch 186: val_loss improved from 0.08234 to 0.08194, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0686 - val_loss: 0.0819\n","Epoch 187/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0681\n","Epoch 187: val_loss improved from 0.08194 to 0.08154, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0681 - val_loss: 0.0815\n","Epoch 188/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0676\n","Epoch 188: val_loss improved from 0.08154 to 0.08115, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 152ms/step - loss: 0.0676 - val_loss: 0.0811\n","Epoch 189/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0671\n","Epoch 189: val_loss improved from 0.08115 to 0.08076, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 122ms/step - loss: 0.0671 - val_loss: 0.0808\n","Epoch 190/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0666\n","Epoch 190: val_loss improved from 0.08076 to 0.08038, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 145ms/step - loss: 0.0666 - val_loss: 0.0804\n","Epoch 191/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0662\n","Epoch 191: val_loss improved from 0.08038 to 0.08000, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0662 - val_loss: 0.0800\n","Epoch 192/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0657\n","Epoch 192: val_loss improved from 0.08000 to 0.07962, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0657 - val_loss: 0.0796\n","Epoch 193/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0652\n","Epoch 193: val_loss improved from 0.07962 to 0.07926, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0652 - val_loss: 0.0793\n","Epoch 194/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0648\n","Epoch 194: val_loss improved from 0.07926 to 0.07889, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 147ms/step - loss: 0.0648 - val_loss: 0.0789\n","Epoch 195/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0643\n","Epoch 195: val_loss improved from 0.07889 to 0.07853, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 151ms/step - loss: 0.0643 - val_loss: 0.0785\n","Epoch 196/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0639\n","Epoch 196: val_loss improved from 0.07853 to 0.07818, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 139ms/step - loss: 0.0639 - val_loss: 0.0782\n","Epoch 197/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0635\n","Epoch 197: val_loss improved from 0.07818 to 0.07783, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 152ms/step - loss: 0.0635 - val_loss: 0.0778\n","Epoch 198/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0630\n","Epoch 198: val_loss improved from 0.07783 to 0.07748, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 141ms/step - loss: 0.0630 - val_loss: 0.0775\n","Epoch 199/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0626\n","Epoch 199: val_loss improved from 0.07748 to 0.07714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0626 - val_loss: 0.0771\n","Epoch 200/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0622\n","Epoch 200: val_loss improved from 0.07714 to 0.07681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0622 - val_loss: 0.0768\n","Epoch 201/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0618\n","Epoch 201: val_loss improved from 0.07681 to 0.07647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0618 - val_loss: 0.0765\n","Epoch 202/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0613\n","Epoch 202: val_loss improved from 0.07647 to 0.07615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0613 - val_loss: 0.0761\n","Epoch 203/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0609\n","Epoch 203: val_loss improved from 0.07615 to 0.07582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0609 - val_loss: 0.0758\n","Epoch 204/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0605\n","Epoch 204: val_loss improved from 0.07582 to 0.07551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0605 - val_loss: 0.0755\n","Epoch 205/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0601\n","Epoch 205: val_loss improved from 0.07551 to 0.07519, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0601 - val_loss: 0.0752\n","Epoch 206/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0597\n","Epoch 206: val_loss improved from 0.07519 to 0.07488, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0597 - val_loss: 0.0749\n","Epoch 207/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0593\n","Epoch 207: val_loss improved from 0.07488 to 0.07457, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0593 - val_loss: 0.0746\n","Epoch 208/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0589\n","Epoch 208: val_loss improved from 0.07457 to 0.07427, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0589 - val_loss: 0.0743\n","Epoch 209/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0586\n","Epoch 209: val_loss improved from 0.07427 to 0.07397, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0586 - val_loss: 0.0740\n","Epoch 210/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0582\n","Epoch 210: val_loss improved from 0.07397 to 0.07368, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0582 - val_loss: 0.0737\n","Epoch 211/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0578\n","Epoch 211: val_loss improved from 0.07368 to 0.07339, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0578 - val_loss: 0.0734\n","Epoch 212/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0574\n","Epoch 212: val_loss improved from 0.07339 to 0.07310, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0574 - val_loss: 0.0731\n","Epoch 213/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0571\n","Epoch 213: val_loss improved from 0.07310 to 0.07281, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0571 - val_loss: 0.0728\n","Epoch 214/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0567\n","Epoch 214: val_loss improved from 0.07281 to 0.07253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0567 - val_loss: 0.0725\n","Epoch 215/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0564\n","Epoch 215: val_loss improved from 0.07253 to 0.07226, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0564 - val_loss: 0.0723\n","Epoch 216/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0560\n","Epoch 216: val_loss improved from 0.07226 to 0.07198, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0560 - val_loss: 0.0720\n","Epoch 217/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0557\n","Epoch 217: val_loss improved from 0.07198 to 0.07171, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0557 - val_loss: 0.0717\n","Epoch 218/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0553\n","Epoch 218: val_loss improved from 0.07171 to 0.07145, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0553 - val_loss: 0.0714\n","Epoch 219/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0550\n","Epoch 219: val_loss improved from 0.07145 to 0.07119, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0550 - val_loss: 0.0712\n","Epoch 220/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0546\n","Epoch 220: val_loss improved from 0.07119 to 0.07093, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0546 - val_loss: 0.0709\n","Epoch 221/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0543\n","Epoch 221: val_loss improved from 0.07093 to 0.07067, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0543 - val_loss: 0.0707\n","Epoch 222/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0540\n","Epoch 222: val_loss improved from 0.07067 to 0.07042, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0540 - val_loss: 0.0704\n","Epoch 223/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0536\n","Epoch 223: val_loss improved from 0.07042 to 0.07017, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0536 - val_loss: 0.0702\n","Epoch 224/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0533\n","Epoch 224: val_loss improved from 0.07017 to 0.06993, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0533 - val_loss: 0.0699\n","Epoch 225/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0530\n","Epoch 225: val_loss improved from 0.06993 to 0.06969, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0530 - val_loss: 0.0697\n","Epoch 226/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0527\n","Epoch 226: val_loss improved from 0.06969 to 0.06945, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0527 - val_loss: 0.0695\n","Epoch 227/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 227: val_loss improved from 0.06945 to 0.06922, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0524 - val_loss: 0.0692\n","Epoch 228/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0521\n","Epoch 228: val_loss improved from 0.06922 to 0.06899, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0521 - val_loss: 0.0690\n","Epoch 229/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0518\n","Epoch 229: val_loss improved from 0.06899 to 0.06876, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0518 - val_loss: 0.0688\n","Epoch 230/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 230: val_loss improved from 0.06876 to 0.06853, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0515 - val_loss: 0.0685\n","Epoch 231/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0512\n","Epoch 231: val_loss improved from 0.06853 to 0.06831, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0512 - val_loss: 0.0683\n","Epoch 232/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0509\n","Epoch 232: val_loss improved from 0.06831 to 0.06809, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0509 - val_loss: 0.0681\n","Epoch 233/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0506\n","Epoch 233: val_loss improved from 0.06809 to 0.06788, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0506 - val_loss: 0.0679\n","Epoch 234/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0503\n","Epoch 234: val_loss improved from 0.06788 to 0.06767, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0503 - val_loss: 0.0677\n","Epoch 235/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0500\n","Epoch 235: val_loss improved from 0.06767 to 0.06746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0500 - val_loss: 0.0675\n","Epoch 236/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0498\n","Epoch 236: val_loss improved from 0.06746 to 0.06725, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0498 - val_loss: 0.0672\n","Epoch 237/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0495\n","Epoch 237: val_loss improved from 0.06725 to 0.06705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0495 - val_loss: 0.0670\n","Epoch 238/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0492\n","Epoch 238: val_loss improved from 0.06705 to 0.06685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0492 - val_loss: 0.0668\n","Epoch 239/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0490\n","Epoch 239: val_loss improved from 0.06685 to 0.06665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0490 - val_loss: 0.0666\n","Epoch 240/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0487\n","Epoch 240: val_loss improved from 0.06665 to 0.06645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0487 - val_loss: 0.0665\n","Epoch 241/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0484\n","Epoch 241: val_loss improved from 0.06645 to 0.06626, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0484 - val_loss: 0.0663\n","Epoch 242/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0482\n","Epoch 242: val_loss improved from 0.06626 to 0.06607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0482 - val_loss: 0.0661\n","Epoch 243/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 243: val_loss improved from 0.06607 to 0.06589, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0479 - val_loss: 0.0659\n","Epoch 244/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0477\n","Epoch 244: val_loss improved from 0.06589 to 0.06570, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0477 - val_loss: 0.0657\n","Epoch 245/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 245: val_loss improved from 0.06570 to 0.06552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0474 - val_loss: 0.0655\n","Epoch 246/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0472\n","Epoch 246: val_loss improved from 0.06552 to 0.06534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0472 - val_loss: 0.0653\n","Epoch 247/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0469\n","Epoch 247: val_loss improved from 0.06534 to 0.06517, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0469 - val_loss: 0.0652\n","Epoch 248/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0467\n","Epoch 248: val_loss improved from 0.06517 to 0.06499, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0467 - val_loss: 0.0650\n","Epoch 249/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0465\n","Epoch 249: val_loss improved from 0.06499 to 0.06482, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0465 - val_loss: 0.0648\n","Epoch 250/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 250: val_loss improved from 0.06482 to 0.06465, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0462 - val_loss: 0.0647\n","Epoch 251/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0460\n","Epoch 251: val_loss improved from 0.06465 to 0.06449, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0460 - val_loss: 0.0645\n","Epoch 252/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 252: val_loss improved from 0.06449 to 0.06432, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0458 - val_loss: 0.0643\n","Epoch 253/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0455\n","Epoch 253: val_loss improved from 0.06432 to 0.06416, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0455 - val_loss: 0.0642\n","Epoch 254/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0453\n","Epoch 254: val_loss improved from 0.06416 to 0.06400, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0453 - val_loss: 0.0640\n","Epoch 255/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0451\n","Epoch 255: val_loss improved from 0.06400 to 0.06385, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0451 - val_loss: 0.0638\n","Epoch 256/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0449\n","Epoch 256: val_loss improved from 0.06385 to 0.06369, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0449 - val_loss: 0.0637\n","Epoch 257/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0447\n","Epoch 257: val_loss improved from 0.06369 to 0.06354, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0447 - val_loss: 0.0635\n","Epoch 258/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0444\n","Epoch 258: val_loss improved from 0.06354 to 0.06339, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0444 - val_loss: 0.0634\n","Epoch 259/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 259: val_loss improved from 0.06339 to 0.06324, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0442 - val_loss: 0.0632\n","Epoch 260/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 260: val_loss improved from 0.06324 to 0.06310, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0440 - val_loss: 0.0631\n","Epoch 261/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0438\n","Epoch 261: val_loss improved from 0.06310 to 0.06296, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0438 - val_loss: 0.0630\n","Epoch 262/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 262: val_loss improved from 0.06296 to 0.06282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0436 - val_loss: 0.0628\n","Epoch 263/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0434\n","Epoch 263: val_loss improved from 0.06282 to 0.06268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0434 - val_loss: 0.0627\n","Epoch 264/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 264: val_loss improved from 0.06268 to 0.06254, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0432 - val_loss: 0.0625\n","Epoch 265/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0430\n","Epoch 265: val_loss improved from 0.06254 to 0.06241, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0430 - val_loss: 0.0624\n","Epoch 266/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 266: val_loss improved from 0.06241 to 0.06227, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0428 - val_loss: 0.0623\n","Epoch 267/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0427\n","Epoch 267: val_loss improved from 0.06227 to 0.06214, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0427 - val_loss: 0.0621\n","Epoch 268/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0425\n","Epoch 268: val_loss improved from 0.06214 to 0.06201, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0425 - val_loss: 0.0620\n","Epoch 269/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0423\n","Epoch 269: val_loss improved from 0.06201 to 0.06189, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0423 - val_loss: 0.0619\n","Epoch 270/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 270: val_loss improved from 0.06189 to 0.06176, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0421 - val_loss: 0.0618\n","Epoch 271/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 271: val_loss improved from 0.06176 to 0.06164, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0419 - val_loss: 0.0616\n","Epoch 272/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0417\n","Epoch 272: val_loss improved from 0.06164 to 0.06152, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0417 - val_loss: 0.0615\n","Epoch 273/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 273: val_loss improved from 0.06152 to 0.06140, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0416 - val_loss: 0.0614\n","Epoch 274/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 274: val_loss improved from 0.06140 to 0.06128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0414 - val_loss: 0.0613\n","Epoch 275/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0412\n","Epoch 275: val_loss improved from 0.06128 to 0.06117, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0412 - val_loss: 0.0612\n","Epoch 276/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0411\n","Epoch 276: val_loss improved from 0.06117 to 0.06105, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0411 - val_loss: 0.0611\n","Epoch 277/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 277: val_loss improved from 0.06105 to 0.06094, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0409 - val_loss: 0.0609\n","Epoch 278/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 278: val_loss improved from 0.06094 to 0.06083, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0407 - val_loss: 0.0608\n","Epoch 279/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0406\n","Epoch 279: val_loss improved from 0.06083 to 0.06072, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0406 - val_loss: 0.0607\n","Epoch 280/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0404\n","Epoch 280: val_loss improved from 0.06072 to 0.06062, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0404 - val_loss: 0.0606\n","Epoch 281/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 281: val_loss improved from 0.06062 to 0.06051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0402 - val_loss: 0.0605\n","Epoch 282/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0401\n","Epoch 282: val_loss improved from 0.06051 to 0.06041, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0401 - val_loss: 0.0604\n","Epoch 283/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 283: val_loss improved from 0.06041 to 0.06031, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0399 - val_loss: 0.0603\n","Epoch 284/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0398\n","Epoch 284: val_loss improved from 0.06031 to 0.06021, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0398 - val_loss: 0.0602\n","Epoch 285/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 285: val_loss improved from 0.06021 to 0.06011, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0396 - val_loss: 0.0601\n","Epoch 286/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 286: val_loss improved from 0.06011 to 0.06001, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0395 - val_loss: 0.0600\n","Epoch 287/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0393\n","Epoch 287: val_loss improved from 0.06001 to 0.05992, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0393 - val_loss: 0.0599\n","Epoch 288/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 288: val_loss improved from 0.05992 to 0.05982, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0392 - val_loss: 0.0598\n","Epoch 289/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0390\n","Epoch 289: val_loss improved from 0.05982 to 0.05973, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0390 - val_loss: 0.0597\n","Epoch 290/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 290: val_loss improved from 0.05973 to 0.05964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0389 - val_loss: 0.0596\n","Epoch 291/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0388\n","Epoch 291: val_loss improved from 0.05964 to 0.05955, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0388 - val_loss: 0.0596\n","Epoch 292/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 292: val_loss improved from 0.05955 to 0.05946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0386 - val_loss: 0.0595\n","Epoch 293/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0385\n","Epoch 293: val_loss improved from 0.05946 to 0.05938, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0385 - val_loss: 0.0594\n","Epoch 294/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 294: val_loss improved from 0.05938 to 0.05929, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0383 - val_loss: 0.0593\n","Epoch 295/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0382\n","Epoch 295: val_loss improved from 0.05929 to 0.05921, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0382 - val_loss: 0.0592\n","Epoch 296/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 296: val_loss improved from 0.05921 to 0.05912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0381 - val_loss: 0.0591\n","Epoch 297/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 297: val_loss improved from 0.05912 to 0.05904, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0380 - val_loss: 0.0590\n","Epoch 298/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 298: val_loss improved from 0.05904 to 0.05896, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0378 - val_loss: 0.0590\n","Epoch 299/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 299: val_loss improved from 0.05896 to 0.05889, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0377 - val_loss: 0.0589\n","Epoch 300/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 300: val_loss improved from 0.05889 to 0.05881, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0376 - val_loss: 0.0588\n","Epoch 301/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 301: val_loss improved from 0.05881 to 0.05873, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0374 - val_loss: 0.0587\n","Epoch 302/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0373\n","Epoch 302: val_loss improved from 0.05873 to 0.05866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0373 - val_loss: 0.0587\n","Epoch 303/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 303: val_loss improved from 0.05866 to 0.05858, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0372 - val_loss: 0.0586\n","Epoch 304/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 304: val_loss improved from 0.05858 to 0.05851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0371 - val_loss: 0.0585\n","Epoch 305/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 305: val_loss improved from 0.05851 to 0.05844, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0370 - val_loss: 0.0584\n","Epoch 306/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 306: val_loss improved from 0.05844 to 0.05837, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0369 - val_loss: 0.0584\n","Epoch 307/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 307: val_loss improved from 0.05837 to 0.05830, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0367 - val_loss: 0.0583\n","Epoch 308/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0366\n","Epoch 308: val_loss improved from 0.05830 to 0.05824, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0366 - val_loss: 0.0582\n","Epoch 309/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 309: val_loss improved from 0.05824 to 0.05817, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 106ms/step - loss: 0.0365 - val_loss: 0.0582\n","Epoch 310/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0364\n","Epoch 310: val_loss improved from 0.05817 to 0.05811, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 140ms/step - loss: 0.0364 - val_loss: 0.0581\n","Epoch 311/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 311: val_loss improved from 0.05811 to 0.05804, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 153ms/step - loss: 0.0363 - val_loss: 0.0580\n","Epoch 312/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 312: val_loss improved from 0.05804 to 0.05798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.0362 - val_loss: 0.0580\n","Epoch 313/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 313: val_loss improved from 0.05798 to 0.05792, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.0361 - val_loss: 0.0579\n","Epoch 314/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 314: val_loss improved from 0.05792 to 0.05786, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 140ms/step - loss: 0.0360 - val_loss: 0.0579\n","Epoch 315/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 315: val_loss improved from 0.05786 to 0.05780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 144ms/step - loss: 0.0359 - val_loss: 0.0578\n","Epoch 316/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 316: val_loss improved from 0.05780 to 0.05774, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 157ms/step - loss: 0.0358 - val_loss: 0.0577\n","Epoch 317/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 317: val_loss improved from 0.05774 to 0.05768, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 141ms/step - loss: 0.0357 - val_loss: 0.0577\n","Epoch 318/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0356\n","Epoch 318: val_loss improved from 0.05768 to 0.05762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.0356 - val_loss: 0.0576\n","Epoch 319/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 319: val_loss improved from 0.05762 to 0.05757, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 137ms/step - loss: 0.0355 - val_loss: 0.0576\n","Epoch 320/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 320: val_loss improved from 0.05757 to 0.05751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 116ms/step - loss: 0.0354 - val_loss: 0.0575\n","Epoch 321/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 321: val_loss improved from 0.05751 to 0.05746, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 143ms/step - loss: 0.0353 - val_loss: 0.0575\n","Epoch 322/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0352\n","Epoch 322: val_loss improved from 0.05746 to 0.05741, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 137ms/step - loss: 0.0352 - val_loss: 0.0574\n","Epoch 323/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 323: val_loss improved from 0.05741 to 0.05736, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0351 - val_loss: 0.0574\n","Epoch 324/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0350\n","Epoch 324: val_loss improved from 0.05736 to 0.05731, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0350 - val_loss: 0.0573\n","Epoch 325/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 325: val_loss improved from 0.05731 to 0.05726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0349 - val_loss: 0.0573\n","Epoch 326/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 326: val_loss improved from 0.05726 to 0.05721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 137ms/step - loss: 0.0348 - val_loss: 0.0572\n","Epoch 327/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 327: val_loss improved from 0.05721 to 0.05716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 140ms/step - loss: 0.0347 - val_loss: 0.0572\n","Epoch 328/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 328: val_loss improved from 0.05716 to 0.05711, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 152ms/step - loss: 0.0347 - val_loss: 0.0571\n","Epoch 329/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 329: val_loss improved from 0.05711 to 0.05707, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 131ms/step - loss: 0.0346 - val_loss: 0.0571\n","Epoch 330/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 330: val_loss improved from 0.05707 to 0.05702, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0345 - val_loss: 0.0570\n","Epoch 331/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 331: val_loss improved from 0.05702 to 0.05698, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 162ms/step - loss: 0.0344 - val_loss: 0.0570\n","Epoch 332/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 332: val_loss improved from 0.05698 to 0.05693, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0343 - val_loss: 0.0569\n","Epoch 333/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 333: val_loss improved from 0.05693 to 0.05689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0342 - val_loss: 0.0569\n","Epoch 334/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 334: val_loss improved from 0.05689 to 0.05685, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0342 - val_loss: 0.0568\n","Epoch 335/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 335: val_loss improved from 0.05685 to 0.05681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0341 - val_loss: 0.0568\n","Epoch 336/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 336: val_loss improved from 0.05681 to 0.05677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0340 - val_loss: 0.0568\n","Epoch 337/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0339\n","Epoch 337: val_loss improved from 0.05677 to 0.05673, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0339 - val_loss: 0.0567\n","Epoch 338/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 338: val_loss improved from 0.05673 to 0.05669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0338 - val_loss: 0.0567\n","Epoch 339/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 339: val_loss improved from 0.05669 to 0.05665, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0338 - val_loss: 0.0566\n","Epoch 340/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 340: val_loss improved from 0.05665 to 0.05661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0337 - val_loss: 0.0566\n","Epoch 341/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 341: val_loss improved from 0.05661 to 0.05657, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0336 - val_loss: 0.0566\n","Epoch 342/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 342: val_loss improved from 0.05657 to 0.05654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0335 - val_loss: 0.0565\n","Epoch 343/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 343: val_loss improved from 0.05654 to 0.05650, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0335 - val_loss: 0.0565\n","Epoch 344/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 344: val_loss improved from 0.05650 to 0.05647, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0334 - val_loss: 0.0565\n","Epoch 345/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 345: val_loss improved from 0.05647 to 0.05643, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 346/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 346: val_loss improved from 0.05643 to 0.05640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0333 - val_loss: 0.0564\n","Epoch 347/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 347: val_loss improved from 0.05640 to 0.05637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0332 - val_loss: 0.0564\n","Epoch 348/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 348: val_loss improved from 0.05637 to 0.05634, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 349/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 349: val_loss improved from 0.05634 to 0.05631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0331 - val_loss: 0.0563\n","Epoch 350/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 350: val_loss improved from 0.05631 to 0.05627, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0330 - val_loss: 0.0563\n","Epoch 351/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 351: val_loss improved from 0.05627 to 0.05624, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 352/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 352: val_loss improved from 0.05624 to 0.05622, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0329 - val_loss: 0.0562\n","Epoch 353/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 353: val_loss improved from 0.05622 to 0.05619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0328 - val_loss: 0.0562\n","Epoch 354/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0328\n","Epoch 354: val_loss improved from 0.05619 to 0.05616, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0328 - val_loss: 0.0562\n","Epoch 355/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 355: val_loss improved from 0.05616 to 0.05613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0327 - val_loss: 0.0561\n","Epoch 356/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 356: val_loss improved from 0.05613 to 0.05610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 357/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 357: val_loss improved from 0.05610 to 0.05608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0326 - val_loss: 0.0561\n","Epoch 358/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 358: val_loss improved from 0.05608 to 0.05605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0325 - val_loss: 0.0561\n","Epoch 359/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 359: val_loss improved from 0.05605 to 0.05603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0325 - val_loss: 0.0560\n","Epoch 360/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 360: val_loss improved from 0.05603 to 0.05600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0324 - val_loss: 0.0560\n","Epoch 361/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 361: val_loss improved from 0.05600 to 0.05598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0323 - val_loss: 0.0560\n","Epoch 362/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 362: val_loss improved from 0.05598 to 0.05595, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0323 - val_loss: 0.0560\n","Epoch 363/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 363: val_loss improved from 0.05595 to 0.05593, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 364/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 364: val_loss improved from 0.05593 to 0.05591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0322 - val_loss: 0.0559\n","Epoch 365/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 365: val_loss improved from 0.05591 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 366/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 366: val_loss improved from 0.05588 to 0.05586, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0321 - val_loss: 0.0559\n","Epoch 367/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 367: val_loss improved from 0.05586 to 0.05584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 368/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 368: val_loss improved from 0.05584 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0320 - val_loss: 0.0558\n","Epoch 369/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 369: val_loss improved from 0.05582 to 0.05580, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 370/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 370: val_loss improved from 0.05580 to 0.05578, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0319 - val_loss: 0.0558\n","Epoch 371/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 371: val_loss improved from 0.05578 to 0.05576, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 372/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 372: val_loss improved from 0.05576 to 0.05574, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0318 - val_loss: 0.0557\n","Epoch 373/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 373: val_loss improved from 0.05574 to 0.05572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 374/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 374: val_loss improved from 0.05572 to 0.05571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0317 - val_loss: 0.0557\n","Epoch 375/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 375: val_loss improved from 0.05571 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 376/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 376: val_loss improved from 0.05569 to 0.05567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0316 - val_loss: 0.0557\n","Epoch 377/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 377: val_loss improved from 0.05567 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0315 - val_loss: 0.0557\n","Epoch 378/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 378: val_loss improved from 0.05565 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0315 - val_loss: 0.0556\n","Epoch 379/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 379: val_loss improved from 0.05564 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 380/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 380: val_loss improved from 0.05562 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 381/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 381: val_loss improved from 0.05561 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0314 - val_loss: 0.0556\n","Epoch 382/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 382: val_loss improved from 0.05559 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 383/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 383: val_loss improved from 0.05558 to 0.05556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 384/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 384: val_loss improved from 0.05556 to 0.05555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 385/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 385: val_loss improved from 0.05555 to 0.05554, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 386/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 386: val_loss improved from 0.05554 to 0.05552, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 387/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 387: val_loss improved from 0.05552 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 388/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 388: val_loss improved from 0.05551 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0311 - val_loss: 0.0555\n","Epoch 389/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 389: val_loss improved from 0.05550 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 390/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 390: val_loss improved from 0.05548 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 391/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 391: val_loss improved from 0.05547 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0310 - val_loss: 0.0555\n","Epoch 392/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 392: val_loss improved from 0.05546 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 393/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 393: val_loss improved from 0.05545 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 394/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 394: val_loss improved from 0.05544 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 395/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 395: val_loss improved from 0.05543 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 396/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 396: val_loss improved from 0.05542 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 397/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 397: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 398/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 398: val_loss improved from 0.05540 to 0.05539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 399/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 399: val_loss improved from 0.05539 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0307 - val_loss: 0.0554\n","Epoch 400/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 400: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 401/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 401: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 402/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 402: val_loss improved from 0.05536 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 403/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 403: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 404/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 404: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 405/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 405: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 406/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 406: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 407/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 407: val_loss improved from 0.05532 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 408/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 408: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 409/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 409: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 410/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 410: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 411/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 411: val_loss improved from 0.05530 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 412/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 412: val_loss improved from 0.05529 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 413/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 413: val_loss improved from 0.05529 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 414/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 414: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 415/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 415: val_loss improved from 0.05528 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 104ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 416/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 416: val_loss improved from 0.05527 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 417/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 417: val_loss improved from 0.05527 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 418/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 418: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 419/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 419: val_loss improved from 0.05526 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 420/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 420: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0301 - val_loss: 0.0552\n","Epoch 421/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 421: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 89ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 422/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 422: val_loss improved from 0.05525 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 423/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 423: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 424/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 424: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0300 - val_loss: 0.0552\n","Epoch 425/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 425: val_loss improved from 0.05524 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 145ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 426/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 426: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 253ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 427/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 427: val_loss improved from 0.05523 to 0.05523, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 193ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 428/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 428: val_loss improved from 0.05523 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 239ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 429/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 429: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 430/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 430: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 431/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 431: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 156ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 432/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 432: val_loss improved from 0.05522 to 0.05522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 433/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 433: val_loss improved from 0.05522 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 434/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 434: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 435/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 435: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 436/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 436: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 437/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 437: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 144ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 438/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 438: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 130ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 439/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 439: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 141ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 440/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 440: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 147ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 441/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 441: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 442/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 442: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 443/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 443: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 444/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 444: val_loss improved from 0.05521 to 0.05521, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 445/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 445: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 446/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 446: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 93ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 447/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 447: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 448/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 448: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0295 - val_loss: 0.0552\n","Epoch 449/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 449: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 450/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 450: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 451/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 451: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 452/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 452: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 453/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 453: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 126ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 454/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 454: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0294 - val_loss: 0.0552\n","Epoch 455/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 455: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 456/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 456: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 457/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 457: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 458/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 458: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 459/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 459: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 460/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 460: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 461/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 461: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0293 - val_loss: 0.0552\n","Epoch 462/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 462: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 463/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 463: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 464/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 464: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 465/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 465: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 466/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 466: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 467/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 467: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 468/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 468: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0292 - val_loss: 0.0552\n","Epoch 469/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 469: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 470/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 470: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 471/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 471: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 472/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 472: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 473/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 473: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0291 - val_loss: 0.0552\n","Epoch 474/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 474: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 475/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 475: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 476/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 476: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 477/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 477: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 478/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 478: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 479/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 479: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 480/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 480: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 481/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 481: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 482/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 482: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 483/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 483: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 484/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 484: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 485/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 485: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 486/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 486: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 487/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 487: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 488/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 488: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 489/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 489: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 490/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 490: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 491/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 491: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 492/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 492: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 493/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 493: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 494/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 494: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 495/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 495: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0289 - val_loss: 0.0553\n","Epoch 496/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 496: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 497/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 497: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 498/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 498: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 499/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 499: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 500/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 500: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 501/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 501: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 502/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 502: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0288 - val_loss: 0.0553\n","Epoch 503/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 503: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 504/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 504: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 505/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 505: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 506/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 506: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 507/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 507: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 508/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 508: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 509/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 509: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 510/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 510: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 511/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 511: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 512/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 512: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 513/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 513: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 514/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 514: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 515/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 515: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 516/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 516: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 517/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 517: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 518/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 518: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 519/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 519: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 520/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 520: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 521/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 521: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0287 - val_loss: 0.0554\n","Epoch 522/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 522: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 523/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 523: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 524/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 524: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 525/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 525: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 526/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 526: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0286 - val_loss: 0.0554\n","Epoch 527/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 527: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 528/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 528: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 529/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 529: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 530/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 530: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 531/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 531: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 532/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 532: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 533/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 533: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 534/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 534: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 535/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 535: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 536/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 536: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 537/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 537: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 538/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 538: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 539/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 539: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 540/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 540: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 541/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 541: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 542/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 542: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 543/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 543: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0285 - val_loss: 0.0555\n","Epoch 544/1000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 544: val_loss did not improve from 0.05521\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0285 - val_loss: 0.0555\n","1/1 [==============================] - 0s 184ms/step - loss: 0.0734\n","loss_and_metrics : 0.07344134151935577\n","1/1 [==============================] - 0s 165ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","#from tensorflow.keras.optimizers import SGD\n","#from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import PReLU\n","#from tensorflow.keras.optimizers import Adam\n","#from tensorflow.keras.optimizers import Nadam\n","#from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"PReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"PReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"PReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"PReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"PReLU\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=1000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}