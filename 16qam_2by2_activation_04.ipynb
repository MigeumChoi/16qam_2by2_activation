{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+cMXwI13VdaMhCkhN+a9s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"WDs_7sOgop9n","executionInfo":{"status":"ok","timestamp":1695012949801,"user_tz":-540,"elapsed":27024,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"79a59221-8480-4175-db68-44251f353771"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.4015\n","Epoch 1: val_loss improved from inf to 0.37274, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 0.4015 - val_loss: 0.3727\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3855\n","Epoch 2: val_loss improved from 0.37274 to 0.35896, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.3855 - val_loss: 0.3590\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3707\n","Epoch 3: val_loss improved from 0.35896 to 0.34603, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3707 - val_loss: 0.3460\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3568"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4: val_loss improved from 0.34603 to 0.33377, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3568 - val_loss: 0.3338\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3435\n","Epoch 5: val_loss improved from 0.33377 to 0.32213, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3435 - val_loss: 0.3221\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3309\n","Epoch 6: val_loss improved from 0.32213 to 0.31091, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.3309 - val_loss: 0.3109\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3187\n","Epoch 7: val_loss improved from 0.31091 to 0.30023, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.3187 - val_loss: 0.3002\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3070\n","Epoch 8: val_loss improved from 0.30023 to 0.29000, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3070 - val_loss: 0.2900\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2958\n","Epoch 9: val_loss improved from 0.29000 to 0.28009, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2958 - val_loss: 0.2801\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2850\n","Epoch 10: val_loss improved from 0.28009 to 0.27040, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.2850 - val_loss: 0.2704\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2744\n","Epoch 11: val_loss improved from 0.27040 to 0.26105, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2744 - val_loss: 0.2610\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2641\n","Epoch 12: val_loss improved from 0.26105 to 0.25203, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.2641 - val_loss: 0.2520\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2543\n","Epoch 13: val_loss improved from 0.25203 to 0.24329, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2543 - val_loss: 0.2433\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2447\n","Epoch 14: val_loss improved from 0.24329 to 0.23487, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2447 - val_loss: 0.2349\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2356\n","Epoch 15: val_loss improved from 0.23487 to 0.22668, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2356 - val_loss: 0.2267\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2266\n","Epoch 16: val_loss improved from 0.22668 to 0.21866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2266 - val_loss: 0.2187\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2178\n","Epoch 17: val_loss improved from 0.21866 to 0.21088, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2178 - val_loss: 0.2109\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2093\n","Epoch 18: val_loss improved from 0.21088 to 0.20333, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2093 - val_loss: 0.2033\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2010\n","Epoch 19: val_loss improved from 0.20333 to 0.19601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2010 - val_loss: 0.1960\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1930\n","Epoch 20: val_loss improved from 0.19601 to 0.18891, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1930 - val_loss: 0.1889\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1851\n","Epoch 21: val_loss improved from 0.18891 to 0.18199, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1851 - val_loss: 0.1820\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1775\n","Epoch 22: val_loss improved from 0.18199 to 0.17527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1775 - val_loss: 0.1753\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1700\n","Epoch 23: val_loss improved from 0.17527 to 0.16876, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1700 - val_loss: 0.1688\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1628\n","Epoch 24: val_loss improved from 0.16876 to 0.16244, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 56ms/step - loss: 0.1628 - val_loss: 0.1624\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1557\n","Epoch 25: val_loss improved from 0.16244 to 0.15633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.1557 - val_loss: 0.1563\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1489\n","Epoch 26: val_loss improved from 0.15633 to 0.15042, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1489 - val_loss: 0.1504\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1422\n","Epoch 27: val_loss improved from 0.15042 to 0.14471, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.1422 - val_loss: 0.1447\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1358\n","Epoch 28: val_loss improved from 0.14471 to 0.13920, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1358 - val_loss: 0.1392\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1296\n","Epoch 29: val_loss improved from 0.13920 to 0.13390, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1296 - val_loss: 0.1339\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1237\n","Epoch 30: val_loss improved from 0.13390 to 0.12882, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1237 - val_loss: 0.1288\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1179\n","Epoch 31: val_loss improved from 0.12882 to 0.12395, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1179 - val_loss: 0.1240\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1124\n","Epoch 32: val_loss improved from 0.12395 to 0.11930, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1124 - val_loss: 0.1193\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1071\n","Epoch 33: val_loss improved from 0.11930 to 0.11485, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1071 - val_loss: 0.1149\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1020\n","Epoch 34: val_loss improved from 0.11485 to 0.11061, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1020 - val_loss: 0.1106\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0972\n","Epoch 35: val_loss improved from 0.11061 to 0.10659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0972 - val_loss: 0.1066\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0926\n","Epoch 36: val_loss improved from 0.10659 to 0.10277, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0926 - val_loss: 0.1028\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0882\n","Epoch 37: val_loss improved from 0.10277 to 0.09916, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0882 - val_loss: 0.0992\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0840\n","Epoch 38: val_loss improved from 0.09916 to 0.09575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0840 - val_loss: 0.0957\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0801\n","Epoch 39: val_loss improved from 0.09575 to 0.09254, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0801 - val_loss: 0.0925\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0763\n","Epoch 40: val_loss improved from 0.09254 to 0.08952, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0763 - val_loss: 0.0895\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0728\n","Epoch 41: val_loss improved from 0.08952 to 0.08670, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0728 - val_loss: 0.0867\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0695\n","Epoch 42: val_loss improved from 0.08670 to 0.08407, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0695 - val_loss: 0.0841\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0664\n","Epoch 43: val_loss improved from 0.08407 to 0.08161, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0664 - val_loss: 0.0816\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0635\n","Epoch 44: val_loss improved from 0.08161 to 0.07933, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0635 - val_loss: 0.0793\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0607\n","Epoch 45: val_loss improved from 0.07933 to 0.07721, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0607 - val_loss: 0.0772\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0582\n","Epoch 46: val_loss improved from 0.07721 to 0.07524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0582 - val_loss: 0.0752\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0558\n","Epoch 47: val_loss improved from 0.07524 to 0.07343, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0558 - val_loss: 0.0734\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0536\n","Epoch 48: val_loss improved from 0.07343 to 0.07175, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0536 - val_loss: 0.0718\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0515\n","Epoch 49: val_loss improved from 0.07175 to 0.07021, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0515 - val_loss: 0.0702\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0496\n","Epoch 50: val_loss improved from 0.07021 to 0.06879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0496 - val_loss: 0.0688\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0478\n","Epoch 51: val_loss improved from 0.06879 to 0.06749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0478 - val_loss: 0.0675\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0461\n","Epoch 52: val_loss improved from 0.06749 to 0.06630, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0461 - val_loss: 0.0663\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 53: val_loss improved from 0.06630 to 0.06522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0446 - val_loss: 0.0652\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 54: val_loss improved from 0.06522 to 0.06423, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0432 - val_loss: 0.0642\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0419\n","Epoch 55: val_loss improved from 0.06423 to 0.06333, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0419 - val_loss: 0.0633\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0407\n","Epoch 56: val_loss improved from 0.06333 to 0.06252, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0407 - val_loss: 0.0625\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0397\n","Epoch 57: val_loss improved from 0.06252 to 0.06178, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0397 - val_loss: 0.0618\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0387\n","Epoch 58: val_loss improved from 0.06178 to 0.06111, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0387 - val_loss: 0.0611\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0377\n","Epoch 59: val_loss improved from 0.06111 to 0.06050, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0377 - val_loss: 0.0605\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 60: val_loss improved from 0.06050 to 0.05995, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0369 - val_loss: 0.0600\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 61: val_loss improved from 0.05995 to 0.05946, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0361 - val_loss: 0.0595\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0354\n","Epoch 62: val_loss improved from 0.05946 to 0.05901, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0354 - val_loss: 0.0590\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 63: val_loss improved from 0.05901 to 0.05861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0347 - val_loss: 0.0586\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 64: val_loss improved from 0.05861 to 0.05825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0341 - val_loss: 0.0583\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 65: val_loss improved from 0.05825 to 0.05793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0336 - val_loss: 0.0579\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 66: val_loss improved from 0.05793 to 0.05764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0330 - val_loss: 0.0576\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 67: val_loss improved from 0.05764 to 0.05738, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0326 - val_loss: 0.0574\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 68: val_loss improved from 0.05738 to 0.05714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0321 - val_loss: 0.0571\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 69: val_loss improved from 0.05714 to 0.05694, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0317 - val_loss: 0.0569\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 70: val_loss improved from 0.05694 to 0.05675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0314 - val_loss: 0.0568\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 71: val_loss improved from 0.05675 to 0.05659, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0310 - val_loss: 0.0566\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 72: val_loss improved from 0.05659 to 0.05645, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0307 - val_loss: 0.0564\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 73: val_loss improved from 0.05645 to 0.05632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0305 - val_loss: 0.0563\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 74: val_loss improved from 0.05632 to 0.05620, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0302 - val_loss: 0.0562\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 75: val_loss improved from 0.05620 to 0.05610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0300 - val_loss: 0.0561\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 76: val_loss improved from 0.05610 to 0.05602, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0297 - val_loss: 0.0560\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 77: val_loss improved from 0.05602 to 0.05594, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0295 - val_loss: 0.0559\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 78: val_loss improved from 0.05594 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0293 - val_loss: 0.0559\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 79: val_loss improved from 0.05588 to 0.05582, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0292 - val_loss: 0.0558\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 80: val_loss improved from 0.05582 to 0.05577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0290 - val_loss: 0.0558\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 81: val_loss improved from 0.05577 to 0.05573, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0289 - val_loss: 0.0557\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 82: val_loss improved from 0.05573 to 0.05569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0287 - val_loss: 0.0557\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 83: val_loss improved from 0.05569 to 0.05567, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 84: val_loss improved from 0.05567 to 0.05564, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 85: val_loss improved from 0.05564 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0284 - val_loss: 0.0556\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 86: val_loss improved from 0.05562 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0283 - val_loss: 0.0556\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 87: val_loss improved from 0.05561 to 0.05560, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0282 - val_loss: 0.0556\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 88: val_loss improved from 0.05560 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0281 - val_loss: 0.0556\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 89: val_loss improved from 0.05559 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0280 - val_loss: 0.0556\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0279\n","Epoch 90: val_loss improved from 0.05558 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0279 - val_loss: 0.0556\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 91: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0278 - val_loss: 0.0556\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0278\n","Epoch 92: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0278 - val_loss: 0.0556\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0277\n","Epoch 93: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0277 - val_loss: 0.0556\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 94: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0276 - val_loss: 0.0556\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0276\n","Epoch 95: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0276 - val_loss: 0.0556\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 96: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0275 - val_loss: 0.0556\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0275\n","Epoch 97: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0275 - val_loss: 0.0556\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 98: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0274 - val_loss: 0.0556\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0274\n","Epoch 99: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 36ms/step - loss: 0.0274 - val_loss: 0.0557\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 100: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0273 - val_loss: 0.0557\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0273\n","Epoch 101: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0273 - val_loss: 0.0557\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 102: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0272 - val_loss: 0.0557\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 103: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0272 - val_loss: 0.0557\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0272\n","Epoch 104: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0272 - val_loss: 0.0557\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0271\n","Epoch 105: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0271 - val_loss: 0.0558\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0271\n","Epoch 106: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0271 - val_loss: 0.0558\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0271\n","Epoch 107: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0271 - val_loss: 0.0558\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0270\n","Epoch 108: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0270 - val_loss: 0.0558\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0270\n","Epoch 109: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0270 - val_loss: 0.0558\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0270\n","Epoch 110: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0270 - val_loss: 0.0559\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0269\n","Epoch 111: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0269 - val_loss: 0.0559\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0269\n","Epoch 112: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0269 - val_loss: 0.0559\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0269\n","Epoch 113: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0269 - val_loss: 0.0559\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0269\n","Epoch 114: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0269 - val_loss: 0.0559\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0268\n","Epoch 115: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0268 - val_loss: 0.0560\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0268\n","Epoch 116: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0268 - val_loss: 0.0560\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0268\n","Epoch 117: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0268 - val_loss: 0.0560\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0268\n","Epoch 118: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0268 - val_loss: 0.0560\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0267\n","Epoch 119: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0267 - val_loss: 0.0560\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0267\n","Epoch 120: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0267 - val_loss: 0.0561\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0267\n","Epoch 121: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0267 - val_loss: 0.0561\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0267\n","Epoch 122: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0267 - val_loss: 0.0561\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0267\n","Epoch 123: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0267 - val_loss: 0.0561\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0266\n","Epoch 124: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0266 - val_loss: 0.0561\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0266\n","Epoch 125: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0266 - val_loss: 0.0562\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0266\n","Epoch 126: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0266 - val_loss: 0.0562\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0266\n","Epoch 127: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0266 - val_loss: 0.0562\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0265\n","Epoch 128: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0265 - val_loss: 0.0562\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0265\n","Epoch 129: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0265 - val_loss: 0.0562\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0265\n","Epoch 130: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0265 - val_loss: 0.0563\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0265\n","Epoch 131: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0265 - val_loss: 0.0563\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0265\n","Epoch 132: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0265 - val_loss: 0.0563\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 133: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0264 - val_loss: 0.0563\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 134: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0264 - val_loss: 0.0564\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 135: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0264 - val_loss: 0.0564\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 136: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0264 - val_loss: 0.0564\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 137: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0264 - val_loss: 0.0564\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0264\n","Epoch 138: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0264 - val_loss: 0.0564\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 139: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0263 - val_loss: 0.0565\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 140: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0263 - val_loss: 0.0565\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 141: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0263 - val_loss: 0.0565\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 142: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0263 - val_loss: 0.0565\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0263\n","Epoch 143: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0263 - val_loss: 0.0565\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 144: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0262 - val_loss: 0.0566\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 145: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0262 - val_loss: 0.0566\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 146: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0262 - val_loss: 0.0566\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 147: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0262 - val_loss: 0.0566\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 148: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0262 - val_loss: 0.0566\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0262\n","Epoch 149: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0262 - val_loss: 0.0567\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 150: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0261 - val_loss: 0.0567\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 151: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0261 - val_loss: 0.0567\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 152: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0261 - val_loss: 0.0567\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 153: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0261 - val_loss: 0.0567\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 154: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0261 - val_loss: 0.0568\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 155: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0260 - val_loss: 0.0568\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 156: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0260 - val_loss: 0.0568\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 157: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0260 - val_loss: 0.0568\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 158: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0260 - val_loss: 0.0569\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 159: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0260 - val_loss: 0.0569\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0260\n","Epoch 160: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0260 - val_loss: 0.0569\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 161: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0259 - val_loss: 0.0569\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 162: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0259 - val_loss: 0.0569\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 163: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0259 - val_loss: 0.0570\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 164: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0259 - val_loss: 0.0570\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 165: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0259 - val_loss: 0.0570\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0259\n","Epoch 166: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0259 - val_loss: 0.0570\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 167: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 38ms/step - loss: 0.0258 - val_loss: 0.0570\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 168: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0258 - val_loss: 0.0571\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 169: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0258 - val_loss: 0.0571\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 170: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0258 - val_loss: 0.0571\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0258\n","Epoch 171: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0258 - val_loss: 0.0571\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 172: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0257 - val_loss: 0.0571\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 173: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0257 - val_loss: 0.0572\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 174: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0257 - val_loss: 0.0572\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 175: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0257 - val_loss: 0.0572\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 176: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0257 - val_loss: 0.0572\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 177: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0257 - val_loss: 0.0572\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 178: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0256 - val_loss: 0.0573\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 179: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0256 - val_loss: 0.0573\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 180: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0256 - val_loss: 0.0573\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 181: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0256 - val_loss: 0.0573\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 182: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0256 - val_loss: 0.0574\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0256\n","Epoch 183: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0256 - val_loss: 0.0574\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 184: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0255 - val_loss: 0.0574\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 185: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0255 - val_loss: 0.0574\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 186: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0255 - val_loss: 0.0574\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 187: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0255 - val_loss: 0.0575\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 188: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0255 - val_loss: 0.0575\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0255\n","Epoch 189: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 41ms/step - loss: 0.0255 - val_loss: 0.0575\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0254\n","Epoch 190: val_loss did not improve from 0.05558\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0254 - val_loss: 0.0575\n","1/1 [==============================] - 0s 120ms/step - loss: 0.0739\n","loss_and_metrics : 0.0739499181509018\n","1/1 [==============================] - 0s 159ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"selu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"selu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"selu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"selu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"selu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}