{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhxcMqmKaI0dx/mYgoRdXA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dgnmmdC40HMk","executionInfo":{"status":"ok","timestamp":1695012220844,"user_tz":-540,"elapsed":49860,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"d584b272-adec-4e96-f8a6-85597436e53c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3710\n","Epoch 1: val_loss improved from inf to 0.34997, saving model to hl5_0100.h5\n","1/1 [==============================] - 2s 2s/step - loss: 0.3710 - val_loss: 0.3500\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3662\n","Epoch 2: val_loss improved from 0.34997 to 0.34551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.3662 - val_loss: 0.3455\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3614\n","Epoch 3: val_loss improved from 0.34551 to 0.34111, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.3614 - val_loss: 0.3411\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3567"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4: val_loss improved from 0.34111 to 0.33678, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3567 - val_loss: 0.3368\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3520\n","Epoch 5: val_loss improved from 0.33678 to 0.33251, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3520 - val_loss: 0.3325\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3475\n","Epoch 6: val_loss improved from 0.33251 to 0.32831, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3475 - val_loss: 0.3283\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3429\n","Epoch 7: val_loss improved from 0.32831 to 0.32416, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3429 - val_loss: 0.3242\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3385\n","Epoch 8: val_loss improved from 0.32416 to 0.32008, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3385 - val_loss: 0.3201\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3341\n","Epoch 9: val_loss improved from 0.32008 to 0.31605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.3341 - val_loss: 0.3161\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3298\n","Epoch 10: val_loss improved from 0.31605 to 0.31209, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.3298 - val_loss: 0.3121\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3255\n","Epoch 11: val_loss improved from 0.31209 to 0.30817, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3255 - val_loss: 0.3082\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3213\n","Epoch 12: val_loss improved from 0.30817 to 0.30432, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.3213 - val_loss: 0.3043\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3172\n","Epoch 13: val_loss improved from 0.30432 to 0.30051, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.3172 - val_loss: 0.3005\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3131\n","Epoch 14: val_loss improved from 0.30051 to 0.29677, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.3131 - val_loss: 0.2968\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3090\n","Epoch 15: val_loss improved from 0.29677 to 0.29307, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.3090 - val_loss: 0.2931\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3050\n","Epoch 16: val_loss improved from 0.29307 to 0.28942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.3050 - val_loss: 0.2894\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3011\n","Epoch 17: val_loss improved from 0.28942 to 0.28583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.3011 - val_loss: 0.2858\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2972\n","Epoch 18: val_loss improved from 0.28583 to 0.28228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 144ms/step - loss: 0.2972 - val_loss: 0.2823\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2934\n","Epoch 19: val_loss improved from 0.28228 to 0.27879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 135ms/step - loss: 0.2934 - val_loss: 0.2788\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2896\n","Epoch 20: val_loss improved from 0.27879 to 0.27534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 140ms/step - loss: 0.2896 - val_loss: 0.2753\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2859\n","Epoch 21: val_loss improved from 0.27534 to 0.27194, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.2859 - val_loss: 0.2719\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2822\n","Epoch 22: val_loss improved from 0.27194 to 0.26858, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 203ms/step - loss: 0.2822 - val_loss: 0.2686\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2786\n","Epoch 23: val_loss improved from 0.26858 to 0.26527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 109ms/step - loss: 0.2786 - val_loss: 0.2653\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2750\n","Epoch 24: val_loss improved from 0.26527 to 0.26201, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 137ms/step - loss: 0.2750 - val_loss: 0.2620\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2715\n","Epoch 25: val_loss improved from 0.26201 to 0.25879, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 151ms/step - loss: 0.2715 - val_loss: 0.2588\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2680\n","Epoch 26: val_loss improved from 0.25879 to 0.25561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 136ms/step - loss: 0.2680 - val_loss: 0.2556\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2645\n","Epoch 27: val_loss improved from 0.25561 to 0.25248, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 261ms/step - loss: 0.2645 - val_loss: 0.2525\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2611\n","Epoch 28: val_loss improved from 0.25248 to 0.24938, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 150ms/step - loss: 0.2611 - val_loss: 0.2494\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2578\n","Epoch 29: val_loss improved from 0.24938 to 0.24633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 211ms/step - loss: 0.2578 - val_loss: 0.2463\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2545\n","Epoch 30: val_loss improved from 0.24633 to 0.24332, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 247ms/step - loss: 0.2545 - val_loss: 0.2433\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2512\n","Epoch 31: val_loss improved from 0.24332 to 0.24035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 151ms/step - loss: 0.2512 - val_loss: 0.2404\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2480\n","Epoch 32: val_loss improved from 0.24035 to 0.23742, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 142ms/step - loss: 0.2480 - val_loss: 0.2374\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2448\n","Epoch 33: val_loss improved from 0.23742 to 0.23453, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 158ms/step - loss: 0.2448 - val_loss: 0.2345\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2416\n","Epoch 34: val_loss improved from 0.23453 to 0.23168, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 172ms/step - loss: 0.2416 - val_loss: 0.2317\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2385\n","Epoch 35: val_loss improved from 0.23168 to 0.22886, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 166ms/step - loss: 0.2385 - val_loss: 0.2289\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2354\n","Epoch 36: val_loss improved from 0.22886 to 0.22609, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 155ms/step - loss: 0.2354 - val_loss: 0.2261\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2324\n","Epoch 37: val_loss improved from 0.22609 to 0.22334, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 173ms/step - loss: 0.2324 - val_loss: 0.2233\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2294\n","Epoch 38: val_loss improved from 0.22334 to 0.22064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.2294 - val_loss: 0.2206\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2265\n","Epoch 39: val_loss improved from 0.22064 to 0.21797, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 207ms/step - loss: 0.2265 - val_loss: 0.2180\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2235\n","Epoch 40: val_loss improved from 0.21797 to 0.21534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 212ms/step - loss: 0.2235 - val_loss: 0.2153\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2207\n","Epoch 41: val_loss improved from 0.21534 to 0.21274, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.2207 - val_loss: 0.2127\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2178\n","Epoch 42: val_loss improved from 0.21274 to 0.21018, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 322ms/step - loss: 0.2178 - val_loss: 0.2102\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2150\n","Epoch 43: val_loss improved from 0.21018 to 0.20764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 241ms/step - loss: 0.2150 - val_loss: 0.2076\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2122\n","Epoch 44: val_loss improved from 0.20764 to 0.20515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 329ms/step - loss: 0.2122 - val_loss: 0.2051\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2095\n","Epoch 45: val_loss improved from 0.20515 to 0.20268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 255ms/step - loss: 0.2095 - val_loss: 0.2027\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2068\n","Epoch 46: val_loss improved from 0.20268 to 0.20025, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 253ms/step - loss: 0.2068 - val_loss: 0.2003\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2041\n","Epoch 47: val_loss improved from 0.20025 to 0.19785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 203ms/step - loss: 0.2041 - val_loss: 0.1979\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2015\n","Epoch 48: val_loss improved from 0.19785 to 0.19549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 199ms/step - loss: 0.2015 - val_loss: 0.1955\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1989\n","Epoch 49: val_loss improved from 0.19549 to 0.19315, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 139ms/step - loss: 0.1989 - val_loss: 0.1932\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1963\n","Epoch 50: val_loss improved from 0.19315 to 0.19085, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 133ms/step - loss: 0.1963 - val_loss: 0.1908\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1938\n","Epoch 51: val_loss improved from 0.19085 to 0.18857, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 266ms/step - loss: 0.1938 - val_loss: 0.1886\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1913\n","Epoch 52: val_loss improved from 0.18857 to 0.18633, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 163ms/step - loss: 0.1913 - val_loss: 0.1863\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1888\n","Epoch 53: val_loss improved from 0.18633 to 0.18411, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 165ms/step - loss: 0.1888 - val_loss: 0.1841\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1863\n","Epoch 54: val_loss improved from 0.18411 to 0.18193, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 248ms/step - loss: 0.1863 - val_loss: 0.1819\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1839\n","Epoch 55: val_loss improved from 0.18193 to 0.17977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 225ms/step - loss: 0.1839 - val_loss: 0.1798\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1815\n","Epoch 56: val_loss improved from 0.17977 to 0.17764, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 174ms/step - loss: 0.1815 - val_loss: 0.1776\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1792\n","Epoch 57: val_loss improved from 0.17764 to 0.17555, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 274ms/step - loss: 0.1792 - val_loss: 0.1755\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1769\n","Epoch 58: val_loss improved from 0.17555 to 0.17348, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 281ms/step - loss: 0.1769 - val_loss: 0.1735\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1746\n","Epoch 59: val_loss improved from 0.17348 to 0.17143, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 174ms/step - loss: 0.1746 - val_loss: 0.1714\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1723\n","Epoch 60: val_loss improved from 0.17143 to 0.16942, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 260ms/step - loss: 0.1723 - val_loss: 0.1694\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1700\n","Epoch 61: val_loss improved from 0.16942 to 0.16743, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 275ms/step - loss: 0.1700 - val_loss: 0.1674\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1678\n","Epoch 62: val_loss improved from 0.16743 to 0.16547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 250ms/step - loss: 0.1678 - val_loss: 0.1655\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1657\n","Epoch 63: val_loss improved from 0.16547 to 0.16353, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 340ms/step - loss: 0.1657 - val_loss: 0.1635\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1635\n","Epoch 64: val_loss improved from 0.16353 to 0.16162, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 341ms/step - loss: 0.1635 - val_loss: 0.1616\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1614\n","Epoch 65: val_loss improved from 0.16162 to 0.15974, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 254ms/step - loss: 0.1614 - val_loss: 0.1597\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1593\n","Epoch 66: val_loss improved from 0.15974 to 0.15788, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 355ms/step - loss: 0.1593 - val_loss: 0.1579\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1572\n","Epoch 67: val_loss improved from 0.15788 to 0.15605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 314ms/step - loss: 0.1572 - val_loss: 0.1561\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1551\n","Epoch 68: val_loss improved from 0.15605 to 0.15425, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 398ms/step - loss: 0.1551 - val_loss: 0.1542\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1531\n","Epoch 69: val_loss improved from 0.15425 to 0.15247, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 289ms/step - loss: 0.1531 - val_loss: 0.1525\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1511\n","Epoch 70: val_loss improved from 0.15247 to 0.15071, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 235ms/step - loss: 0.1511 - val_loss: 0.1507\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1492\n","Epoch 71: val_loss improved from 0.15071 to 0.14898, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 141ms/step - loss: 0.1492 - val_loss: 0.1490\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1472\n","Epoch 72: val_loss improved from 0.14898 to 0.14727, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 223ms/step - loss: 0.1472 - val_loss: 0.1473\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1453\n","Epoch 73: val_loss improved from 0.14727 to 0.14558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 174ms/step - loss: 0.1453 - val_loss: 0.1456\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1434\n","Epoch 74: val_loss improved from 0.14558 to 0.14392, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1434 - val_loss: 0.1439\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1415\n","Epoch 75: val_loss improved from 0.14392 to 0.14228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.1415 - val_loss: 0.1423\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1397\n","Epoch 76: val_loss improved from 0.14228 to 0.14067, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1397 - val_loss: 0.1407\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1379\n","Epoch 77: val_loss improved from 0.14067 to 0.13907, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1379 - val_loss: 0.1391\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1361\n","Epoch 78: val_loss improved from 0.13907 to 0.13750, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1361 - val_loss: 0.1375\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1343\n","Epoch 79: val_loss improved from 0.13750 to 0.13596, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1343 - val_loss: 0.1360\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1325\n","Epoch 80: val_loss improved from 0.13596 to 0.13443, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1325 - val_loss: 0.1344\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1308\n","Epoch 81: val_loss improved from 0.13443 to 0.13293, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1308 - val_loss: 0.1329\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1291\n","Epoch 82: val_loss improved from 0.13293 to 0.13144, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1291 - val_loss: 0.1314\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1274\n","Epoch 83: val_loss improved from 0.13144 to 0.12998, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1274 - val_loss: 0.1300\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1257\n","Epoch 84: val_loss improved from 0.12998 to 0.12854, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.1257 - val_loss: 0.1285\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1241\n","Epoch 85: val_loss improved from 0.12854 to 0.12712, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.1241 - val_loss: 0.1271\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1225\n","Epoch 86: val_loss improved from 0.12712 to 0.12572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.1225 - val_loss: 0.1257\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1209\n","Epoch 87: val_loss improved from 0.12572 to 0.12435, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1209 - val_loss: 0.1243\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1193\n","Epoch 88: val_loss improved from 0.12435 to 0.12299, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1193 - val_loss: 0.1230\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1178\n","Epoch 89: val_loss improved from 0.12299 to 0.12165, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.1178 - val_loss: 0.1217\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1162\n","Epoch 90: val_loss improved from 0.12165 to 0.12033, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1162 - val_loss: 0.1203\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1147\n","Epoch 91: val_loss improved from 0.12033 to 0.11904, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.1147 - val_loss: 0.1190\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1132\n","Epoch 92: val_loss improved from 0.11904 to 0.11776, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1132 - val_loss: 0.1178\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1117\n","Epoch 93: val_loss improved from 0.11776 to 0.11650, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1117 - val_loss: 0.1165\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1103\n","Epoch 94: val_loss improved from 0.11650 to 0.11526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1103 - val_loss: 0.1153\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1089\n","Epoch 95: val_loss improved from 0.11526 to 0.11404, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1089 - val_loss: 0.1140\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1074\n","Epoch 96: val_loss improved from 0.11404 to 0.11284, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1074 - val_loss: 0.1128\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1061\n","Epoch 97: val_loss improved from 0.11284 to 0.11165, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.1061 - val_loss: 0.1117\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1047\n","Epoch 98: val_loss improved from 0.11165 to 0.11049, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.1047 - val_loss: 0.1105\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1033\n","Epoch 99: val_loss improved from 0.11049 to 0.10934, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1033 - val_loss: 0.1093\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1020\n","Epoch 100: val_loss improved from 0.10934 to 0.10821, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1020 - val_loss: 0.1082\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1007\n","Epoch 101: val_loss improved from 0.10821 to 0.10710, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.1007 - val_loss: 0.1071\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0994\n","Epoch 102: val_loss improved from 0.10710 to 0.10600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0994 - val_loss: 0.1060\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0981\n","Epoch 103: val_loss improved from 0.10600 to 0.10492, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0981 - val_loss: 0.1049\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0968\n","Epoch 104: val_loss improved from 0.10492 to 0.10386, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0968 - val_loss: 0.1039\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0956\n","Epoch 105: val_loss improved from 0.10386 to 0.10282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0956 - val_loss: 0.1028\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0944\n","Epoch 106: val_loss improved from 0.10282 to 0.10180, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0944 - val_loss: 0.1018\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0932\n","Epoch 107: val_loss improved from 0.10180 to 0.10079, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0932 - val_loss: 0.1008\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0920\n","Epoch 108: val_loss improved from 0.10079 to 0.09979, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0920 - val_loss: 0.0998\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0908\n","Epoch 109: val_loss improved from 0.09979 to 0.09882, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0908 - val_loss: 0.0988\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0897\n","Epoch 110: val_loss improved from 0.09882 to 0.09786, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0897 - val_loss: 0.0979\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0885\n","Epoch 111: val_loss improved from 0.09786 to 0.09691, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0885 - val_loss: 0.0969\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0874\n","Epoch 112: val_loss improved from 0.09691 to 0.09598, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0874 - val_loss: 0.0960\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0863\n","Epoch 113: val_loss improved from 0.09598 to 0.09507, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0863 - val_loss: 0.0951\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0852\n","Epoch 114: val_loss improved from 0.09507 to 0.09417, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0852 - val_loss: 0.0942\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0841\n","Epoch 115: val_loss improved from 0.09417 to 0.09329, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0841 - val_loss: 0.0933\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0831\n","Epoch 116: val_loss improved from 0.09329 to 0.09242, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0831 - val_loss: 0.0924\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0820\n","Epoch 117: val_loss improved from 0.09242 to 0.09157, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0820 - val_loss: 0.0916\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0810\n","Epoch 118: val_loss improved from 0.09157 to 0.09073, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0810 - val_loss: 0.0907\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0800\n","Epoch 119: val_loss improved from 0.09073 to 0.08990, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0800 - val_loss: 0.0899\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0790\n","Epoch 120: val_loss improved from 0.08990 to 0.08910, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0790 - val_loss: 0.0891\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0780\n","Epoch 121: val_loss improved from 0.08910 to 0.08830, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0780 - val_loss: 0.0883\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0771\n","Epoch 122: val_loss improved from 0.08830 to 0.08752, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0771 - val_loss: 0.0875\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0761\n","Epoch 123: val_loss improved from 0.08752 to 0.08675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0761 - val_loss: 0.0868\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0752\n","Epoch 124: val_loss improved from 0.08675 to 0.08600, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0752 - val_loss: 0.0860\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0743\n","Epoch 125: val_loss improved from 0.08600 to 0.08526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0743 - val_loss: 0.0853\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0734\n","Epoch 126: val_loss improved from 0.08526 to 0.08453, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0734 - val_loss: 0.0845\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0725\n","Epoch 127: val_loss improved from 0.08453 to 0.08382, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0725 - val_loss: 0.0838\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0716\n","Epoch 128: val_loss improved from 0.08382 to 0.08312, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0716 - val_loss: 0.0831\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0708\n","Epoch 129: val_loss improved from 0.08312 to 0.08244, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0708 - val_loss: 0.0824\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0699\n","Epoch 130: val_loss improved from 0.08244 to 0.08176, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0699 - val_loss: 0.0818\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0691\n","Epoch 131: val_loss improved from 0.08176 to 0.08110, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0691 - val_loss: 0.0811\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0683\n","Epoch 132: val_loss improved from 0.08110 to 0.08045, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0683 - val_loss: 0.0805\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0675\n","Epoch 133: val_loss improved from 0.08045 to 0.07982, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0675 - val_loss: 0.0798\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0667\n","Epoch 134: val_loss improved from 0.07982 to 0.07919, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0667 - val_loss: 0.0792\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0659\n","Epoch 135: val_loss improved from 0.07919 to 0.07858, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0659 - val_loss: 0.0786\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0651\n","Epoch 136: val_loss improved from 0.07858 to 0.07798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0651 - val_loss: 0.0780\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0644\n","Epoch 137: val_loss improved from 0.07798 to 0.07739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0644 - val_loss: 0.0774\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0636\n","Epoch 138: val_loss improved from 0.07739 to 0.07681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0636 - val_loss: 0.0768\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0629\n","Epoch 139: val_loss improved from 0.07681 to 0.07625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0629 - val_loss: 0.0762\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0622\n","Epoch 140: val_loss improved from 0.07625 to 0.07569, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0622 - val_loss: 0.0757\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0615\n","Epoch 141: val_loss improved from 0.07569 to 0.07515, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0615 - val_loss: 0.0751\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0608\n","Epoch 142: val_loss improved from 0.07515 to 0.07462, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0608 - val_loss: 0.0746\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0601\n","Epoch 143: val_loss improved from 0.07462 to 0.07410, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0601 - val_loss: 0.0741\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0594\n","Epoch 144: val_loss improved from 0.07410 to 0.07359, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0594 - val_loss: 0.0736\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0588\n","Epoch 145: val_loss improved from 0.07359 to 0.07308, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0588 - val_loss: 0.0731\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0581\n","Epoch 146: val_loss improved from 0.07308 to 0.07260, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0581 - val_loss: 0.0726\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0575\n","Epoch 147: val_loss improved from 0.07260 to 0.07212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0575 - val_loss: 0.0721\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0569\n","Epoch 148: val_loss improved from 0.07212 to 0.07165, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0569 - val_loss: 0.0716\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0563\n","Epoch 149: val_loss improved from 0.07165 to 0.07119, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0563 - val_loss: 0.0712\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0557\n","Epoch 150: val_loss improved from 0.07119 to 0.07074, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0557 - val_loss: 0.0707\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0551\n","Epoch 151: val_loss improved from 0.07074 to 0.07030, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0551 - val_loss: 0.0703\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0545\n","Epoch 152: val_loss improved from 0.07030 to 0.06987, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0545 - val_loss: 0.0699\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0539\n","Epoch 153: val_loss improved from 0.06987 to 0.06945, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0539 - val_loss: 0.0694\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0534\n","Epoch 154: val_loss improved from 0.06945 to 0.06903, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0534 - val_loss: 0.0690\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0528\n","Epoch 155: val_loss improved from 0.06903 to 0.06863, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0528 - val_loss: 0.0686\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0523\n","Epoch 156: val_loss improved from 0.06863 to 0.06824, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0523 - val_loss: 0.0682\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0518\n","Epoch 157: val_loss improved from 0.06824 to 0.06785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0518 - val_loss: 0.0679\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0512\n","Epoch 158: val_loss improved from 0.06785 to 0.06748, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0512 - val_loss: 0.0675\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0507\n","Epoch 159: val_loss improved from 0.06748 to 0.06711, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0507 - val_loss: 0.0671\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0502\n","Epoch 160: val_loss improved from 0.06711 to 0.06675, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0502 - val_loss: 0.0667\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0498\n","Epoch 161: val_loss improved from 0.06675 to 0.06640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0498 - val_loss: 0.0664\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0493\n","Epoch 162: val_loss improved from 0.06640 to 0.06605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0493 - val_loss: 0.0661\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0488\n","Epoch 163: val_loss improved from 0.06605 to 0.06572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0488 - val_loss: 0.0657\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0483\n","Epoch 164: val_loss improved from 0.06572 to 0.06539, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0483 - val_loss: 0.0654\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 165: val_loss improved from 0.06539 to 0.06507, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0479 - val_loss: 0.0651\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0474\n","Epoch 166: val_loss improved from 0.06507 to 0.06476, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0474 - val_loss: 0.0648\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0470\n","Epoch 167: val_loss improved from 0.06476 to 0.06446, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0470 - val_loss: 0.0645\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0466\n","Epoch 168: val_loss improved from 0.06446 to 0.06416, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0466 - val_loss: 0.0642\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0462\n","Epoch 169: val_loss improved from 0.06416 to 0.06387, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0462 - val_loss: 0.0639\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0458\n","Epoch 170: val_loss improved from 0.06387 to 0.06359, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0458 - val_loss: 0.0636\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 171: val_loss improved from 0.06359 to 0.06332, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0454 - val_loss: 0.0633\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0450\n","Epoch 172: val_loss improved from 0.06332 to 0.06305, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0450 - val_loss: 0.0630\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0446\n","Epoch 173: val_loss improved from 0.06305 to 0.06279, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0446 - val_loss: 0.0628\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0442\n","Epoch 174: val_loss improved from 0.06279 to 0.06253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0442 - val_loss: 0.0625\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0438\n","Epoch 175: val_loss improved from 0.06253 to 0.06228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0438 - val_loss: 0.0623\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0435\n","Epoch 176: val_loss improved from 0.06228 to 0.06204, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0435 - val_loss: 0.0620\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0431\n","Epoch 177: val_loss improved from 0.06204 to 0.06180, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0431 - val_loss: 0.0618\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 178: val_loss improved from 0.06180 to 0.06158, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0428 - val_loss: 0.0616\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 179: val_loss improved from 0.06158 to 0.06135, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0424 - val_loss: 0.0614\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0421\n","Epoch 180: val_loss improved from 0.06135 to 0.06113, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0421 - val_loss: 0.0611\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0418\n","Epoch 181: val_loss improved from 0.06113 to 0.06092, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0418 - val_loss: 0.0609\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0414\n","Epoch 182: val_loss improved from 0.06092 to 0.06072, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0414 - val_loss: 0.0607\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0411\n","Epoch 183: val_loss improved from 0.06072 to 0.06052, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0411 - val_loss: 0.0605\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0408\n","Epoch 184: val_loss improved from 0.06052 to 0.06032, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0408 - val_loss: 0.0603\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0405\n","Epoch 185: val_loss improved from 0.06032 to 0.06013, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0405 - val_loss: 0.0601\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 186: val_loss improved from 0.06013 to 0.05995, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0402 - val_loss: 0.0599\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 187: val_loss improved from 0.05995 to 0.05977, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0399 - val_loss: 0.0598\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0396\n","Epoch 188: val_loss improved from 0.05977 to 0.05960, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0396 - val_loss: 0.0596\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0394\n","Epoch 189: val_loss improved from 0.05960 to 0.05943, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 125ms/step - loss: 0.0394 - val_loss: 0.0594\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0391\n","Epoch 190: val_loss improved from 0.05943 to 0.05927, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 132ms/step - loss: 0.0391 - val_loss: 0.0593\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0388\n","Epoch 191: val_loss improved from 0.05927 to 0.05911, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 119ms/step - loss: 0.0388 - val_loss: 0.0591\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 192: val_loss improved from 0.05911 to 0.05895, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.0386 - val_loss: 0.0590\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 193: val_loss improved from 0.05895 to 0.05880, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0383 - val_loss: 0.0588\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0381\n","Epoch 194: val_loss improved from 0.05880 to 0.05866, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 148ms/step - loss: 0.0381 - val_loss: 0.0587\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 195: val_loss improved from 0.05866 to 0.05852, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0378 - val_loss: 0.0585\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0376\n","Epoch 196: val_loss improved from 0.05852 to 0.05838, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 139ms/step - loss: 0.0376 - val_loss: 0.0584\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0374\n","Epoch 197: val_loss improved from 0.05838 to 0.05825, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0374 - val_loss: 0.0582\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0371\n","Epoch 198: val_loss improved from 0.05825 to 0.05812, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 117ms/step - loss: 0.0371 - val_loss: 0.0581\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0369\n","Epoch 199: val_loss improved from 0.05812 to 0.05800, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0369 - val_loss: 0.0580\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 200: val_loss improved from 0.05800 to 0.05788, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 105ms/step - loss: 0.0367 - val_loss: 0.0579\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 201: val_loss improved from 0.05788 to 0.05776, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 127ms/step - loss: 0.0365 - val_loss: 0.0578\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0363\n","Epoch 202: val_loss improved from 0.05776 to 0.05765, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0363 - val_loss: 0.0576\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0361\n","Epoch 203: val_loss improved from 0.05765 to 0.05754, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0361 - val_loss: 0.0575\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0359\n","Epoch 204: val_loss improved from 0.05754 to 0.05743, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0359 - val_loss: 0.0574\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0357\n","Epoch 205: val_loss improved from 0.05743 to 0.05733, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0357 - val_loss: 0.0573\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 206: val_loss improved from 0.05733 to 0.05723, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 121ms/step - loss: 0.0355 - val_loss: 0.0572\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 207: val_loss improved from 0.05723 to 0.05714, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0353 - val_loss: 0.0571\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 208: val_loss improved from 0.05714 to 0.05705, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 128ms/step - loss: 0.0351 - val_loss: 0.0570\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 209: val_loss improved from 0.05705 to 0.05696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 140ms/step - loss: 0.0349 - val_loss: 0.0570\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0348\n","Epoch 210: val_loss improved from 0.05696 to 0.05687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 134ms/step - loss: 0.0348 - val_loss: 0.0569\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0346\n","Epoch 211: val_loss improved from 0.05687 to 0.05679, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0346 - val_loss: 0.0568\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 212: val_loss improved from 0.05679 to 0.05671, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 0.0344 - val_loss: 0.0567\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 213: val_loss improved from 0.05671 to 0.05663, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0343 - val_loss: 0.0566\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0341\n","Epoch 214: val_loss improved from 0.05663 to 0.05656, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 83ms/step - loss: 0.0341 - val_loss: 0.0566\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 215: val_loss improved from 0.05656 to 0.05649, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0340 - val_loss: 0.0565\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 216: val_loss improved from 0.05649 to 0.05642, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0338 - val_loss: 0.0564\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0337\n","Epoch 217: val_loss improved from 0.05642 to 0.05636, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0337 - val_loss: 0.0564\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 218: val_loss improved from 0.05636 to 0.05629, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0335 - val_loss: 0.0563\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0334\n","Epoch 219: val_loss improved from 0.05629 to 0.05623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0334 - val_loss: 0.0562\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 220: val_loss improved from 0.05623 to 0.05617, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0333 - val_loss: 0.0562\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0331\n","Epoch 221: val_loss improved from 0.05617 to 0.05612, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0331 - val_loss: 0.0561\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 222: val_loss improved from 0.05612 to 0.05607, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0330 - val_loss: 0.0561\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 223: val_loss improved from 0.05607 to 0.05601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0329 - val_loss: 0.0560\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 224: val_loss improved from 0.05601 to 0.05597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0327 - val_loss: 0.0560\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 225: val_loss improved from 0.05597 to 0.05592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0326 - val_loss: 0.0559\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 226: val_loss improved from 0.05592 to 0.05587, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0325 - val_loss: 0.0559\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0324\n","Epoch 227: val_loss improved from 0.05587 to 0.05583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0324 - val_loss: 0.0558\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 228: val_loss improved from 0.05583 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0323 - val_loss: 0.0558\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 229: val_loss improved from 0.05579 to 0.05575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0322 - val_loss: 0.0558\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 230: val_loss improved from 0.05575 to 0.05572, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0321 - val_loss: 0.0557\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 231: val_loss improved from 0.05572 to 0.05568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0320 - val_loss: 0.0557\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 232: val_loss improved from 0.05568 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0319 - val_loss: 0.0556\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 233: val_loss improved from 0.05565 to 0.05561, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0318 - val_loss: 0.0556\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 234: val_loss improved from 0.05561 to 0.05558, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0317 - val_loss: 0.0556\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 235: val_loss improved from 0.05558 to 0.05556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0316 - val_loss: 0.0556\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 236: val_loss improved from 0.05556 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0315 - val_loss: 0.0555\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 237: val_loss improved from 0.05553 to 0.05550, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0314 - val_loss: 0.0555\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 238: val_loss improved from 0.05550 to 0.05548, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0313 - val_loss: 0.0555\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 239: val_loss improved from 0.05548 to 0.05546, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0312 - val_loss: 0.0555\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 240: val_loss improved from 0.05546 to 0.05544, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0311 - val_loss: 0.0554\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 241: val_loss improved from 0.05544 to 0.05542, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0310 - val_loss: 0.0554\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 242: val_loss improved from 0.05542 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0310 - val_loss: 0.0554\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 243: val_loss improved from 0.05540 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0309 - val_loss: 0.0554\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 244: val_loss improved from 0.05538 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0308 - val_loss: 0.0554\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 245: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0307 - val_loss: 0.0553\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 246: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0307 - val_loss: 0.0553\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 247: val_loss improved from 0.05534 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 100ms/step - loss: 0.0306 - val_loss: 0.0553\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 248: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 249: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0305 - val_loss: 0.0553\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 250: val_loss improved from 0.05530 to 0.05529, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0304 - val_loss: 0.0553\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 251: val_loss improved from 0.05529 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 252: val_loss improved from 0.05528 to 0.05528, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0303 - val_loss: 0.0553\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 253: val_loss improved from 0.05528 to 0.05527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 254: val_loss improved from 0.05527 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0302 - val_loss: 0.0553\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 255: val_loss improved from 0.05526 to 0.05526, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 256: val_loss improved from 0.05526 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 257: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 258: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 259: val_loss improved from 0.05525 to 0.05525, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 98ms/step - loss: 0.0299 - val_loss: 0.0552\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 260: val_loss improved from 0.05525 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 261: val_loss improved from 0.05524 to 0.05524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0298 - val_loss: 0.0552\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 262: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 263: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0297 - val_loss: 0.0552\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 264: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 265: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0296 - val_loss: 0.0552\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 266: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 267: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 268: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 269: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 270: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 271: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 272: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 273: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 274: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 275: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 276: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 277: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 278: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 279: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 280: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 281: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0290 - val_loss: 0.0553\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 282: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 283: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 284: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 285: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 286: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 287: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 288: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 289: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 290: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 291: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0288 - val_loss: 0.0554\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 292: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 293: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 294: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 295: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 296: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 297: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 298: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 299: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 300: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0286 - val_loss: 0.0555\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 301: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 302: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 303: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 304: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 305: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 306: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 307: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 308: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0285 - val_loss: 0.0556\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 309: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0284 - val_loss: 0.0557\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 310: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0284 - val_loss: 0.0557\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 311: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0284 - val_loss: 0.0557\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 312: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0284 - val_loss: 0.0557\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 313: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0284 - val_loss: 0.0557\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 314: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0284 - val_loss: 0.0557\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 315: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0284 - val_loss: 0.0557\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 316: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0283 - val_loss: 0.0557\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 317: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0283 - val_loss: 0.0558\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 318: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0283 - val_loss: 0.0558\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 319: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0283 - val_loss: 0.0558\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 320: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0283 - val_loss: 0.0558\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 321: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0283 - val_loss: 0.0558\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 322: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0283 - val_loss: 0.0558\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 323: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0283 - val_loss: 0.0558\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 324: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0283 - val_loss: 0.0558\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 325: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 326: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 327: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 328: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 329: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 330: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 331: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 332: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 333: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0282 - val_loss: 0.0559\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 334: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0282 - val_loss: 0.0560\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 335: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0282 - val_loss: 0.0560\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 336: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0281 - val_loss: 0.0560\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 337: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0281 - val_loss: 0.0560\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 338: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0281 - val_loss: 0.0560\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 339: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0281 - val_loss: 0.0560\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 340: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0281 - val_loss: 0.0560\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 341: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0281 - val_loss: 0.0560\n","Epoch 342/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 342: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 343/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 343: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 344/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 344: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 345/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 345: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 346/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 346: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 347/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 347: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 348/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 348: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 349/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 349: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 350/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0281\n","Epoch 350: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 94ms/step - loss: 0.0281 - val_loss: 0.0561\n","Epoch 351/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 351: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 84ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 352/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 352: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 353/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 353: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 354/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 354: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 355/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 355: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 101ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 356/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 356: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 129ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 357/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 357: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 358/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 358: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 359/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 359: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 360/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 360: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 99ms/step - loss: 0.0280 - val_loss: 0.0562\n","Epoch 361/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0280\n","Epoch 361: val_loss did not improve from 0.05524\n","1/1 [==============================] - 0s 108ms/step - loss: 0.0280 - val_loss: 0.0563\n","1/1 [==============================] - 0s 211ms/step - loss: 0.0738\n","loss_and_metrics : 0.07384906709194183\n","1/1 [==============================] - 0s 199ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","#model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","history = model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))\n","\n","import matplotlib.pyplot as plt\n","import os\n","\n","y_vloss = history.history['val_loss']\n","y_loss = history.history['loss']\n","\n","x_len = numpy.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"]}]}