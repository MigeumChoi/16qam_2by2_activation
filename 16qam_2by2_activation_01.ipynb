{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZO6++HUq04f2+29juWjUw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgnmmdC40HMk","executionInfo":{"status":"ok","timestamp":1694932206164,"user_tz":-540,"elapsed":30901,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"255ffcfa-7453-46ae-ea87-7fffd5564159"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3648\n","Epoch 1: val_loss improved from inf to 0.34695, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 0.3648 - val_loss: 0.3470\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3600\n","Epoch 2: val_loss improved from 0.34695 to 0.34253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3600 - val_loss: 0.3425\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3553\n","Epoch 3: val_loss improved from 0.34253 to 0.33817, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 55ms/step - loss: 0.3553 - val_loss: 0.3382\n","Epoch 4/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3507\n","Epoch 4: val_loss improved from 0.33817 to 0.33388, saving model to hl5_0100.h5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 62ms/step - loss: 0.3507 - val_loss: 0.3339\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3461\n","Epoch 5: val_loss improved from 0.33388 to 0.32964, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.3461 - val_loss: 0.3296\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3415\n","Epoch 6: val_loss improved from 0.32964 to 0.32547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.3415 - val_loss: 0.3255\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3370\n","Epoch 7: val_loss improved from 0.32547 to 0.32135, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.3370 - val_loss: 0.3213\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3326\n","Epoch 8: val_loss improved from 0.32135 to 0.31729, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3326 - val_loss: 0.3173\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3283\n","Epoch 9: val_loss improved from 0.31729 to 0.31328, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3283 - val_loss: 0.3133\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3240\n","Epoch 10: val_loss improved from 0.31328 to 0.30933, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3240 - val_loss: 0.3093\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3197\n","Epoch 11: val_loss improved from 0.30933 to 0.30543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3197 - val_loss: 0.3054\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3155\n","Epoch 12: val_loss improved from 0.30543 to 0.30159, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.3155 - val_loss: 0.3016\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3114\n","Epoch 13: val_loss improved from 0.30159 to 0.29780, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.3114 - val_loss: 0.2978\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3073\n","Epoch 14: val_loss improved from 0.29780 to 0.29406, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3073 - val_loss: 0.2941\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3033\n","Epoch 15: val_loss improved from 0.29406 to 0.29036, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.3033 - val_loss: 0.2904\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2993\n","Epoch 16: val_loss improved from 0.29036 to 0.28672, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.2993 - val_loss: 0.2867\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2954\n","Epoch 17: val_loss improved from 0.28672 to 0.28313, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2954 - val_loss: 0.2831\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2915\n","Epoch 18: val_loss improved from 0.28313 to 0.27958, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2915 - val_loss: 0.2796\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2877\n","Epoch 19: val_loss improved from 0.27958 to 0.27608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2877 - val_loss: 0.2761\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2839\n","Epoch 20: val_loss improved from 0.27608 to 0.27262, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2839 - val_loss: 0.2726\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2802\n","Epoch 21: val_loss improved from 0.27262 to 0.26921, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2802 - val_loss: 0.2692\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2765\n","Epoch 22: val_loss improved from 0.26921 to 0.26585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2765 - val_loss: 0.2658\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2729\n","Epoch 23: val_loss improved from 0.26585 to 0.26253, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2729 - val_loss: 0.2625\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2693\n","Epoch 24: val_loss improved from 0.26253 to 0.25925, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2693 - val_loss: 0.2592\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2658\n","Epoch 25: val_loss improved from 0.25925 to 0.25601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2658 - val_loss: 0.2560\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2622\n","Epoch 26: val_loss improved from 0.25601 to 0.25282, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 57ms/step - loss: 0.2622 - val_loss: 0.2528\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2588\n","Epoch 27: val_loss improved from 0.25282 to 0.24966, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2588 - val_loss: 0.2497\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2554\n","Epoch 28: val_loss improved from 0.24966 to 0.24655, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2554 - val_loss: 0.2465\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2520\n","Epoch 29: val_loss improved from 0.24655 to 0.24347, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2520 - val_loss: 0.2435\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2487\n","Epoch 30: val_loss improved from 0.24347 to 0.24044, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.2487 - val_loss: 0.2404\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2454\n","Epoch 31: val_loss improved from 0.24044 to 0.23744, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.2454 - val_loss: 0.2374\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2421\n","Epoch 32: val_loss improved from 0.23744 to 0.23449, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.2421 - val_loss: 0.2345\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2389\n","Epoch 33: val_loss improved from 0.23449 to 0.23157, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2389 - val_loss: 0.2316\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2357\n","Epoch 34: val_loss improved from 0.23157 to 0.22868, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2357 - val_loss: 0.2287\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2326\n","Epoch 35: val_loss improved from 0.22868 to 0.22584, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2326 - val_loss: 0.2258\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2295\n","Epoch 36: val_loss improved from 0.22584 to 0.22303, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2295 - val_loss: 0.2230\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2264\n","Epoch 37: val_loss improved from 0.22303 to 0.22025, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 58ms/step - loss: 0.2264 - val_loss: 0.2203\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2234\n","Epoch 38: val_loss improved from 0.22025 to 0.21751, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.2234 - val_loss: 0.2175\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2204\n","Epoch 39: val_loss improved from 0.21751 to 0.21481, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2204 - val_loss: 0.2148\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2174\n","Epoch 40: val_loss improved from 0.21481 to 0.21213, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.2174 - val_loss: 0.2121\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2145\n","Epoch 41: val_loss improved from 0.21213 to 0.20950, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2145 - val_loss: 0.2095\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2116\n","Epoch 42: val_loss improved from 0.20950 to 0.20689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.2116 - val_loss: 0.2069\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2088\n","Epoch 43: val_loss improved from 0.20689 to 0.20432, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.2088 - val_loss: 0.2043\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2060\n","Epoch 44: val_loss improved from 0.20432 to 0.20179, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.2060 - val_loss: 0.2018\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2032\n","Epoch 45: val_loss improved from 0.20179 to 0.19928, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.2032 - val_loss: 0.1993\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2004\n","Epoch 46: val_loss improved from 0.19928 to 0.19681, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.2004 - val_loss: 0.1968\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1977\n","Epoch 47: val_loss improved from 0.19681 to 0.19436, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.1977 - val_loss: 0.1944\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1950\n","Epoch 48: val_loss improved from 0.19436 to 0.19195, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1950 - val_loss: 0.1920\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1924\n","Epoch 49: val_loss improved from 0.19195 to 0.18957, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1924 - val_loss: 0.1896\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1898\n","Epoch 50: val_loss improved from 0.18957 to 0.18722, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1898 - val_loss: 0.1872\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1872\n","Epoch 51: val_loss improved from 0.18722 to 0.18490, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1872 - val_loss: 0.1849\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1846\n","Epoch 52: val_loss improved from 0.18490 to 0.18261, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1846 - val_loss: 0.1826\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1821\n","Epoch 53: val_loss improved from 0.18261 to 0.18035, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.1821 - val_loss: 0.1804\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1796\n","Epoch 54: val_loss improved from 0.18035 to 0.17812, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1796 - val_loss: 0.1781\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1772\n","Epoch 55: val_loss improved from 0.17812 to 0.17592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1772 - val_loss: 0.1759\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1747\n","Epoch 56: val_loss improved from 0.17592 to 0.17374, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1747 - val_loss: 0.1737\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1723\n","Epoch 57: val_loss improved from 0.17374 to 0.17160, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1723 - val_loss: 0.1716\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1699\n","Epoch 58: val_loss improved from 0.17160 to 0.16948, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1699 - val_loss: 0.1695\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1676\n","Epoch 59: val_loss improved from 0.16948 to 0.16739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1676 - val_loss: 0.1674\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1653\n","Epoch 60: val_loss improved from 0.16739 to 0.16533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1653 - val_loss: 0.1653\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1630\n","Epoch 61: val_loss improved from 0.16533 to 0.16329, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1630 - val_loss: 0.1633\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1607\n","Epoch 62: val_loss improved from 0.16329 to 0.16128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.1607 - val_loss: 0.1613\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1585\n","Epoch 63: val_loss improved from 0.16128 to 0.15930, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1585 - val_loss: 0.1593\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1563\n","Epoch 64: val_loss improved from 0.15930 to 0.15734, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1563 - val_loss: 0.1573\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1541\n","Epoch 65: val_loss improved from 0.15734 to 0.15541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1541 - val_loss: 0.1554\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1519\n","Epoch 66: val_loss improved from 0.15541 to 0.15351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1519 - val_loss: 0.1535\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1498\n","Epoch 67: val_loss improved from 0.15351 to 0.15163, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1498 - val_loss: 0.1516\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1477\n","Epoch 68: val_loss improved from 0.15163 to 0.14978, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1477 - val_loss: 0.1498\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1457\n","Epoch 69: val_loss improved from 0.14978 to 0.14795, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.1457 - val_loss: 0.1480\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1436\n","Epoch 70: val_loss improved from 0.14795 to 0.14615, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1436 - val_loss: 0.1461\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1416\n","Epoch 71: val_loss improved from 0.14615 to 0.14437, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.1416 - val_loss: 0.1444\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1396\n","Epoch 72: val_loss improved from 0.14437 to 0.14262, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1396 - val_loss: 0.1426\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1376\n","Epoch 73: val_loss improved from 0.14262 to 0.14089, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1376 - val_loss: 0.1409\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1357\n","Epoch 74: val_loss improved from 0.14089 to 0.13919, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1357 - val_loss: 0.1392\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1338\n","Epoch 75: val_loss improved from 0.13919 to 0.13750, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1338 - val_loss: 0.1375\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1319\n","Epoch 76: val_loss improved from 0.13750 to 0.13585, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.1319 - val_loss: 0.1358\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1300\n","Epoch 77: val_loss improved from 0.13585 to 0.13421, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1300 - val_loss: 0.1342\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1282\n","Epoch 78: val_loss improved from 0.13421 to 0.13260, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1282 - val_loss: 0.1326\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1263\n","Epoch 79: val_loss improved from 0.13260 to 0.13102, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.1263 - val_loss: 0.1310\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1245\n","Epoch 80: val_loss improved from 0.13102 to 0.12945, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 59ms/step - loss: 0.1245 - val_loss: 0.1295\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1228\n","Epoch 81: val_loss improved from 0.12945 to 0.12791, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1228 - val_loss: 0.1279\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1210\n","Epoch 82: val_loss improved from 0.12791 to 0.12639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.1210 - val_loss: 0.1264\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1193\n","Epoch 83: val_loss improved from 0.12639 to 0.12489, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.1193 - val_loss: 0.1249\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1176\n","Epoch 84: val_loss improved from 0.12489 to 0.12342, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.1176 - val_loss: 0.1234\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1159\n","Epoch 85: val_loss improved from 0.12342 to 0.12197, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1159 - val_loss: 0.1220\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1143\n","Epoch 86: val_loss improved from 0.12197 to 0.12054, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 87ms/step - loss: 0.1143 - val_loss: 0.1205\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1126\n","Epoch 87: val_loss improved from 0.12054 to 0.11913, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.1126 - val_loss: 0.1191\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1110\n","Epoch 88: val_loss improved from 0.11913 to 0.11774, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.1110 - val_loss: 0.1177\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1094\n","Epoch 89: val_loss improved from 0.11774 to 0.11637, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 115ms/step - loss: 0.1094 - val_loss: 0.1164\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1079\n","Epoch 90: val_loss improved from 0.11637 to 0.11503, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 123ms/step - loss: 0.1079 - val_loss: 0.1150\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1063\n","Epoch 91: val_loss improved from 0.11503 to 0.11370, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.1063 - val_loss: 0.1137\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1048\n","Epoch 92: val_loss improved from 0.11370 to 0.11240, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 110ms/step - loss: 0.1048 - val_loss: 0.1124\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1033\n","Epoch 93: val_loss improved from 0.11240 to 0.11112, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.1033 - val_loss: 0.1111\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1018\n","Epoch 94: val_loss improved from 0.11112 to 0.10985, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.1018 - val_loss: 0.1099\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1004\n","Epoch 95: val_loss improved from 0.10985 to 0.10861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.1004 - val_loss: 0.1086\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0989\n","Epoch 96: val_loss improved from 0.10861 to 0.10739, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0989 - val_loss: 0.1074\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0975\n","Epoch 97: val_loss improved from 0.10739 to 0.10619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0975 - val_loss: 0.1062\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0961\n","Epoch 98: val_loss improved from 0.10619 to 0.10500, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0961 - val_loss: 0.1050\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0948\n","Epoch 99: val_loss improved from 0.10500 to 0.10384, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0948 - val_loss: 0.1038\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0934\n","Epoch 100: val_loss improved from 0.10384 to 0.10270, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0934 - val_loss: 0.1027\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0921\n","Epoch 101: val_loss improved from 0.10270 to 0.10157, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0921 - val_loss: 0.1016\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0908\n","Epoch 102: val_loss improved from 0.10157 to 0.10047, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 107ms/step - loss: 0.0908 - val_loss: 0.1005\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0895\n","Epoch 103: val_loss improved from 0.10047 to 0.09938, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 114ms/step - loss: 0.0895 - val_loss: 0.0994\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0882\n","Epoch 104: val_loss improved from 0.09938 to 0.09831, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0882 - val_loss: 0.0983\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0870\n","Epoch 105: val_loss improved from 0.09831 to 0.09726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 113ms/step - loss: 0.0870 - val_loss: 0.0973\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0857\n","Epoch 106: val_loss improved from 0.09726 to 0.09623, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 86ms/step - loss: 0.0857 - val_loss: 0.0962\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0845\n","Epoch 107: val_loss improved from 0.09623 to 0.09522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 112ms/step - loss: 0.0845 - val_loss: 0.0952\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0833\n","Epoch 108: val_loss improved from 0.09522 to 0.09422, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 124ms/step - loss: 0.0833 - val_loss: 0.0942\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0821\n","Epoch 109: val_loss improved from 0.09422 to 0.09324, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0821 - val_loss: 0.0932\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0810\n","Epoch 110: val_loss improved from 0.09324 to 0.09228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0810 - val_loss: 0.0923\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0799\n","Epoch 111: val_loss improved from 0.09228 to 0.09134, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0799 - val_loss: 0.0913\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0787\n","Epoch 112: val_loss improved from 0.09134 to 0.09042, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 90ms/step - loss: 0.0787 - val_loss: 0.0904\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0776\n","Epoch 113: val_loss improved from 0.09042 to 0.08951, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 97ms/step - loss: 0.0776 - val_loss: 0.0895\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0766\n","Epoch 114: val_loss improved from 0.08951 to 0.08862, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0766 - val_loss: 0.0886\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0755\n","Epoch 115: val_loss improved from 0.08862 to 0.08775, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0755 - val_loss: 0.0877\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0744\n","Epoch 116: val_loss improved from 0.08775 to 0.08689, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0744 - val_loss: 0.0869\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0734\n","Epoch 117: val_loss improved from 0.08689 to 0.08605, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0734 - val_loss: 0.0860\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0724\n","Epoch 118: val_loss improved from 0.08605 to 0.08522, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0724 - val_loss: 0.0852\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0714\n","Epoch 119: val_loss improved from 0.08522 to 0.08442, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0714 - val_loss: 0.0844\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0704\n","Epoch 120: val_loss improved from 0.08442 to 0.08362, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0704 - val_loss: 0.0836\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0695\n","Epoch 121: val_loss improved from 0.08362 to 0.08285, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0695 - val_loss: 0.0828\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0685\n","Epoch 122: val_loss improved from 0.08285 to 0.08209, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0685 - val_loss: 0.0821\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0676\n","Epoch 123: val_loss improved from 0.08209 to 0.08134, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0676 - val_loss: 0.0813\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0667\n","Epoch 124: val_loss improved from 0.08134 to 0.08061, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0667 - val_loss: 0.0806\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0658\n","Epoch 125: val_loss improved from 0.08061 to 0.07990, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0658 - val_loss: 0.0799\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0649\n","Epoch 126: val_loss improved from 0.07990 to 0.07920, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0649 - val_loss: 0.0792\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0641\n","Epoch 127: val_loss improved from 0.07920 to 0.07851, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0641 - val_loss: 0.0785\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0632\n","Epoch 128: val_loss improved from 0.07851 to 0.07784, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0632 - val_loss: 0.0778\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0624\n","Epoch 129: val_loss improved from 0.07784 to 0.07718, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0624 - val_loss: 0.0772\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0616\n","Epoch 130: val_loss improved from 0.07718 to 0.07654, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0616 - val_loss: 0.0765\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0608\n","Epoch 131: val_loss improved from 0.07654 to 0.07591, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0608 - val_loss: 0.0759\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0600\n","Epoch 132: val_loss improved from 0.07591 to 0.07530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0600 - val_loss: 0.0753\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0593\n","Epoch 133: val_loss improved from 0.07530 to 0.07470, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0593 - val_loss: 0.0747\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0585\n","Epoch 134: val_loss improved from 0.07470 to 0.07411, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0585 - val_loss: 0.0741\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0578\n","Epoch 135: val_loss improved from 0.07411 to 0.07354, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0578 - val_loss: 0.0735\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0571\n","Epoch 136: val_loss improved from 0.07354 to 0.07297, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0571 - val_loss: 0.0730\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0564\n","Epoch 137: val_loss improved from 0.07297 to 0.07243, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0564 - val_loss: 0.0724\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0557\n","Epoch 138: val_loss improved from 0.07243 to 0.07189, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 61ms/step - loss: 0.0557 - val_loss: 0.0719\n","Epoch 139/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0550\n","Epoch 139: val_loss improved from 0.07189 to 0.07137, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0550 - val_loss: 0.0714\n","Epoch 140/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0543\n","Epoch 140: val_loss improved from 0.07137 to 0.07086, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0543 - val_loss: 0.0709\n","Epoch 141/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0537\n","Epoch 141: val_loss improved from 0.07086 to 0.07036, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0537 - val_loss: 0.0704\n","Epoch 142/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0530\n","Epoch 142: val_loss improved from 0.07036 to 0.06987, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0530 - val_loss: 0.0699\n","Epoch 143/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0524\n","Epoch 143: val_loss improved from 0.06987 to 0.06940, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0524 - val_loss: 0.0694\n","Epoch 144/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0518\n","Epoch 144: val_loss improved from 0.06940 to 0.06894, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0518 - val_loss: 0.0689\n","Epoch 145/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0512\n","Epoch 145: val_loss improved from 0.06894 to 0.06849, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0512 - val_loss: 0.0685\n","Epoch 146/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0506\n","Epoch 146: val_loss improved from 0.06849 to 0.06805, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0506 - val_loss: 0.0680\n","Epoch 147/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0500\n","Epoch 147: val_loss improved from 0.06805 to 0.06762, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0500 - val_loss: 0.0676\n","Epoch 148/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0495\n","Epoch 148: val_loss improved from 0.06762 to 0.06720, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0495 - val_loss: 0.0672\n","Epoch 149/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0489\n","Epoch 149: val_loss improved from 0.06720 to 0.06679, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0489 - val_loss: 0.0668\n","Epoch 150/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0484\n","Epoch 150: val_loss improved from 0.06679 to 0.06640, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0484 - val_loss: 0.0664\n","Epoch 151/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0479\n","Epoch 151: val_loss improved from 0.06640 to 0.06601, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0479 - val_loss: 0.0660\n","Epoch 152/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0473\n","Epoch 152: val_loss improved from 0.06601 to 0.06563, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0473 - val_loss: 0.0656\n","Epoch 153/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0468\n","Epoch 153: val_loss improved from 0.06563 to 0.06527, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0468 - val_loss: 0.0653\n","Epoch 154/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0463\n","Epoch 154: val_loss improved from 0.06527 to 0.06491, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0463 - val_loss: 0.0649\n","Epoch 155/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0459\n","Epoch 155: val_loss improved from 0.06491 to 0.06457, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0459 - val_loss: 0.0646\n","Epoch 156/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0454\n","Epoch 156: val_loss improved from 0.06457 to 0.06423, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0454 - val_loss: 0.0642\n","Epoch 157/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0449\n","Epoch 157: val_loss improved from 0.06423 to 0.06390, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0449 - val_loss: 0.0639\n","Epoch 158/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0445\n","Epoch 158: val_loss improved from 0.06390 to 0.06358, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0445 - val_loss: 0.0636\n","Epoch 159/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0440\n","Epoch 159: val_loss improved from 0.06358 to 0.06327, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0440 - val_loss: 0.0633\n","Epoch 160/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0436\n","Epoch 160: val_loss improved from 0.06327 to 0.06297, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0436 - val_loss: 0.0630\n","Epoch 161/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0432\n","Epoch 161: val_loss improved from 0.06297 to 0.06268, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0432 - val_loss: 0.0627\n","Epoch 162/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0428\n","Epoch 162: val_loss improved from 0.06268 to 0.06240, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0428 - val_loss: 0.0624\n","Epoch 163/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0424\n","Epoch 163: val_loss improved from 0.06240 to 0.06212, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0424 - val_loss: 0.0621\n","Epoch 164/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0420\n","Epoch 164: val_loss improved from 0.06212 to 0.06186, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0420 - val_loss: 0.0619\n","Epoch 165/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0416\n","Epoch 165: val_loss improved from 0.06186 to 0.06160, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0416 - val_loss: 0.0616\n","Epoch 166/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0413\n","Epoch 166: val_loss improved from 0.06160 to 0.06135, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0413 - val_loss: 0.0613\n","Epoch 167/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0409\n","Epoch 167: val_loss improved from 0.06135 to 0.06110, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0409 - val_loss: 0.0611\n","Epoch 168/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0405\n","Epoch 168: val_loss improved from 0.06110 to 0.06087, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0405 - val_loss: 0.0609\n","Epoch 169/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0402\n","Epoch 169: val_loss improved from 0.06087 to 0.06064, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0402 - val_loss: 0.0606\n","Epoch 170/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0399\n","Epoch 170: val_loss improved from 0.06064 to 0.06042, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0399 - val_loss: 0.0604\n","Epoch 171/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0395\n","Epoch 171: val_loss improved from 0.06042 to 0.06020, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0395 - val_loss: 0.0602\n","Epoch 172/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0392\n","Epoch 172: val_loss improved from 0.06020 to 0.05999, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0392 - val_loss: 0.0600\n","Epoch 173/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0389\n","Epoch 173: val_loss improved from 0.05999 to 0.05979, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0389 - val_loss: 0.0598\n","Epoch 174/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0386\n","Epoch 174: val_loss improved from 0.05979 to 0.05960, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0386 - val_loss: 0.0596\n","Epoch 175/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0383\n","Epoch 175: val_loss improved from 0.05960 to 0.05941, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0383 - val_loss: 0.0594\n","Epoch 176/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0380\n","Epoch 176: val_loss improved from 0.05941 to 0.05923, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0380 - val_loss: 0.0592\n","Epoch 177/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0378\n","Epoch 177: val_loss improved from 0.05923 to 0.05905, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0378 - val_loss: 0.0591\n","Epoch 178/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0375\n","Epoch 178: val_loss improved from 0.05905 to 0.05888, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0375 - val_loss: 0.0589\n","Epoch 179/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0372\n","Epoch 179: val_loss improved from 0.05888 to 0.05872, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0372 - val_loss: 0.0587\n","Epoch 180/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0370\n","Epoch 180: val_loss improved from 0.05872 to 0.05856, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0370 - val_loss: 0.0586\n","Epoch 181/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0367\n","Epoch 181: val_loss improved from 0.05856 to 0.05841, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0367 - val_loss: 0.0584\n","Epoch 182/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0365\n","Epoch 182: val_loss improved from 0.05841 to 0.05826, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0365 - val_loss: 0.0583\n","Epoch 183/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0362\n","Epoch 183: val_loss improved from 0.05826 to 0.05812, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0362 - val_loss: 0.0581\n","Epoch 184/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0360\n","Epoch 184: val_loss improved from 0.05812 to 0.05798, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0360 - val_loss: 0.0580\n","Epoch 185/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0358\n","Epoch 185: val_loss improved from 0.05798 to 0.05785, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0358 - val_loss: 0.0579\n","Epoch 186/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0355\n","Epoch 186: val_loss improved from 0.05785 to 0.05772, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0355 - val_loss: 0.0577\n","Epoch 187/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0353\n","Epoch 187: val_loss improved from 0.05772 to 0.05760, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0353 - val_loss: 0.0576\n","Epoch 188/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0351\n","Epoch 188: val_loss improved from 0.05760 to 0.05748, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0351 - val_loss: 0.0575\n","Epoch 189/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0349\n","Epoch 189: val_loss improved from 0.05748 to 0.05737, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0349 - val_loss: 0.0574\n","Epoch 190/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0347\n","Epoch 190: val_loss improved from 0.05737 to 0.05726, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0347 - val_loss: 0.0573\n","Epoch 191/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0345\n","Epoch 191: val_loss improved from 0.05726 to 0.05716, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0345 - val_loss: 0.0572\n","Epoch 192/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0343\n","Epoch 192: val_loss improved from 0.05716 to 0.05706, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0343 - val_loss: 0.0571\n","Epoch 193/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0342\n","Epoch 193: val_loss improved from 0.05706 to 0.05696, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0342 - val_loss: 0.0570\n","Epoch 194/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0340\n","Epoch 194: val_loss improved from 0.05696 to 0.05687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0340 - val_loss: 0.0569\n","Epoch 195/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0338\n","Epoch 195: val_loss improved from 0.05687 to 0.05678, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0338 - val_loss: 0.0568\n","Epoch 196/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0336\n","Epoch 196: val_loss improved from 0.05678 to 0.05669, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0336 - val_loss: 0.0567\n","Epoch 197/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0335\n","Epoch 197: val_loss improved from 0.05669 to 0.05661, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0335 - val_loss: 0.0566\n","Epoch 198/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0333\n","Epoch 198: val_loss improved from 0.05661 to 0.05653, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0333 - val_loss: 0.0565\n","Epoch 199/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0332\n","Epoch 199: val_loss improved from 0.05653 to 0.05646, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0332 - val_loss: 0.0565\n","Epoch 200/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0330\n","Epoch 200: val_loss improved from 0.05646 to 0.05639, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0330 - val_loss: 0.0564\n","Epoch 201/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0329\n","Epoch 201: val_loss improved from 0.05639 to 0.05632, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0329 - val_loss: 0.0563\n","Epoch 202/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0327\n","Epoch 202: val_loss improved from 0.05632 to 0.05625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0327 - val_loss: 0.0563\n","Epoch 203/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0326\n","Epoch 203: val_loss improved from 0.05625 to 0.05619, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0326 - val_loss: 0.0562\n","Epoch 204/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0325\n","Epoch 204: val_loss improved from 0.05619 to 0.05613, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0325 - val_loss: 0.0561\n","Epoch 205/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0323\n","Epoch 205: val_loss improved from 0.05613 to 0.05608, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0323 - val_loss: 0.0561\n","Epoch 206/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0322\n","Epoch 206: val_loss improved from 0.05608 to 0.05602, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0322 - val_loss: 0.0560\n","Epoch 207/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0321\n","Epoch 207: val_loss improved from 0.05602 to 0.05597, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0321 - val_loss: 0.0560\n","Epoch 208/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 208: val_loss improved from 0.05597 to 0.05592, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0320 - val_loss: 0.0559\n","Epoch 209/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0319\n","Epoch 209: val_loss improved from 0.05592 to 0.05588, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0319 - val_loss: 0.0559\n","Epoch 210/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0318\n","Epoch 210: val_loss improved from 0.05588 to 0.05583, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0318 - val_loss: 0.0558\n","Epoch 211/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0317\n","Epoch 211: val_loss improved from 0.05583 to 0.05579, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0317 - val_loss: 0.0558\n","Epoch 212/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0316\n","Epoch 212: val_loss improved from 0.05579 to 0.05575, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0316 - val_loss: 0.0558\n","Epoch 213/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0315\n","Epoch 213: val_loss improved from 0.05575 to 0.05571, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0315 - val_loss: 0.0557\n","Epoch 214/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0314\n","Epoch 214: val_loss improved from 0.05571 to 0.05568, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0314 - val_loss: 0.0557\n","Epoch 215/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0313\n","Epoch 215: val_loss improved from 0.05568 to 0.05565, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0313 - val_loss: 0.0556\n","Epoch 216/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0312\n","Epoch 216: val_loss improved from 0.05565 to 0.05562, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0312 - val_loss: 0.0556\n","Epoch 217/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0311\n","Epoch 217: val_loss improved from 0.05562 to 0.05559, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0311 - val_loss: 0.0556\n","Epoch 218/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0310\n","Epoch 218: val_loss improved from 0.05559 to 0.05556, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0310 - val_loss: 0.0556\n","Epoch 219/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0309\n","Epoch 219: val_loss improved from 0.05556 to 0.05553, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0309 - val_loss: 0.0555\n","Epoch 220/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0308\n","Epoch 220: val_loss improved from 0.05553 to 0.05551, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0308 - val_loss: 0.0555\n","Epoch 221/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 221: val_loss improved from 0.05551 to 0.05549, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 62ms/step - loss: 0.0307 - val_loss: 0.0555\n","Epoch 222/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0307\n","Epoch 222: val_loss improved from 0.05549 to 0.05547, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0307 - val_loss: 0.0555\n","Epoch 223/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0306\n","Epoch 223: val_loss improved from 0.05547 to 0.05545, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0306 - val_loss: 0.0554\n","Epoch 224/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 224: val_loss improved from 0.05545 to 0.05543, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0305 - val_loss: 0.0554\n","Epoch 225/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0305\n","Epoch 225: val_loss improved from 0.05543 to 0.05541, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0305 - val_loss: 0.0554\n","Epoch 226/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0304\n","Epoch 226: val_loss improved from 0.05541 to 0.05540, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0304 - val_loss: 0.0554\n","Epoch 227/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 227: val_loss improved from 0.05540 to 0.05538, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 228/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0303\n","Epoch 228: val_loss improved from 0.05538 to 0.05537, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0303 - val_loss: 0.0554\n","Epoch 229/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0302\n","Epoch 229: val_loss improved from 0.05537 to 0.05536, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0302 - val_loss: 0.0554\n","Epoch 230/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 230: val_loss improved from 0.05536 to 0.05535, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 231/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0301\n","Epoch 231: val_loss improved from 0.05535 to 0.05534, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0301 - val_loss: 0.0553\n","Epoch 232/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 232: val_loss improved from 0.05534 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 233/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0300\n","Epoch 233: val_loss improved from 0.05533 to 0.05533, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0300 - val_loss: 0.0553\n","Epoch 234/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 234: val_loss improved from 0.05533 to 0.05532, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0299 - val_loss: 0.0553\n","Epoch 235/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0299\n","Epoch 235: val_loss improved from 0.05532 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 64ms/step - loss: 0.0299 - val_loss: 0.0553\n","Epoch 236/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 236: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0298 - val_loss: 0.0553\n","Epoch 237/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0298\n","Epoch 237: val_loss improved from 0.05531 to 0.05531, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0298 - val_loss: 0.0553\n","Epoch 238/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 238: val_loss improved from 0.05531 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0297 - val_loss: 0.0553\n","Epoch 239/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0297\n","Epoch 239: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 0.0297 - val_loss: 0.0553\n","Epoch 240/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 240: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 241/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0296\n","Epoch 241: val_loss improved from 0.05530 to 0.05530, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0296 - val_loss: 0.0553\n","Epoch 242/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 242: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 243/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 243: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 244/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0295\n","Epoch 244: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0295 - val_loss: 0.0553\n","Epoch 245/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 245: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 246/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 246: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 247/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0294\n","Epoch 247: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0294 - val_loss: 0.0553\n","Epoch 248/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 248: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 249/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 249: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 250/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0293\n","Epoch 250: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0293 - val_loss: 0.0553\n","Epoch 251/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 251: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 252/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 252: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 253/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0292\n","Epoch 253: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0292 - val_loss: 0.0553\n","Epoch 254/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 254: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0291 - val_loss: 0.0553\n","Epoch 255/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 255: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 256/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 256: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 257/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0291\n","Epoch 257: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0291 - val_loss: 0.0554\n","Epoch 258/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 258: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 259/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 259: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 260/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 260: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 261/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0290\n","Epoch 261: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0290 - val_loss: 0.0554\n","Epoch 262/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 262: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 263/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 263: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 264/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 264: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 265/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 265: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 91ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 266/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0289\n","Epoch 266: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0289 - val_loss: 0.0554\n","Epoch 267/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 267: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 103ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 268/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 268: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 92ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 269/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 269: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 270/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 270: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 271/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 271: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 272/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0288\n","Epoch 272: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0288 - val_loss: 0.0555\n","Epoch 273/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 273: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 274/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 274: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 275/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 275: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 276/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 276: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 85ms/step - loss: 0.0287 - val_loss: 0.0555\n","Epoch 277/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 277: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 278/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 278: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 279/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0287\n","Epoch 279: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0287 - val_loss: 0.0556\n","Epoch 280/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 280: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 281/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 281: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 282/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 282: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 111ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 283/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 283: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 284/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 284: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 285/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 285: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 286/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 286: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 70ms/step - loss: 0.0286 - val_loss: 0.0556\n","Epoch 287/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 287: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 288/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0286\n","Epoch 288: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 102ms/step - loss: 0.0286 - val_loss: 0.0557\n","Epoch 289/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 289: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 59ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 290/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 290: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 291/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 291: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 292/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 292: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 75ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 293/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 293: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 294/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 294: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 295/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 295: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0285 - val_loss: 0.0557\n","Epoch 296/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 296: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 297/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 297: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 298/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 298: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 299/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 299: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 58ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 300/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0285\n","Epoch 300: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0285 - val_loss: 0.0558\n","Epoch 301/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 301: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0284 - val_loss: 0.0558\n","Epoch 302/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 302: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 81ms/step - loss: 0.0284 - val_loss: 0.0558\n","Epoch 303/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 303: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0284 - val_loss: 0.0558\n","Epoch 304/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 304: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 79ms/step - loss: 0.0284 - val_loss: 0.0558\n","Epoch 305/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 305: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0284 - val_loss: 0.0558\n","Epoch 306/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 306: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 307/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 307: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 308/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 308: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 309/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 309: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 96ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 310/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 310: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 80ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 311/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 311: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 312/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 312: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 313/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 313: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 66ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 314/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 314: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 60ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 315/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 315: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 316/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0284\n","Epoch 316: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0284 - val_loss: 0.0559\n","Epoch 317/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 317: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 318/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 318: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 319/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 319: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 320/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 320: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 321/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 321: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 322/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 322: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 323/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 323: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 324/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 324: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 325/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 325: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 326/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 326: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 327/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 327: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 328/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 328: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0283 - val_loss: 0.0560\n","Epoch 329/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 329: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 330/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 330: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 331/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 331: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 332/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 332: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 333/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 333: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 67ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 334/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 334: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 46ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 335/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 335: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 336/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 336: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 337/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 337: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 338/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 338: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 339/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 339: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 340/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0283\n","Epoch 340: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 50ms/step - loss: 0.0283 - val_loss: 0.0561\n","Epoch 341/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0282\n","Epoch 341: val_loss did not improve from 0.05530\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0282 - val_loss: 0.0561\n","1/1 [==============================] - 0s 123ms/step - loss: 0.0735\n","loss_and_metrics : 0.07353220880031586\n","1/1 [==============================] - 0s 108ms/step\n"]}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","from keras.optimizers import SGD\n","#from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"tanh\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='sgd')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))"]}]}